{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MalmoPython\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from IPython.display import clear_output,display\n",
    "actions = {\n",
    "    'strafe':{\n",
    "        'left': 'strafe -1',\n",
    "        'right': 'strafe 1'\n",
    "    },\n",
    "    'move':{\n",
    "        'back':'move -1',\n",
    "        'forward':'move 1'\n",
    "    },\n",
    "    'pitch':{\n",
    "        'up':'pitch -0.03',\n",
    "        'down':'pitch 0.03'\n",
    "    },\n",
    "    'turn':{\n",
    "        'anti':'turn -1',\n",
    "        'clk':'turn 1'\n",
    "    },\n",
    "    'jump':{\n",
    "        'on':'jump 1',\n",
    "        'off':'jump 0'\n",
    "    },\n",
    "    'attack':{\n",
    "        'on': 'attack 1',\n",
    "        'off': 'attack 0'\n",
    "    },\n",
    "    'use':{\n",
    "        'on': 'use 1',\n",
    "        'off': 'use 0'\n",
    "    },\n",
    "    'crouch':{\n",
    "        'on':'crouch 1',\n",
    "        'off':'crouch 0'\n",
    "    }\n",
    "}\n",
    "# Create default Malmo objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_actions = {\n",
    "    'strafe':{\n",
    "        'left': 'strafe -1',\n",
    "        'right': 'strafe 1'\n",
    "    },\n",
    "    'move':{\n",
    "        'back':'move -1',\n",
    "        'forward':'move 1'\n",
    "    }   \n",
    "}\n",
    "# flatten dict of actions\n",
    "ractions = []\n",
    "for action_type in simple_actions.keys():\n",
    "    \n",
    "    for action in simple_actions[action_type]:\n",
    "        ractions.append(simple_actions[action_type][action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unrecognised option '-f'\n",
      "Malmo version: 0.31.0\n",
      "\n",
      "Allowed options:\n",
      "  -h [ --help ]         show description of allowed options\n",
      "  --test                run this as an integration test\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent_host = MalmoPython.AgentHost()\n",
    "try:\n",
    "    agent_host.parse( sys.argv )\n",
    "except RuntimeError as e:\n",
    "    print 'ERROR:',e\n",
    "    print agent_host.getUsage()\n",
    "    exit(1)\n",
    "if agent_host.receivedArgument(\"help\"):\n",
    "    print agent_host.getUsage()\n",
    "    exit(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.model = self._createModel()\n",
    "        # self.model.load_weights(\"cartpole-basic.h5\")\n",
    "\n",
    "    def _createModel(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(output_dim=64, activation='relu', input_dim=self.stateCnt))\n",
    "        model.add(Dense(output_dim=self.actionCnt, activation='linear'))\n",
    "\n",
    "        opt = RMSprop(lr=0.00025)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, x, y, epoch=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=64, nb_epoch=epoch, verbose=verbose)\n",
    "\n",
    "    def predict(self, s):\n",
    "        return self.model.predict(s)\n",
    "\n",
    "    def predictOne(self, s):\n",
    "        return self.predict(s.reshape(1, self.stateCnt)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, x, y):\n",
    "    model.fit(x, y, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:   # stored as ( s, a, r, s_ )\n",
    "    samples = []\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add(self, sample):\n",
    "        self.samples.append(sample)        \n",
    "\n",
    "        if len(self.samples) > self.capacity:\n",
    "            self.samples.pop(0)\n",
    "\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_CAPACITY = 100000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "LAMBDA = 0.001      # speed of decay\n",
    "\n",
    "class Agent:\n",
    "    steps = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "\n",
    "    def __init__(self, stateCnt, actionCnt,actions):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "        self.actions = actions\n",
    "\n",
    "        self.brain = Brain(stateCnt, actionCnt)\n",
    "        self.memory = Memory(MEMORY_CAPACITY)\n",
    "        \n",
    "    def act(self, s):\n",
    "        # Epsilon greedy action selection\n",
    "        if random.random() < self.epsilon:\n",
    "            act_int = random.randint(0, self.actionCnt-1)\n",
    "        else:\n",
    "            act_int = numpy.argmax(self.brain.predictOne(s))\n",
    "        return self.actions[act_int]\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        self.memory.add(sample)        \n",
    "\n",
    "        # slowly decrease Epsilon based on our eperience\n",
    "        self.steps += 1\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "\n",
    "    def replay(self):    \n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        batchLen = len(batch)\n",
    "\n",
    "        no_state = numpy.zeros(self.stateCnt)\n",
    "\n",
    "        states = numpy.array([ o[0] for o in batch ])\n",
    "        states_ = numpy.array([ (no_state if o[3] is None else o[3]) for o in batch ])\n",
    "\n",
    "        p = agent.brain.predict(states)\n",
    "        p_ = agent.brain.predict(states_)\n",
    "\n",
    "        x = numpy.zeros((batchLen, self.stateCnt))\n",
    "        y = numpy.zeros((batchLen, self.actionCnt))\n",
    "        \n",
    "        for i in range(batchLen):\n",
    "            o = batch[i]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "            \n",
    "            t = p[i]\n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + GAMMA * numpy.amax(p_[i])\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "            \n",
    "            self.brain.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, agent,agent_host):\n",
    "        self.world_state = None\n",
    "        self.agent = agent\n",
    "        self.agent_host = agent_host\n",
    "        self.my_mission_record = MalmoPython.MissionRecordSpec()\n",
    "        self.episode_length = 100\n",
    "    def parse_state(self,state):\n",
    "        data = json.loads(state.observations[-1].text)\n",
    "        \n",
    "        # get reward if detected, else reward is -1\n",
    "        reward = -1\n",
    "        if state.number_of_rewards_since_last_state > 0:\n",
    "            reward = state.rewards[0].getValue()\n",
    "    \n",
    "        # reformat grid to a vector that only show the floor with blocks\n",
    "        grid = data['grid'][:9]\n",
    "        new_grid = list()\n",
    "        for i,item in enumerate(grid):\n",
    "            if item == 'lava':\n",
    "                new_grid.append(1)\n",
    "            else:\n",
    "                new_grid.append(0)\n",
    "        data['grid'] = new_grid\n",
    "        \n",
    "        return(reward,new_grid)\n",
    "        \n",
    "    def startworld(self,world_file):\n",
    "        # load world\n",
    "        with open('CliffWalking.xml','r') as f:\n",
    "            my_mission = MalmoPython.MissionSpec(f.read(),True)\n",
    "           \n",
    "        # Attempt to start a mission:\n",
    "        max_retries = 3\n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                self.agent_host.startMission( my_mission, self.my_mission_record )\n",
    "                self.world_state = agent_host.getWorldState()\n",
    "                return self.world_state\n",
    "            except RuntimeError as e:\n",
    "                if retry == max_retries - 1:\n",
    "                    print \"Error starting mission:\",\n",
    "                    self.world_state = agent_host.getWorldState()\n",
    "                    \n",
    "    def run(self,world,epochs=0):\n",
    "        # load world\n",
    "        R = 0\n",
    "        for i in range(epochs):\n",
    "            observation = self.startworld(world)\n",
    "            _, s = self.parse_state(observation)\n",
    "            done = !self.observation.is_mission running\n",
    "            while(done):\n",
    "                # get state\n",
    "                \n",
    "                a = agent.act(s)\n",
    "                \n",
    "                \n",
    "                print(\"action:{}\".format(a))\n",
    "                self.agent_host.sendCommand(a)\n",
    "                \n",
    "                observation = self.agent_host.getWorldState()\n",
    "                r, s_prime = self.parse_state(observation)\n",
    "                done = !self.observation.is_mission running\n",
    "                \n",
    "                self.agent.observe(s,a,r,s_prime)\n",
    "                self.agent.replay()\n",
    "                \n",
    "                s = s_prime\n",
    "                R += r\n",
    "                \n",
    "            print(\"done epoch: {}\".format(i))\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "        #loop\n",
    "        # observe\n",
    "        \n",
    "        # take action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function for sanity\n",
    "\n",
    "def observe(world_state):\n",
    "    ## this gonna add overhead :(\n",
    "    data = json.loads(world_state.observations[-1].text)\n",
    "    \n",
    "    \n",
    "    # get reward if detected, else reward is -1\n",
    "    reward = -1\n",
    "    if world_state.number_of_rewards_since_last_state > 0:\n",
    "        reward = world_state.rewards[0].getValue()\n",
    "        \n",
    "    # reformat grid to a vector that only show the floor with blocks\n",
    "    grid = data['grid'][:9]\n",
    "    new_grid = list()\n",
    "    for i,item in enumerate(grid):\n",
    "        if item == 'lava':\n",
    "            new_grid.append(1)\n",
    "        else:\n",
    "            new_grid.append(0)\n",
    "            \n",
    "    data['grid'] = new_grid\n",
    "    \n",
    "    \n",
    "    return (world_state.observations[-1].timestamp, data, reward)\n",
    "\n",
    "def act(state,action,agent):\n",
    "    agent.sendCommand(action)\n",
    "    _, data, reward = observe(state)\n",
    "    \n",
    "    return (state,action,reward,data['grid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2017, 10, 10, 19, 1, 29, 36442),\n",
       " {u'Air': 300,\n",
       "  u'DamageDealt': 0,\n",
       "  u'DamageTaken': 1660,\n",
       "  u'DistanceTravelled': 10093,\n",
       "  u'Food': 20,\n",
       "  u'IsAlive': True,\n",
       "  u'Life': 20.0,\n",
       "  u'MobsKilled': 0,\n",
       "  u'Name': u'Cristina',\n",
       "  u'Pitch': 0.0,\n",
       "  u'PlayersKilled': 0,\n",
       "  u'Score': 0,\n",
       "  u'TimeAlive': 6496,\n",
       "  u'TotalTime': 342607,\n",
       "  u'WorldTime': 6000,\n",
       "  u'XP': 0,\n",
       "  u'XPos': 4.5,\n",
       "  u'YPos': 46.0,\n",
       "  u'Yaw': 0.0,\n",
       "  u'ZPos': 1.5,\n",
       "  u'grid': [1, 1, 1, 0, 0, 1, 0, 1, 1]},\n",
       " -1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-95b629d7bfb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mworld_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_mission_running\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mworld_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_host\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetWorldState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# print(chose_act)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while world_state.is_mission_running:\n",
    "    time.sleep(2)\n",
    "    world_state = agent_host.getWorldState()\n",
    "    # print(chose_act)\n",
    "    clear_output(wait=True)\n",
    "    display(observe(world_state))\n",
    "    # agent_host.sendCommand(chose_act)\n",
    "    for error in world_state.errors:\n",
    "        print \"Error:\",error.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_host.sendCommand('quit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-48bb60cefd09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mworld_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Index out of range"
     ]
    }
   ],
   "source": [
    "world_state.rewards[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-f288cfa02e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent_host\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetWorldState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Index out of range"
     ]
    }
   ],
   "source": [
    "agent_host.getWorldState().rewards[0].getValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda2/envs/mine-env/lib/python2.7/site-packages/ipykernel/__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=64, activation=\"relu\", input_dim=1000)`\n",
      "/home/ubuntu/miniconda2/envs/mine-env/lib/python2.7/site-packages/ipykernel/__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=4, activation=\"linear\")`\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(1000,len(ractions),ractions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(agent,agent_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-338f19e27333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CliffWalking.xml\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-4f7adef084b7>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, world, epochs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartworld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetoutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'self.observation.is_mission running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-4f7adef084b7>\u001b[0m in \u001b[0;36mparse_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# get reward if detected, else reward is -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Index out of range"
     ]
    }
   ],
   "source": [
    "env.run(\"CliffWalking.xml\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mine-env]",
   "language": "python",
   "name": "conda-env-mine-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
