{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:11:07.207800Z",
     "start_time": "2018-01-15T00:10:21.674798Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb;\n",
    "import scipy.misc as scimisc\n",
    "\n",
    "from tkinter import *\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "from PIL import Image\n",
    "\n",
    "import MalmoPython\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output,display\n",
    "import logging\n",
    "import math\n",
    "\n",
    "\n",
    "import gym\n",
    "import gym_minecraft\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import baselines.common.tf_util as U\n",
    "\n",
    "from baselines import logger\n",
    "from baselines import deepq\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer,PrioritizedReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:11:12.311237Z",
     "start_time": "2018-01-15T00:11:07.219665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"3bf6e036-c451-4628-9422-d2ff4b8ec749\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(`.${CLASS_NAME.split(' ')[0]}`);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"3bf6e036-c451-4628-9422-d2ff4b8ec749\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"3bf6e036-c451-4628-9422-d2ff4b8ec749\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '3bf6e036-c451-4628-9422-d2ff4b8ec749' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"3bf6e036-c451-4628-9422-d2ff4b8ec749\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"3bf6e036-c451-4628-9422-d2ff4b8ec749\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"3bf6e036-c451-4628-9422-d2ff4b8ec749\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '3bf6e036-c451-4628-9422-d2ff4b8ec749' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"3bf6e036-c451-4628-9422-d2ff4b8ec749\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, push_notebook, show\n",
    "from bokeh.driving import linear\n",
    "from bokeh.layouts import row,gridplot\n",
    "from IPython.display import clear_output,display\n",
    "import bokeh\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:41:43.814380Z",
     "start_time": "2018-01-15T00:41:43.803442Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential,model_from_json,Model\n",
    "from keras.layers import Conv2D,LSTM,GRU,TimeDistributed,Dense,Flatten,Input,Lambda\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:11:12.366400Z",
     "start_time": "2018-01-15T00:11:12.360833Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_minecraft\n",
    "from MinecraftGym import MinecraftWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:42:06.077584Z",
     "start_time": "2018-01-15T00:42:05.771818Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self,stateCnt,actionCnt,batch_size,recurrent,mode,learning_rate):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.recurrent = recurrent\n",
    "        self.mode = mode\n",
    "        \n",
    "    def build(self):\n",
    "        model = Sequential()\n",
    "        # Lets try a CNN to take screen as input\n",
    "        # batch size is 64, 320x240 video RGB channels with an extra channel for depth\n",
    "        #conv1 = \n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3)),\n",
    "                          input_shape=self.stateCnt,batch_size=self.batch_size))\n",
    "        \n",
    "        #model.add(layers.TimeDistributed(layers.Conv2D(32,(8,8),input_shape=self.stateCnt,activation='relu')))\n",
    "        conv2 = layers.Conv2D(64,(4,4),activation='relu')\n",
    "        conv3 = layers.Conv2D(64,(3,3),activation='relu')\n",
    "        model.add(layers.TimeDistributed(conv2))\n",
    "        model.add(layers.TimeDistributed(conv3))\n",
    "        model.add(layers.TimeDistributed(layers.Flatten()))\n",
    "        #model.add(layers.Flatten())\n",
    "        #model.add(layers.Permute((0,2,1)))\n",
    "        #self.add(Reshape(input_width, num_filters))\n",
    "        model.add(layers.GRU(units=70,stateful=True))\n",
    "        #model.add(layers.Dense(256,activation='relu')\n",
    "        model.add(layers.Dense(output_dim=self.actionCnt))\n",
    "        \n",
    "        model.compile(loss=self._huber_loss,optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    def build2(self):\n",
    "        inpt = Input(shape = self.stateCnt, name = \"input\")\n",
    "        \n",
    "        if self.mode == \"linear\":\n",
    "            \n",
    "            flatten_hidden = Flatten(name = \"flatten\")(input_data)\n",
    "            output = Dense(num_actions, name = \"output\")(flatten_hidden)\n",
    "        else:\n",
    "            if self.recurrent:\n",
    "                # shape should be (timesteps,height,width,color)\n",
    "                conv1 = TimeDistributed(Conv2D(32, (8, 8), strides = 4, activation = \"relu\", name = \"conv1\"))(inpt)\n",
    "                conv2 = TimeDistributed(Conv2D(64, (4, 4), strides = 2, activation = \"relu\", name = \"conv2\"))(conv1)\n",
    "                conv3 = TimeDistributed(Conv2D(64, (3, 3), strides = 1, activation = \"relu\", name = \"conv3\"))(conv2)\n",
    "                flatten_hidden = TimeDistributed(Flatten())(conv3)\n",
    "                hidden_input = TimeDistributed(Dense(512, activation = 'relu', name = 'flat_to_512')) (flatten_hidden)\n",
    "                context = GRU(512, return_sequences=False, stateful=False) (hidden_input)\n",
    "                \n",
    "            if self.mode == \"dqn\":\n",
    "                h4 = Dense(512, activation='relu', name = \"fc\")(context)\n",
    "                output = Dense(num_actions, name = \"output\")(h4)\n",
    "            elif self.mode == \"duel\":\n",
    "                value_hidden = Dense(512, activation = 'relu', name = 'value_fc')(context)\n",
    "                value = Dense(1, name = \"value\")(value_hidden)\n",
    "                \n",
    "                action_hidden = Dense(512, activation = 'relu', name = 'action_fc')(context)\n",
    "                action = Dense(self.actionCnt, name = \"action\")(action_hidden)\n",
    "                \n",
    "                action_mean = Lambda(lambda x: K.mean(x, axis = 1, keepdims = True), name = 'action_mean')(action) \n",
    "                output = Lambda(lambda x: x[0] + x[1] - x[2], name = 'output')([action, value, action_mean])\n",
    "        model = Model(inputs = inpt, outputs = output)\n",
    "        model.compile(loss=self._huber_loss,optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "            \n",
    "        \n",
    "    def _huber_loss(self, target, prediction):\n",
    "        # sqrt(1+error^2)-1\n",
    "        error = prediction - target\n",
    "        return K.mean(K.sqrt(1+K.square(error))-1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:42:07.602090Z",
     "start_time": "2018-01-15T00:42:06.599343Z"
    }
   },
   "outputs": [],
   "source": [
    "test_model = Network((10,80,80,1),6,32,True,'duel',0.001).build2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:46:25.940048Z",
     "start_time": "2018-01-15T00:46:25.929284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input (InputLayer)               (None, 10, 80, 80, 1) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistrib (None, 10, 19, 19, 32 2080        input[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistrib (None, 10, 8, 8, 64)  32832       time_distributed_16[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_18 (TimeDistrib (None, 10, 6, 6, 64)  36928       time_distributed_17[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistrib (None, 10, 2304)      0           time_distributed_18[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_20 (TimeDistrib (None, 10, 512)       1180160     time_distributed_19[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "gru_4 (GRU)                      (None, 512)           1574400     time_distributed_20[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "action_fc (Dense)                (None, 512)           262656      gru_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "action (Dense)                   (None, 6)             3078        action_fc[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "value_fc (Dense)                 (None, 512)           262656      gru_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "value (Dense)                    (None, 1)             513         value_fc[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "action_mean (Lambda)             (None, 1)             0           action[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "output (Lambda)                  (None, 6)             0           action[0][0]                     \n",
      "                                                                   value[0][0]                      \n",
      "                                                                   action_mean[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 3,355,303\n",
      "Trainable params: 3,355,303\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:54:09.621217Z",
     "start_time": "2018-01-15T00:54:09.515797Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate random data of batch size 32\n",
    "t_data = np.random.random((32,10,80,80,1)) \n",
    "\n",
    "t_predict = test_model.predict(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:54:11.688680Z",
     "start_time": "2018-01-15T00:54:11.680448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_predict.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Memory will store entire episodes rather than frames from episodes\n",
    "so instead of a 1D array of frames we now have a 2D array of frames of which the length of 2nd dimension varies.\n",
    "\n",
    "To sample these experience we will choose a random place in memory and take in a stack of frames (each frame is of a time step) and we will apply this to a CNN+RNN hoping for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:38:04.872539Z",
     "start_time": "2018-01-15T02:38:04.592011Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Memory:   # stored as ( s, a, r, s_ )\n",
    "    def __init__(self, capacity):\n",
    "        self.samples = []\n",
    "        self.capacity = capacity\n",
    "    def add(self, sample):\n",
    "        self.samples.append(sample)        \n",
    "        if len(self.samples) > self.capacity:\n",
    "            self.samples.pop(0)\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)\n",
    "    def old_sample_episode(self,n,length):\n",
    "        episodes = self.sample(n)\n",
    "        episode_samples = []\n",
    "        for episode in episodes:\n",
    "            start=None\n",
    "            sample = None\n",
    "            if (len(episode)- 1 - length) <= 0 :\n",
    "                sample = episode\n",
    "            else:\n",
    "                start = np.random.randint(0,len(episode)- 1 - length)\n",
    "                sample = episode[start:(start+length)]\n",
    "            episode_samples.append(sample)\n",
    "        return episode_samples\n",
    "    def sample_episode(self,n,length):\n",
    "        '''\n",
    "        return a numpy array of shape n,length,frame_shape\n",
    "        \n",
    "        sample time is O(n*length) eh okay. I think there is a better way by vectorizing. Overall this needs to be fixed up\n",
    "        '''\n",
    "        episodes = self.sample(n)\n",
    "        e_s = []\n",
    "        e_a = []\n",
    "        e_r = []\n",
    "        e_s_ = []\n",
    "        e_done = []\n",
    "        for episode in episodes:\n",
    "            s_list = []\n",
    "            a_list = []\n",
    "            r_list = []\n",
    "            s__list = []\n",
    "            done_list = []\n",
    "            if (len(episode)- 1 - length) <= 0 :\n",
    "                sample = episode\n",
    "            else:\n",
    "                start = np.random.randint(0,len(episode)- 1 - length)\n",
    "                sample = episode[start:(start+length)]\n",
    "            for s,a,r,s_,done in sample:\n",
    "                s_list.append(s)\n",
    "                a_list.append(a)\n",
    "                r_list.append(r)\n",
    "                s__list.append(s_)\n",
    "                done_list.append(done)\n",
    "            \n",
    "            pad_s = np.zeros((length - len(episode),)+ t[0].shape)\n",
    "            pad = np.zeros((length - len(episode)))\n",
    "\n",
    "            #print(\"pad {}\".format(pad_s.shape))\n",
    "            #print(\"arr {}\".format(np.array(s_list).shape))\n",
    "\n",
    "            s = np.concatenate((pad_s,np.array(s_list)))\n",
    "            a = np.concatenate((pad,np.array(a_list)))\n",
    "            r = np.concatenate((pad,np.array(r_list)))\n",
    "            s_ = np.concatenate((pad_s,np.array(s__list)))\n",
    "            done = np.concatenate((pad,np.array(done_list)))\n",
    "            \n",
    "            e_s.append(s)\n",
    "            e_a.append(a)\n",
    "            e_r.append(r)\n",
    "            e_s_.append(s_)\n",
    "            e_done.append(done)\n",
    "            '''\n",
    "            smp_s.append(np.concatenate((np.zeros((length - len(s_list))),np.array(s_list))))\n",
    "            smp_a.append(np.concatenate((np.zeros((length - len(a_list))),np.array(a_list))))\n",
    "            smp_r.append(np.concatenate((np.zeros((length - len(r_list))),np.array(r_list))))\n",
    "            smp_s_.append(np.concatenate((np.zeros((length - len(s__list))),np.array(s__list))))\n",
    "            smp_done.append(np.concatenate((np.zeros((length - len(done_list))),np.array(done_list))))\n",
    "            np.array(smp_s),np.array(smp_a),np.array(smp_r),np.array(smp_s_),np.array(smp_done)\n",
    "            '''\n",
    "            \n",
    "        return np.array(e_s),np.array(e_a),np.array(e_r),np.array(e_s_),np.array(e_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:38:08.989026Z",
     "start_time": "2018-01-15T02:38:08.910129Z"
    }
   },
   "outputs": [],
   "source": [
    "t_mem = Memory(1000)\n",
    "# generate test data\n",
    "for e in range(100):\n",
    "    episode = []\n",
    "    rand_num = np.random.randint(1,10)\n",
    "    for frame in range(rand_num):\n",
    "        s = np.ones((80,80,1))*frame\n",
    "        a = np.random.randint(0,6)\n",
    "        r = np.random.randint(0,100)\n",
    "        s_ = np.ones((80,80,1))*frame+1\n",
    "        done = False\n",
    "        if frame == (rand_num-1):\n",
    "            done = True\n",
    "        episode.append((s,a,r,s_,done))\n",
    "    t_mem.add(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:43:41.302447Z",
     "start_time": "2018-01-15T02:43:41.270419Z"
    }
   },
   "outputs": [],
   "source": [
    "ss,sa,sr,ss_,sdone = t_mem.sample_episode(32,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:39:06.825581Z",
     "start_time": "2018-01-15T02:39:06.796611Z"
    }
   },
   "outputs": [],
   "source": [
    "res = test_model.predict(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:39:11.302121Z",
     "start_time": "2018-01-15T02:39:11.294020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 6)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:34:17.764320Z",
     "start_time": "2018-01-15T02:34:17.634266Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad (2, 80, 80, 1)\n",
      "arr (8, 80, 80, 1)\n",
      "pad (2, 80, 80, 1)\n",
      "arr (8, 80, 80, 1)\n",
      "pad (5, 80, 80, 1)\n",
      "arr (5, 80, 80, 1)\n",
      "pad (6, 80, 80, 1)\n",
      "arr (4, 80, 80, 1)\n",
      "pad (4, 80, 80, 1)\n",
      "arr (6, 80, 80, 1)\n",
      "pad (8, 80, 80, 1)\n",
      "arr (2, 80, 80, 1)\n",
      "pad (7, 80, 80, 1)\n",
      "arr (3, 80, 80, 1)\n",
      "pad (5, 80, 80, 1)\n",
      "arr (5, 80, 80, 1)\n",
      "pad (3, 80, 80, 1)\n",
      "arr (7, 80, 80, 1)\n",
      "pad (7, 80, 80, 1)\n",
      "arr (3, 80, 80, 1)\n",
      "pad (6, 80, 80, 1)\n",
      "arr (4, 80, 80, 1)\n",
      "pad (4, 80, 80, 1)\n",
      "arr (6, 80, 80, 1)\n",
      "pad (7, 80, 80, 1)\n",
      "arr (3, 80, 80, 1)\n",
      "pad (3, 80, 80, 1)\n",
      "arr (7, 80, 80, 1)\n",
      "pad (9, 80, 80, 1)\n",
      "arr (1, 80, 80, 1)\n",
      "pad (7, 80, 80, 1)\n",
      "arr (3, 80, 80, 1)\n",
      "pad (9, 80, 80, 1)\n",
      "arr (1, 80, 80, 1)\n",
      "pad (6, 80, 80, 1)\n",
      "arr (4, 80, 80, 1)\n",
      "pad (9, 80, 80, 1)\n",
      "arr (1, 80, 80, 1)\n",
      "pad (3, 80, 80, 1)\n",
      "arr (7, 80, 80, 1)\n",
      "pad (6, 80, 80, 1)\n",
      "arr (4, 80, 80, 1)\n",
      "pad (1, 80, 80, 1)\n",
      "arr (9, 80, 80, 1)\n",
      "pad (4, 80, 80, 1)\n",
      "arr (6, 80, 80, 1)\n",
      "pad (6, 80, 80, 1)\n",
      "arr (4, 80, 80, 1)\n",
      "pad (3, 80, 80, 1)\n",
      "arr (7, 80, 80, 1)\n",
      "pad (3, 80, 80, 1)\n",
      "arr (7, 80, 80, 1)\n",
      "pad (2, 80, 80, 1)\n",
      "arr (8, 80, 80, 1)\n",
      "pad (6, 80, 80, 1)\n",
      "arr (4, 80, 80, 1)\n",
      "pad (6, 80, 80, 1)\n",
      "arr (4, 80, 80, 1)\n",
      "pad (2, 80, 80, 1)\n",
      "arr (8, 80, 80, 1)\n",
      "pad (5, 80, 80, 1)\n",
      "arr (5, 80, 80, 1)\n",
      "pad (5, 80, 80, 1)\n",
      "arr (5, 80, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "e_s = []\n",
    "e_a = []\n",
    "e_r = []\n",
    "e_s_ = []\n",
    "e_done = []\n",
    "for episode in episodes:\n",
    "    s_list = []\n",
    "    a_list = []\n",
    "    r_list = []\n",
    "    s__list = []\n",
    "    done_list = []\n",
    "    #get random location to start\n",
    "    if (len(episode)- 1 - length) <= 0 :\n",
    "        sample = episode\n",
    "    else:\n",
    "        start = np.random.randint(0,len(episode)- 1 - length)\n",
    "        sample = episode[start:(start+length)]\n",
    "    for s,a,r,s_,done in sample:\n",
    "        s_list.append(s)\n",
    "        a_list.append(a)\n",
    "        r_list.append(r)\n",
    "        s__list.append(s_)\n",
    "        done_list.append(done)\n",
    "    pad_s = np.zeros((length - len(episode),)+ t[0].shape)\n",
    "    pad = np.zeros((length - len(episode)))\n",
    "    \n",
    "    #print(\"pad {}\".format(pad_s.shape))\n",
    "    #print(\"arr {}\".format(np.array(s_list).shape))\n",
    "    \n",
    "    s = np.concatenate((pad_s,np.array(s_list)))\n",
    "    a = np.concatenate((pad,np.array(a_list)))\n",
    "    r = np.concatenate((pad,np.array(r_list)))\n",
    "    s_ = np.concatenate((pad_s,np.array(s__list)))\n",
    "    done = np.concatenate((pad,np.array(done_list)))\n",
    "    e_s.append(s)\n",
    "    e_a.append(a)\n",
    "    e_r.append(r)\n",
    "    e_s_.append(s_)\n",
    "    e_done.append(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:34:36.182719Z",
     "start_time": "2018-01-15T02:34:36.165531Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10, 80, 80, 1)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(e_s).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:27:12.543627Z",
     "start_time": "2018-01-15T02:27:12.534880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 80, 80, 1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_ = np.array(s__list)\n",
    "t_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:25:46.976291Z",
     "start_time": "2018-01-15T02:25:46.972846Z"
    }
   },
   "outputs": [],
   "source": [
    "padarr = np.zeros((length - 8,)+t[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:26:15.478099Z",
     "start_time": "2018-01-15T02:26:15.469160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 80, 80, 1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate((padarr,t))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-07T18:53:25.065179Z",
     "start_time": "2018-01-07T18:53:24.808121Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,stateCnt,actionCnt,length,capacity,epsilon,gamma,window_size,learning_rate,name,output_path=\"./logs\"):\n",
    "        inpt = (window_sz,) + stateCnt\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "        self.model = Network(inpt,actionCnt,window_size,learning_rate).build() # model\n",
    "        self.target_model = Network(inpt,actionCnt,window_size,learning_rate).build() # target model\n",
    "        self.writer = tf.summary.FileWriter(output_path)\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 1/10000\n",
    "        self.gamma = gamma\n",
    "        self.steps = 0\n",
    "        self.memory = Memory(capacity)\n",
    "        self.name = name\n",
    "        \n",
    "        self.loss_count = 0\n",
    "    def save_model(self,path):\n",
    "        self.model.save_weights(path)\n",
    "    def remember(self,sample):\n",
    "        self.memory.add(sample)\n",
    "    def save_scalar(self,step, name, value):\n",
    "        \"\"\"Save a scalar value to tensorboard.\n",
    "          Parameters\n",
    "          ----------\n",
    "          step: int\n",
    "            Training step (sets the position on x-axis of tensorboard graph.\n",
    "          name: str\n",
    "            Name of variable. Will be the name of the graph in tensorboard.\n",
    "          value: float\n",
    "            The value of the variable at this step.\n",
    "          writer: tf.FileWriter\n",
    "            The tensorboard FileWriter instance.\n",
    "          \"\"\"\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = float(value)\n",
    "        summary_value.tag = self.name + name\n",
    "        self.writer.add_summary(summary, step)\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    def act(self, s):\n",
    "        # Epsilon greedy action selection\n",
    "        s = s[None][None] # increase the rank of tensor to have a batch_size of 1 and length 1\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.actionCnt)\n",
    "        act_values = self.model.predict(s)\n",
    "        return np.argmax(act_values[0]) # returns action\n",
    "    def replay(self, batch_size,length):\n",
    "        #print(\"sample\")\n",
    "        batch = self.memory.old_sample_episode(batch_size,length) # tensor size batch_size,length,frame\n",
    "        #print(np.array(batch).size)\n",
    "        for mini in batch:\n",
    "            #print(\"reset\")\n",
    "            self.model.reset_states() # we do this because the RNN is stateful\n",
    "            self.target_model.reset_states()\n",
    "            \n",
    "            for s,a,r,s_,done in mini:\n",
    "                s = s[None][None]\n",
    "                s_ = s_[None][None]\n",
    "                #print(\"predict\")\n",
    "                target = self.model.predict(s)\n",
    "                if done:\n",
    "                    target[0][a] = r\n",
    "                else:\n",
    "                    a_ = self.model.predict(s_)[0]\n",
    "                    t = self.target_model.predict(s)[0]\n",
    "                    target[0][a] = r + self.gamma * t[np.argmax(a_)]\n",
    "                #print(\"fit\")\n",
    "                res = self.model.fit(s, target, epochs=1,batch_size=1, verbose=0,shuffle=False)\n",
    "                self.save_scalar(self.loss_count,\"Loss\",res.history['loss'][0])\n",
    "                self.loss_count +=1\n",
    "        '''     \n",
    "        for s, a, r, s_, done in minibatch:\n",
    "            state = state\n",
    "            next_state = next_state\n",
    "            \n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "            self.model.fit(state, target, epochs=1, verbose=0,shuffle=False)\n",
    "        '''\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:43:48.005286Z",
     "start_time": "2018-01-15T02:43:47.957845Z"
    }
   },
   "outputs": [],
   "source": [
    "target = test_model.predict(ss)\n",
    "a_ = test_model.predict(ss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:43:50.234425Z",
     "start_time": "2018-01-15T02:43:50.226197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.32822734,  0.07543106,  0.14359573,  0.18191774,  0.04122964,\n",
       "         0.03736237],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.32822734,  0.07543106,  0.14359573,  0.18191774,  0.04122964,\n",
       "         0.03736237],\n",
       "       [ 0.38206726,  0.08710781,  0.15901302,  0.20687872,  0.04828427,\n",
       "         0.03911546]], dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T02:45:04.004266Z",
     "start_time": "2018-01-15T02:45:03.999875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-29T07:36:50.138Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess(rgb_array,scale = 1/12):\n",
    "    frame_shape = rgb_array.shape\n",
    "    \n",
    "    frame = np.array(rgb_array)\n",
    "    gray_frame = np.dot(frame[...,:3],[0.299,0.587,0.114]).reshape((frame_shape[0],frame_shape[1]))\n",
    "    smaller = scimisc.imresize(gray_frame,scale,mode='L').astype('float64')\n",
    "    smaller /= 255.0\n",
    "    smaller = np.expand_dims(smaller,2) # convert to a 3D array of shape (height,width,grayscale)\n",
    "    smaller = np.reshape(smaller, [1, *(smaller.shape)])\n",
    "    return smaller.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-19T00:05:33.116500Z",
     "start_time": "2017-12-19T00:05:33.107652Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def render(obs,root,canvas):\n",
    "    obs = np.squeeze(obs,2)\n",
    "    image = Image.fromarray(obs.astype('int8'),mode='L')\n",
    "    photo = ImageTk.PhotoImage(image)\n",
    "    root.one = photo\n",
    "    canvas.delete(\"all\")\n",
    "    canvas.create_image(frame_height,frame_width, image=photo)\n",
    "    root.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "root = Tk()\n",
    "root_frame = Frame(root)\n",
    "canvas = Canvas(root_frame, borderwidth=0, highlightthickness=0, width=200, height=130, bg=\"black\" )\n",
    "root_frame.pack()\n",
    "canvas.pack()\n",
    "\n",
    "frame_height = 25\n",
    "frame_width = 35\n",
    "\n",
    "\n",
    "env = gym.make(\"MinecraftBasic-v0\")\n",
    "env.load_mission_file(\"./CliffWalking.xml\")\n",
    "env.init(videoResolution=[420,300],allowContinuousMovement=[\"move\", \"turn\", \"strafe\"])\n",
    "\n",
    "\n",
    "scale = 1/12 # scale image down by 1/12\n",
    "newshape = (env.video_height*scale,env.video_width*scale,1) # dimension of 1 for grayscale\n",
    "newshape = tuple(map(int,newshape))\n",
    "\n",
    "# the pre processor will adjust the observation space therefore we will edit the property of the environment to take the pre processor into accoutn\n",
    "env.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "shape=newshape)\n",
    "\n",
    "done = False\n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        env.reset()\n",
    "        while True:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            proc_obs = preprocess(obs)\n",
    "            \n",
    "            render(proc_obs,root_frame,canvas)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "    except:\n",
    "        root.destroy()\n",
    "        env.close()\n",
    "        raise\n",
    "env.close()\n",
    "root.destroy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T23:32:27.244253Z",
     "start_time": "2018-01-05T23:32:27.238751Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def update(x,y,handle,plot):\n",
    "    plot.data_source.data['x'] += [x]\n",
    "    plot.data_source.data['y'] += [y]\n",
    "    push_notebook(handle=handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T23:49:20.416109Z",
     "start_time": "2018-01-05T23:49:20.320487Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inferno = bokeh.palettes.Inferno9\n",
    "fig1 = figure(plot_width=400, plot_height=400,title=\"rewards\",\n",
    "                      x_axis_label=\"x\",\n",
    "                      y_axis_label=\"y\")\n",
    "rplot = fig1.line([],[],line_width=2)\n",
    "# make a grid\n",
    "handle1 = show(fig1, notebook_handle=True)\n",
    "\n",
    "reward_plot = {\"handle\":handle1,\"plot\":rplot}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-07T08:27:47.998959Z",
     "start_time": "2018-01-07T08:27:47.933920Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_env = gym.make(\"MinecraftCliffWalking1-v0\")\n",
    "pre_env.init(videoResolution=[400,400],allowContinuousMovement=[\"move\", \"turn\", \"strafe\"],observeGrid=[20,-1,20,20,-1,20],observeDistance=[4,45,12])\n",
    "env = MinecraftWrapper(pre_env,1/5,(41,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-29T07:28:40.603Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atari_env = gym.make('SpaceInvaders-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-07T18:54:21.740067Z",
     "start_time": "2018-01-07T18:54:21.667708Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(env,agent,episodes):\n",
    "    # play loop\n",
    "    batch_size = 100\n",
    "    for e in range(episodes):\n",
    "        R = [0.0]\n",
    "        agent.model.reset_states()\n",
    "        s = env.reset()\n",
    "        #s = preprocess(pre_s,1/5)\n",
    "        done = False\n",
    "        e_buffer = []\n",
    "        for t in itertools.count():\n",
    "            a = agent.act(s)\n",
    "\n",
    "            s_, r, done, info = env.step(a)\n",
    "            #s_ = preprocess(pre_s_,1/5)\n",
    "            #agent.remember((s,a,r,s_,done))\n",
    "            e_buffer.append((s,a,r,s_,done))\n",
    "            s = s_\n",
    "            R[-1] += r\n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "                agent.save_scalar(e,\"reward per episode\",R[-1])\n",
    "                #print(\"episode: {}/{}, score: {}, e: {}\".format(e, episodes, t, agent.epsilon))\n",
    "                #exp.metric(\"reward\",R[-1])\n",
    "                #update(e,R[-1],handle1,rplot)\n",
    "                R.append(0.0)\n",
    "                break\n",
    "        agent.remember(np.array(e_buffer))\n",
    "        #if e % 100 == 0:\n",
    "        #    agent.save_model(\"Exp00-CNN\")\n",
    "        if e % 4 == 0 and e > 1:\n",
    "            agent.replay(batch_size,20)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-07T18:54:37.642114Z",
     "start_time": "2018-01-07T18:54:35.099319Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda2/envs/casper/lib/python3.5/site-packages/ipykernel/__main__.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=6)`\n"
     ]
    }
   ],
   "source": [
    "stateCnt = env.observation_space.shape\n",
    "actionCnt = env.action_space.n\n",
    "length = 1\n",
    "mem = 10000\n",
    "epsilon = 1\n",
    "gamma =  0.99\n",
    "window_sz = 1\n",
    "lr = 0.001\n",
    "name = \"DRQN R3 - GRU100/\"\n",
    "agent = Agent(stateCnt,actionCnt,length,mem,epsilon,gamma,window_sz,lr,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-07T18:54:41.679Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(env,agent,1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T18:08:36.568723Z",
     "start_time": "2018-01-06T18:08:36.562721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = agent.memory.old_sample_episode(32,10) # tensor size batch_size,length,frame\n",
    "batch[2].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T18:10:47.201246Z",
     "start_time": "2018-01-06T18:10:47.193134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (1, 1, 78, 78, 64)        640       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (1, 1, 75, 75, 64)        65600     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (1, 1, 73, 73, 64)        36928     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (1, 1, 341056)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, 40)                   54575520  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, 6)                    246       \n",
      "=================================================================\n",
      "Total params: 54,678,934\n",
      "Trainable params: 54,678,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T19:15:14.192482Z",
     "start_time": "2018-01-06T19:15:00.875277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  1.28734303  0.194103  ]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  1.30741799]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9470193796151806\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  1.31043613  0.0521309   0.19755137]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9470122049038944\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  1.30918729  0.05212722  0.19755244]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.945347387749924\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [  1.38434231e-01  -1.00000000e+02   5.88199496e-02   4.83568281e-01\n",
      "   5.21271490e-02   1.97552457e-01]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  1.28734303  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   1.30704522  0.05233217  0.19749497]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9466466179534939\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  1.30930281  0.0521309   0.19755137]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9458789355362225\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  1.30308557  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.939245734955561\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ -1.00000000e+02   6.94979802e-02   5.88199496e-02   4.83568281e-01\n",
      "   5.21271490e-02   1.97552457e-01]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  1.28930151  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9488962438891083\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 1.3102423   0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9498436937217611\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  1.31501365  0.0521309   0.19755137]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9515897734550226\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  1.32068193  0.05212722  0.19755244]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9568421148315791\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 1.33048105  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9665848710329302\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  1.34428549  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9803816802349857\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  1.36042297  0.19755241]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9965181655296396\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422153  0.075168    0.06308176  0.45488104  0.05746615  0.20518148]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  1.37611639  0.48356828  0.05212714  0.19755241]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 1.0122114197894825\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422148  0.07516794  0.06308171  0.45488125  0.0574661   0.20518151]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  1.38631868]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 1.0224136589557253\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.1442215   0.07516795  0.0630817   0.45488131  0.05746609  0.20518149]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 1.3824805   0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 1.0185754550002022\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422147  0.07516793  0.0630817   0.45488131  0.05746609  0.20518149]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  1.28734303  0.194103  ]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target + tderr: [ 0.138631    0.06969203  0.0589791   0.48287073  1.30741799  0.19749497]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9470193796151806\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 1.3108592   0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9474353399066398\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  0.48356801  1.31312752  0.19755244]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9492876308296347\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 1.31782413  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.953927950966577\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  1.32554376]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.961639942291987\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ -1.00000000e+02   6.94979727e-02   5.88199347e-02   4.83568281e-01\n",
      "   5.21271378e-02   1.97552413e-01]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 1.28734303  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  1.30762768  0.48287073  0.05233217  0.19749497]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.947229153439993\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   1.31455016]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9511261904770963\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  1.32398617  0.48356801  0.05212722  0.19755244]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9601462494767203\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [  1.38434231e-01  -1.00000000e+02   5.88199496e-02   4.83568281e-01\n",
      "   5.21271490e-02   1.97552457e-01]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  1.28734303]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  1.30725145  0.48287073  0.05233217  0.19749497]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9468528646512652\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  1.31069744  0.48355547  0.0521309   0.19755137]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9472734793217542\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ -1.00000000e+02   6.94980621e-02   5.88200055e-02   4.83568013e-01\n",
      "   5.21272197e-02   1.97552443e-01]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  1.28734303  0.194103  ]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  1.30741799  0.48287073  0.05233217  0.19749497]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9470193796151806\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  1.31038702  0.0521309   0.19755137]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9469630610543069\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  1.3109237 ]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9470837775156251\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  0.05881995  0.48356828  1.30989754  0.19755246]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.946001299805786\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 1.31196725  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9480633814500782\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 1.31819737  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9542925366593864\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422153  0.075168    0.06308176  0.45488104  0.05746615  0.20518148]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  1.32928431  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9653792786937029\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422148  0.07516794  0.06308171  0.45488125  0.0574661   0.20518151]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02   4.83568281e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5.21271378e-02  -1.00000000e+02]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422148  0.07516794  0.06308171  0.45488125  0.0574661   0.20518151]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 1.28734303  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   1.30762768  0.05233217  0.19749497]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.947229153439993\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  1.31142128  0.0521309   0.19755137]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9479974123462633\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  1.31858838  0.48356801  0.05212722  0.19755244]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.954748546920449\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [  1.38434231e-01  -1.00000000e+02   5.88199496e-02   4.83568281e-01\n",
      "   5.21271490e-02   1.97552457e-01]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  1.28734303  0.06276181  0.194103  ]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 1.30733633  0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  1.31180453  0.48355547  0.0521309   0.19755137]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9483805927189028\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  1.31693196  0.05212722  0.19755244]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9530921406592443\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  1.32600081  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.962104623105135\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02  -1.00000000e+02\n",
      "   5.21271378e-02   1.97552413e-01]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  1.28734303  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  1.30704522  0.48287073  0.05233217  0.19749497]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9466466179534939\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 1.30620337  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9427794174283146\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  1.30280793  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9389681059764475\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  1.29811835  0.48356828  0.05212715  0.19755246]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9342221647571789\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02   4.83568281e-01\n",
      "   5.21271378e-02  -1.00000000e+02]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  1.28734303  0.44944486  0.06276181  0.194103  ]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    1.30733633  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  1.31007886  0.0521309   0.19755137]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9466549521271858\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  0.48356801  1.30667758  0.19755244]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9428377284814025\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  0.05881995  0.48356828  1.29938436  0.19755246]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9354881742133434\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02   4.83568281e-01\n",
      "  -1.00000000e+02   1.97552413e-01]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  1.28734303]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   0.48287073  1.30725145  0.19749497]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9468528646512652\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 1.31028461  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.946860664529539\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  0.48356801  1.31255078  0.19755244]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9487109769085645\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  1.31749868]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9536024717376068\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  1.32530999]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9614060833162204\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02   4.83568281e-01\n",
      "  -1.00000000e+02   1.97552413e-01]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  1.28734303  0.194103  ]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   1.30741799  0.05233217  0.19749497]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9470193796151806\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 1.31083584  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9474119122457821\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  1.31541371  0.05212722  0.19755244]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.951573836515428\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  0.05881995  1.32474685  0.05212715  0.19755246]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9608506218385012\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02   4.83568281e-01\n",
      "   5.21271378e-02  -1.00000000e+02]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  1.28734303  0.194103  ]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  1.30733633]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  0.48355547  1.31027675  0.19755137]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9468528646512652\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  1.3110193   0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9471794039248463\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  1.31006265]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9461664410848178\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  1.30564249]\n",
      "action: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmax next_action: 3\n",
      "reward: 0.941738677499424\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 1.29789197  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9339870993965883\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422153  0.075168    0.06308176  0.45488104  0.05746615  0.20518148]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ -1.00000000e+02   6.94979727e-02   5.88199347e-02   4.83568281e-01\n",
      "   5.21271378e-02   1.97552413e-01]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422153  0.075168    0.06308176  0.45488104  0.05746615  0.20518148]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 1.28734303  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   0.48287073  1.30762768  0.19749497]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.947229153439993\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  1.31145477  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9480308451315772\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  1.3147397   0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.950899812720282\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  1.31479406  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9508978234614449\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 1.310377    0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9464731570747923\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  1.30499625  0.48356828  0.05212714  0.19755241]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9410914402086965\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422153  0.075168    0.06308176  0.45488104  0.05746615  0.20518148]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  1.30266857  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9387636051488931\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422148  0.07516794  0.06308171  0.45488125  0.0574661   0.20518151]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  1.29786777  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9339626747420816\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.1442215   0.07516795  0.0630817   0.45488131  0.05746609  0.20518149]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02   4.83568281e-01\n",
      "   5.21271378e-02  -1.00000000e+02]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.1442215   0.07516795  0.0630817   0.45488131  0.05746609  0.20518149]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 1.28734303  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   1.30762768  0.05233217  0.19749497]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.947229153439993\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   1.31142056]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9479966035177689\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  0.48356801  1.31538296  0.19755244]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9515431413008261\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  1.3259114   0.48356828  0.05212715  0.19755246]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9620151678045462\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02   4.83568281e-01\n",
      "  -1.00000000e+02   1.97552413e-01]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  1.28734303]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  1.30725145]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9468528646512652\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   1.30970943]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9462855217842024\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [  1.38434321e-01   6.94980621e-02  -1.00000000e+02   4.83568013e-01\n",
      "   5.21272197e-02   1.97552443e-01]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  1.28669882]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9462935809594377\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   0.48287073  1.31003559  0.19749497]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.949637043973608\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  1.32312977  0.48355547  0.0521309   0.19755137]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9597058386439856\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  0.48356801  1.33747804  0.19755244]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.973638162565638\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  1.35312474  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9892284888308189\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  1.36731505  0.48356828  0.05212714  0.19755241]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 1.003411196704316\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  1.37457716  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 1.0106722951680567\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422153  0.075168    0.06308176  0.45488104  0.05746615  0.20518148]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 1.37136555  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 1.0074605475930911\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422148  0.07516794  0.06308171  0.45488125  0.0574661   0.20518151]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  1.36289978  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9989947116216952\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.1442215   0.07516795  0.0630817   0.45488131  0.05746609  0.20518149]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  1.34985924]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9859542437368612\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422147  0.07516793  0.0630817   0.45488131  0.05746609  0.20518149]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  1.28734303  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    1.30704522  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9466466179534939\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  1.30620503  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9427811062140965\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  1.29833603  0.48356801  0.05212722  0.19755244]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9344962079545626\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ -1.00000000e+02   6.94979802e-02   5.88199496e-02   4.83568281e-01\n",
      "   5.21271490e-02   1.97552457e-01]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  1.28734303  0.194103  ]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    1.30741799  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9470193796151806\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   1.31029868]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9468747404971414\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  1.30687988  0.48356801  0.05212722  0.19755244]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9430400063975835\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  0.05881995  1.30020523  0.05212715  0.19755246]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9363090733779804\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02   4.83568281e-01\n",
      "  -1.00000000e+02   1.97552413e-01]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  1.28734303  0.06276181  0.194103  ]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  1.30733633]\n",
      "action: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  1.30734313  0.48355547  0.0521309   0.19755137]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.943919211087426\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [  1.38434321e-01   6.94980621e-02   5.88200055e-02  -1.00000000e+02\n",
      "   5.21272197e-02   1.97552443e-01]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  1.28734303  0.06276181  0.194103  ]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 1.30733633  0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  1.310637    0.48355547  0.0521309   0.19755137]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9472130482007358\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  1.31483185  0.05212722  0.19755244]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9509919606943611\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  0.05881995  1.32303643  0.05212715  0.19755246]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9591402420332853\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02  -1.00000000e+02\n",
      "   5.21271378e-02   1.97552413e-01]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  1.28734303  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 1.30704522  0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9466466179534939\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  1.31031322  0.48355547  0.0521309   0.19755137]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.946889265182683\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  0.48356801  1.31381249  0.19755244]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9499725909158206\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  1.31866753  0.48356828  0.05212715  0.19755246]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9547713360840933\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  1.32416916  0.19755241]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9602652790449279\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  1.32896209]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9650573017248759\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422153  0.075168    0.06308176  0.45488104  0.05746615  0.20518148]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02  -1.00000000e+02\n",
      "   5.21271378e-02   1.97552413e-01]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422153  0.075168    0.06308176  0.45488104  0.05746615  0.20518148]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  1.28734303  0.194103  ]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   1.30741799  0.05233217  0.19749497]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9470193796151806\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   1.31179142]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9483674612847952\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  1.31038463  0.48356801  0.05212722  0.19755244]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9465447378217791\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target + tderr: [ 0.13843423  0.06949798  0.05881995  1.30776119  0.05212715  0.19755246]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9438649533126641\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  1.30381155  0.05212714  0.19755241]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.93990766159151\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02   4.83568281e-01\n",
      "   5.21271378e-02  -1.00000000e+02]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  1.28734303  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  1.30704522  0.48287073  0.05233217  0.19749497]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9466466179534939\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   1.30621636]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9427924834549347\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  1.30128741  0.05212722  0.19755244]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9374475029398355\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [  1.38434231e-01   6.94979802e-02   5.88199496e-02  -1.00000000e+02\n",
      "   5.21271490e-02   1.97552457e-01]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 1.28734303  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  1.30762768  0.48287073  0.05233217  0.19749497]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.947229153439993\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  1.31455171  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9511278188337385\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  0.48356801  1.31886065  0.19755244]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.955020811813531\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  1.31591916  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9520229011012907\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  1.30633855]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9424346763184545\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 1.30112886  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9372239875356708\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422153  0.075168    0.06308176  0.45488104  0.05746615  0.20518148]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  1.30143726]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9375322276703262\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422148  0.07516794  0.06308171  0.45488125  0.0574661   0.20518151]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  1.30867016  0.19755241]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9447651554738608\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.1442215   0.07516795  0.0630817   0.45488131  0.05746609  0.20518149]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 1.30685723  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9429522382432951\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422147  0.07516793  0.0630817   0.45488131  0.05746609  0.20518149]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 1.31832254  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9779172592030326\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  1.35216939  0.48287073  0.05233217  0.19749497]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9917708802978169\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  1.36976171  0.0521309   0.19755137]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 1.0063378149246138\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  0.48356801  1.38553381  0.19755244]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 1.0216939338046545\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  1.40284216]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 1.0389459270202823\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  1.41722119  0.19755241]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 1.053317343976052\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  1.43603992]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 1.0721350701492616\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422153  0.075168    0.06308176  0.45488104  0.05746615  0.20518148]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  1.44808328]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 1.0841783148074364\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422148  0.07516794  0.06308171  0.45488125  0.0574661   0.20518151]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  1.45417249  0.19755241]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 1.0902674387867983\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.1442215   0.07516795  0.0630817   0.45488131  0.05746609  0.20518149]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 1.46915078  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 1.1052457560911613\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422147  0.07516793  0.0630817   0.45488131  0.05746609  0.20518149]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 1.28734303  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  1.30762768]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.947229153439993\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   1.31129909]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.947875115178675\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  1.31716192  0.05212722  0.19755244]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9533220174009868\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [  1.38434231e-01   6.94979802e-02   5.88199496e-02  -1.00000000e+02\n",
      "   5.21271490e-02   1.97552457e-01]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 1.28734303  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    1.30762768  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.947229153439993\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  1.31041026  0.0521309   0.19755137]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9469862871650688\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 1.30779111  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "action: 0\n",
      "argmax next_action: 3\n",
      "reward: 0.9439512875825946\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  0.05881995  1.30515456  0.05212715  0.19755246]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9412583711401524\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  1.30559349  0.05212714  0.19755241]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9416896360463727\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422193  0.07516839  0.06308208  0.45487982  0.05746659  0.20518126]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  1.30998302  0.05212714  0.19755241]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9460781795487984\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422153  0.075168    0.06308176  0.45488104  0.05746615  0.20518148]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  1.31783319]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9539282239602159\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422148  0.07516794  0.06308171  0.45488125  0.0574661   0.20518151]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02   5.88199347e-02   4.83568281e-01\n",
      "  -1.00000000e+02   1.97552413e-01]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422148  0.07516794  0.06308171  0.45488125  0.0574661   0.20518151]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  1.28734303]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   1.30725145  0.05233217  0.19749497]\n",
      "action: 3\n",
      "argmax next_action: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.9468528646512652\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  1.3085413   0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9451173207209318\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [  1.38434321e-01  -1.00000000e+02   5.88200055e-02   4.83568013e-01\n",
      "   5.21272197e-02   1.97552443e-01]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  1.28734303  0.06276181  0.194103  ]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    1.30733633  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "action: 1\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  1.31008506  0.0521309   0.19755137]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9466611356412048\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  1.30705583]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.943215947593413\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [ 0.13843423  0.06949798  0.05881995  1.29694331  0.05212715  0.19755246]\n",
      "action: 3\n",
      "argmax next_action: 3\n",
      "reward: 0.9330471491117579\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "target + tderr: [  1.38434231e-01   6.94979727e-02  -1.00000000e+02   4.83568281e-01\n",
      "   5.21271378e-02   1.97552413e-01]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843423  0.06949797  0.05881993  0.48356828  0.05212714  0.19755241]\n",
      "behavioural [ 0.14422464  0.07517131  0.06308445  0.45487025  0.05746979  0.20517959]\n",
      "target [ 0.14771423  0.07921243  0.06676368  0.44944486  0.06276181  0.194103  ]\n",
      "target + tderr: [ 0.14771423  0.07921243  0.06676368  0.44944486  1.28734303  0.194103  ]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: 0.9469377736886901\n",
      "next_action [ 0.13986205  0.070917    0.0599833   0.4784835   0.05363334  0.19712088]\n",
      "behavioural [ 0.15228666  0.08425859  0.07051608  0.42550659  0.06783158  0.19960055]\n",
      "target [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  0.19749497]\n",
      "target + tderr: [ 0.138631    0.06969203  0.0589791   0.48287073  0.05233217  1.30741799]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9470193796151806\n",
      "next_action [ 0.13846093  0.06952428  0.05884152  0.48347369  0.05215491  0.19754465]\n",
      "behavioural [ 0.14546727  0.07650387  0.06417615  0.45049819  0.05894516  0.20440933]\n",
      "target [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   0.19755137]\n",
      "target + tderr: [ 0.13843787  0.06950153  0.05882287  0.48355547  0.0521309   1.31043613]\n",
      "action: 5\n",
      "argmax next_action: 3\n",
      "reward: 0.9470122049038944\n",
      "next_action [ 0.13843472  0.06949845  0.05882033  0.48356655  0.05212763  0.19755226]\n",
      "behavioural [ 0.14439327  0.07535081  0.06323156  0.4542799   0.05766767  0.20507681]\n",
      "target [ 0.13843432  0.06949806  0.05882001  0.48356801  0.05212722  0.19755244]\n",
      "target + tderr: [ 0.13843432  0.06949806  1.31041014  0.48356801  0.05212722  0.19755244]\n",
      "action: 2\n",
      "argmax next_action: 3\n",
      "reward: 0.9465702729984393\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n",
      "target [ 0.13843423  0.06949798  0.05881995  0.48356828  0.05212715  0.19755246]\n",
      "target + tderr: [  1.38434231e-01   6.94979802e-02   5.88199496e-02   4.83568281e-01\n",
      "  -1.00000000e+02   1.97552457e-01]\n",
      "action: 4\n",
      "argmax next_action: 3\n",
      "reward: -100.0\n",
      "next_action [ 0.13843426  0.069498    0.05881996  0.48356816  0.05212716  0.19755244]\n",
      "behavioural [ 0.14424479  0.07519273  0.06310202  0.45479983  0.0574934   0.20516732]\n"
     ]
    }
   ],
   "source": [
    "for mini in batch:\n",
    "    #print(\"reset\")\n",
    "    agent.model.reset_states() # we do this because the RNN is stateful\n",
    "    agent.target_model.reset_states()\n",
    "\n",
    "    for s,a,r,s_,done in mini:\n",
    "        s = s[None][None]\n",
    "        s_ = s_[None][None]\n",
    "        #print(\"predict\")\n",
    "        target = agent.model.predict(s)[0]\n",
    "        #print(\"target {}\".format(target))\n",
    "        if done:\n",
    "            target[a] = r\n",
    "        else:\n",
    "            a_ = agent.model.predict(s_)[0]\n",
    "            t = agent.target_model.predict(s)[0]\n",
    "            target[a] = r + agent.gamma * t[np.argmax(a_)]\n",
    "        #print(\"fit\")\n",
    "        '''\n",
    "        print(\"target + tderr: {}\".format(target))\n",
    "        print(\"action: {}\".format(a))\n",
    "        print(\"argmax next_action: {}\".format(np.argmax(a_)))\n",
    "        print(\"reward: {}\".format(r))\n",
    "        print(\"next_action {}\".format(a_))\n",
    "        print(\"behavioural {}\".format(t))\n",
    "        '''\n",
    "        res = self.model.fit(s, target, epochs=1,batch_size=1, verbose=0,shuffle=False)\n",
    "        self.save_scalar(self.loss_count,\"Loss\",res.history['loss'][0])\n",
    "        self.loss_count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T19:03:32.356556Z",
     "start_time": "2018-01-06T19:03:32.331317Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "contains() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-67270a5d4e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: contains() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "env.action_space.contains()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T05:20:50.663878Z",
     "start_time": "2018-01-03T05:20:50.660147Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_buffer = PrioritizedReplayBuffer(50,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T05:21:03.170555Z",
     "start_time": "2018-01-03T05:21:03.164046Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_buffer.add(1,2,3,4,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T05:26:27.128528Z",
     "start_time": "2018-01-03T05:26:27.115947Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_buffer.sample(1,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T05:23:50.050184Z",
     "start_time": "2018-01-03T05:23:50.039616Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir(test_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T22:19:53.236742Z",
     "start_time": "2018-01-03T22:19:53.228842Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_buffer.sample(3,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T22:19:37.186459Z",
     "start_time": "2018-01-03T22:19:37.183087Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_buffer.update_priorities([0],[0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_buffer.update_priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T23:26:00.948934Z",
     "start_time": "2018-01-03T23:26:00.836569Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        test_model = Sequential()\n",
    "        # Lets try a CNN to take screen as input\n",
    "        # batch size is 64, 320x240 video RGB channels with an extra channel for depth\n",
    "        test_model.add(layers.Conv2D(32,(8,8),input_shape=(80,80,1),activation='relu'))\n",
    "        test_model.add(layers.Conv2D(64,(4,4),activation='relu'))\n",
    "        \n",
    "        test_model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "        test_model.add(layers.Permute((3, 2, 1)))\n",
    "        test_model.add(layers.Reshape((68, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-03T23:25:00.014621Z",
     "start_time": "2018-01-03T23:25:00.005476Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T00:56:08.912137Z",
     "start_time": "2018-01-04T00:56:08.871547Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.TimeDistributed(layers.Conv2D(64, (3, 3)),\n",
    "                          input_shape=(10, 299, 299, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:casper]",
   "language": "python",
   "name": "conda-env-casper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
