{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T21:25:16.470336Z",
     "start_time": "2017-12-06T21:25:15.859812Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb;\n",
    "import scipy.misc as scimisc\n",
    "\n",
    "from tkinter import *\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "from PIL import Image\n",
    "\n",
    "import MalmoPython\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output,display\n",
    "import logging\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is just used to craft the gym environment for Minecraft Malmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T21:25:28.251644Z",
     "start_time": "2017-12-06T21:25:16.565582Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_minecraft\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import baselines.common.tf_util as U\n",
    "\n",
    "from baselines import logger\n",
    "from baselines import deepq\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T21:25:28.366693Z",
     "start_time": "2017-12-06T21:25:28.353659Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(rgb_array,scale = 1/12):\n",
    "    frame_shape = rgb_array.shape\n",
    "    \n",
    "    frame = np.array(rgb_array)\n",
    "    gray_frame = np.dot(frame[...,:3],[0.299,0.587,0.114]).reshape((frame_shape[0],frame_shape[1]))\n",
    "    smaller = scimisc.imresize(gray_frame,scale,mode='L').astype('float64')\n",
    "    smaller = np.expand_dims(smaller,2) # convert to a 3D array of shape (height,width,grayscale)\n",
    "    return smaller\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T21:25:28.487445Z",
     "start_time": "2017-12-06T21:25:28.478378Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def render(obs,root,canvas):\n",
    "    obs = np.squeeze(obs,2)\n",
    "    image = Image.fromarray(obs.astype('int8'),mode='L')\n",
    "    photo = ImageTk.PhotoImage(image)\n",
    "    root.one = photo\n",
    "    canvas.delete(\"all\")\n",
    "    canvas.create_image(frame_height,frame_width, image=photo)\n",
    "    root.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T20:46:20.078406Z",
     "start_time": "2017-12-06T20:46:13.723994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "root = Tk()\n",
    "root_frame = Frame(root)\n",
    "canvas = Canvas(root_frame, borderwidth=0, highlightthickness=0, width=200, height=130, bg=\"black\" )\n",
    "root_frame.pack()\n",
    "canvas.pack()\n",
    "\n",
    "frame_height = 25\n",
    "frame_width = 35\n",
    "\n",
    "\n",
    "env = gym.make(\"MinecraftBasic-v0\")\n",
    "env.load_mission_file(\"./CliffWalking.xml\")\n",
    "env.init(videoResolution=[420,300],allowContinuousMovement=[\"move\", \"turn\", \"strafe\"])\n",
    "\n",
    "\n",
    "scale = 1/12 # scale image down by 1/12\n",
    "newshape = (env.video_height*scale,env.video_width*scale,1) # dimension of 1 for grayscale\n",
    "newshape = tuple(map(int,newshape))\n",
    "\n",
    "# the pre processor will adjust the observation space therefore we will edit the property of the environment to take the pre processor into accoutn\n",
    "env.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "shape=newshape)\n",
    "\n",
    "done = False\n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        env.reset()\n",
    "        while True:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            proc_obs = preprocess(obs)\n",
    "            \n",
    "            render(proc_obs,root_frame,canvas)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "    except:\n",
    "        root.destroy()\n",
    "        env.close()\n",
    "        raise\n",
    "env.close()\n",
    "root.destroy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T21:27:43.016198Z",
     "start_time": "2017-12-06T21:27:43.006640Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(inpt, num_actions, scope, reuse=False):\n",
    "    \"\"\"This model takes as input an observation and returns values of all actions.\"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = inpt\n",
    "        out = layers.fully_connected(out, num_outputs=64, activation_fn=tf.nn.tanh)\n",
    "        out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "        return out\n",
    "qmodel = deepq.models.cnn_to_mlp(\n",
    "        convs=[(32, 8, 4), (64, 4, 2), (64, 3, 1)],\n",
    "        hiddens=[256],\n",
    "        dueling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-06T21:27:49.054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/baselines/baselines/deepq/build_graph.py:366: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "U.reset()\n",
    "with U.make_session(2):\n",
    "    # Create the environment\n",
    "    env = gym.make(\"MinecraftBasic-v0\")\n",
    "    env.init(videoResolution=[420,300],allowContinuousMovement=[\"move\", \"turn\", \"strafe\"])\n",
    "    env.load_mission_file(\"./CliffWalking.xml\")\n",
    "    scale = 1/12 # scale image down by 1/12\n",
    "    newshape = (env.video_height*scale,env.video_width*scale,1) # dimension of 1 for grayscale\n",
    "    newshape = tuple(map(int,newshape))\n",
    "\n",
    "    # the pre processor will adjust the observation space therefore we will edit the property of the environment to take the pre processor into accoutn\n",
    "    env.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "    shape=newshape)\n",
    "    # Create all the functions necessary to train the model\n",
    "    act, train, update_target, debug = deepq.build_train(\n",
    "        make_obs_ph=lambda name: U.BatchInput(env.observation_space.shape, name=name),\n",
    "        q_func=qmodel,\n",
    "        num_actions=env.action_space.n,\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "    )\n",
    "    # Create the replay buffer\n",
    "    replay_buffer = ReplayBuffer(50000)\n",
    "    # Create the schedule for exploration starting from 1 (every action is random) down to\n",
    "    # 0.02 (98% of actions are selected according to values predicted by the model).\n",
    "    exploration = LinearSchedule(schedule_timesteps=10000, initial_p=1.0, final_p=0.02)\n",
    "\n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    update_target()\n",
    "\n",
    "    episode_rewards = [0.0]\n",
    "    preobs = env.reset()\n",
    "    obs = preprocess(preobs)\n",
    "    for t in itertools.count():\n",
    "        # Take action and update exploration to the newest value\n",
    "        action = act(obs[None], update_eps=exploration.value(t))[0]\n",
    "        new_preobs, rew, done, _ = env.step(action)\n",
    "        new_obs = preprocess(new_preobs)\n",
    "        # Store transition in the replay buffer.\n",
    "        replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "        obs = new_obs\n",
    "\n",
    "        episode_rewards[-1] += rew\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            episode_rewards.append(0)\n",
    "\n",
    "        is_solved = t > 100 and np.mean(episode_rewards[-101:-1]) >= 200\n",
    "        if is_solved:\n",
    "            # Show off the result\n",
    "            env.render()\n",
    "        else:\n",
    "            # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "            if t > 1000:\n",
    "                obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(32)\n",
    "                train(obses_t, actions, rewards, obses_tp1, dones, np.ones_like(rewards))\n",
    "            # Update target network periodically.\n",
    "            if t % 1000 == 0:\n",
    "                update_target()\n",
    "\n",
    "        if done and len(episode_rewards) % 10 == 0:\n",
    "            logger.record_tabular(\"steps\", t)\n",
    "            logger.record_tabular(\"episodes\", len(episode_rewards))\n",
    "            logger.record_tabular(\"mean episode reward\", round(np.mean(episode_rewards[-101:-1]), 1))\n",
    "            logger.record_tabular(\"% time spent exploring\", int(100 * exploration.value(t)))\n",
    "            logger.dump_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T21:24:44.871765Z",
     "start_time": "2017-12-06T21:24:44.794435Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown commandhandler MissionQuit\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MinecraftBasic-v0\")\n",
    "env.load_mission_file(\"./CliffWalking.xml\")\n",
    "env.init(videoResolution=[40,40])\n",
    "\n",
    "\n",
    "#scale = 1/12 # scale image down by 1/12\n",
    "#newshape = (env.video_height*scale,env.video_width*scale,1) # dimension of 1 for grayscale\n",
    "#newshape = tuple(map(int,newshape))\n",
    "\n",
    "# the pre processor will adjust the observation space therefore we will edit the property of the environment to take the pre processor into accoutn\n",
    "#env.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "#shape=newshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to override env step to preprocess and calculate reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-06T21:24:46.874844Z",
     "start_time": "2017-12-06T21:24:46.870167Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-06T21:24:49.668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/baselines/baselines/deepq/build_graph.py:366: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n"
     ]
    }
   ],
   "source": [
    "    act = deepq.learn(\n",
    "        env,\n",
    "        q_func=qmodel,\n",
    "        lr=1e-4,\n",
    "        max_timesteps=1000,\n",
    "        buffer_size=10000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_final_eps=0.01,\n",
    "        train_freq=4,\n",
    "        learning_starts=10000,\n",
    "        target_network_update_freq=1000,\n",
    "        gamma=0.99,\n",
    "        prioritized_replay=True\n",
    ")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:casper]",
   "language": "python",
   "name": "conda-env-casper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
