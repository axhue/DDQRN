{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:28.965612Z",
     "start_time": "2018-01-23T18:31:19.605720Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "21eb868f-3165-4224-b1b0-3d2040a59b82"
    }
   },
   "outputs": [],
   "source": [
    "import pdb;\n",
    "import scipy.misc as scimisc\n",
    "\n",
    "from tkinter import *\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "from PIL import Image\n",
    "\n",
    "import MalmoPython\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output,display\n",
    "import logging\n",
    "import math\n",
    "\n",
    "\n",
    "import gym\n",
    "import gym_minecraft\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import baselines.common.tf_util as U\n",
    "\n",
    "from baselines import logger\n",
    "from baselines import deepq\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer,PrioritizedReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:29.393376Z",
     "start_time": "2018-01-23T18:31:28.983498Z"
    },
    "nbpresent": {
     "id": "6394dbd2-aa3d-438b-bddc-de0c2b263ff0"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"581b8587-73cf-42bf-a592-969dfb22ea13\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(`.${CLASS_NAME.split(' ')[0]}`);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"581b8587-73cf-42bf-a592-969dfb22ea13\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"581b8587-73cf-42bf-a592-969dfb22ea13\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '581b8587-73cf-42bf-a592-969dfb22ea13' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"581b8587-73cf-42bf-a592-969dfb22ea13\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"581b8587-73cf-42bf-a592-969dfb22ea13\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"581b8587-73cf-42bf-a592-969dfb22ea13\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '581b8587-73cf-42bf-a592-969dfb22ea13' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"581b8587-73cf-42bf-a592-969dfb22ea13\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, push_notebook, show\n",
    "from bokeh.driving import linear\n",
    "from bokeh.layouts import row,gridplot\n",
    "from IPython.display import clear_output,display\n",
    "import bokeh\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:29.536032Z",
     "start_time": "2018-01-23T18:31:29.410520Z"
    },
    "nbpresent": {
     "id": "7073f323-45ba-43d8-b4fa-0f20a917264d"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential,model_from_json,Model\n",
    "from keras.layers import Conv2D,LSTM,GRU,TimeDistributed,Dense,Flatten,Input,Lambda\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:29.556736Z",
     "start_time": "2018-01-23T18:31:29.553538Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "7c192fe1-b95e-4b2c-a11e-da3e9884f788"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_minecraft\n",
    "from MinecraftGym import MinecraftWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "2ec6aafa-1163-4bca-87f2-2a31e99c5214"
    }
   },
   "source": [
    "## Epsilon Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:29.578946Z",
     "start_time": "2018-01-23T18:31:29.574162Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "4369636f-c1de-48c6-98f8-04c1ebf8313e"
    }
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\"Base class representing an MDP policy.\n",
    "\n",
    "    Policies are used by the agent to choose actions.\n",
    "\n",
    "    Policies are designed to be stacked to get interesting behaviors\n",
    "    of choices. For instances in a discrete action space the lowest\n",
    "    level policy may take in Q-Values and select the action index\n",
    "    corresponding to the largest value. If this policy is wrapped in\n",
    "    an epsilon greedy policy then with some probability epsilon, a\n",
    "    random action will be chosen.\n",
    "    \"\"\"\n",
    "\n",
    "    def select_action(self, **kwargs):\n",
    "        \"\"\"Used by agents to select actions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Any:\n",
    "          An object representing the chosen action. Type depends on\n",
    "          the hierarchy of policy instances.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This method should be overriden.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:29.618200Z",
     "start_time": "2018-01-23T18:31:29.596373Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "b604e303-7495-4a01-8636-df75fa9a576e"
    }
   },
   "outputs": [],
   "source": [
    "class LinearDecayGreedyEpsilonPolicy(Policy):\n",
    "    \"\"\"Policy with a parameter that decays linearly.\n",
    "\n",
    "    Like GreedyEpsilonPolicy but the epsilon decays from a start value\n",
    "    to an end value over k steps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_value: int, float\n",
    "      The initial value of the parameter\n",
    "    end_value: int, float\n",
    "      The value of the policy at the end of the decay.\n",
    "    num_steps: int\n",
    "      The number of steps over which to decay the value.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start_value, end_value, num_steps):  # noqa: D102\n",
    "        self.start_value = start_value\n",
    "        self.decay_rate = float(end_value - start_value) / num_steps\n",
    "        self.end_value = end_value\n",
    "        self.step = 0\n",
    "        self.epsilon = start_value\n",
    "\n",
    "    def update(self,is_training = True):\n",
    "        \"\"\"Decay parameter and select action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        q_values: np.array\n",
    "          The Q-values for each action.\n",
    "        is_training: bool, optional\n",
    "          If true then parameter will be decayed. Defaults to true.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Any:\n",
    "          Selected action.\n",
    "        \"\"\"\n",
    "        epsilon = self.start_value\n",
    "        if is_training:\n",
    "            epsilon += self.decay_rate * self.step\n",
    "            self.step += 1\n",
    "        self.epsilon = max(epsilon, self.end_value)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Start the decay over at the start value.\"\"\"\n",
    "        self.step = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "ecf79551-5703-4d22-85ad-83ea56d2ec06"
    }
   },
   "source": [
    "## Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:29.659961Z",
     "start_time": "2018-01-23T18:31:29.635680Z"
    },
    "code_folding": [],
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "89d86297-2eed-43a8-b9b5-d506dff95ecc"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"Preprocessor base class.\n",
    "\n",
    "    This is a suggested interface for the preprocessing steps. \n",
    "\n",
    "    Preprocessor can be used to perform some fixed operations on the\n",
    "    raw state from an environment. For example, in ConvNet based\n",
    "    networks which use image as the raw state, it is often useful to\n",
    "    convert the image to greyscale or downsample the image.\n",
    "\n",
    "    Preprocessors are implemented as class so that they can have\n",
    "    internal state. This can be useful for things like the\n",
    "    AtariPreproccessor which maxes over k frames.\n",
    "\n",
    "    If you're using internal states, such as for keeping a sequence of\n",
    "    inputs like in Atari, you should probably call reset when a new\n",
    "    episode begins so that state doesn't leak in from episode to\n",
    "    episode.\n",
    "    \"\"\"\n",
    "\n",
    "    def process_state_for_network(self, state):\n",
    "        \"\"\"Preprocess the given state before giving it to the network.\n",
    "\n",
    "        Should be called just before the action is selected.\n",
    "\n",
    "        This is a different method from the process_state_for_memory\n",
    "        because the replay memory may require a different storage\n",
    "        format to reduce memory usage. For example, storing images as\n",
    "        uint8 in memory is a lot more efficient thant float32, but the\n",
    "        networks work better with floating point images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: np.ndarray\n",
    "          Generally a numpy array. A single state from an environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        processed_state: np.ndarray\n",
    "          Generally a numpy array. The state after processing. Can be\n",
    "          modified in anyway.\n",
    "        \"\"\"\n",
    "        return state\n",
    "\n",
    "    def process_state_for_memory(self, state):\n",
    "        \"\"\"Preprocess the given state before giving it to the replay memory.\n",
    "\n",
    "        Should be called just before appending this to the replay memory.\n",
    "\n",
    "        This is a different method from the process_state_for_network\n",
    "        because the replay memory may require a different storage\n",
    "        format to reduce memory usage. For example, storing images as\n",
    "        uint8 in memory and the network expecting images in floating\n",
    "        point.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: np.ndarray\n",
    "          A single state from an environmnet. Generally a numpy array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        processed_state: np.ndarray\n",
    "          Generally a numpy array. The state after processing. Can be\n",
    "          modified in any manner.\n",
    "        \"\"\"\n",
    "        return state\n",
    "\n",
    "    def process_batch(self, samples):\n",
    "        \"\"\"Process batch of samples.\n",
    "\n",
    "        If your replay memory storage format is different than your\n",
    "        network input, you may want to apply this function to your\n",
    "        sampled batch before running it through your update function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples: list(tensorflow_rl.core.Sample)\n",
    "          List of samples to process\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        processed_samples: list(tensorflow_rl.core.Sample)\n",
    "          Samples after processing. Can be modified in anyways, but\n",
    "          the list length will generally stay the same.\n",
    "        \"\"\"\n",
    "        return samples\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"Process the reward.\n",
    "\n",
    "        Useful for things like reward clipping. The Atari environments\n",
    "        from DQN paper do this. Instead of taking real score, they\n",
    "        take the sign of the delta of the score.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward: float\n",
    "          Reward to process\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        processed_reward: float\n",
    "          The processed reward\n",
    "        \"\"\"\n",
    "        return reward\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset any internal state.\n",
    "\n",
    "        Will be called at the start of every new episode. Makes it\n",
    "        possible to do history snapshots.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:29.714829Z",
     "start_time": "2018-01-23T18:31:29.679235Z"
    },
    "code_folding": [
     6,
     14,
     31
    ],
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "da6bfe6a-4ee4-4856-be91-af43a9686319"
    }
   },
   "outputs": [],
   "source": [
    "class AtariPreprocessor(Preprocessor):\n",
    "    \"\"\"Converts images to greyscale and downscales.\n",
    "\n",
    "    Based on the preprocessing step described in:\n",
    "\n",
    "    @article{mnih15_human_level_contr_throug_deep_reinf_learn,\n",
    "    author =\t {Volodymyr Mnih and Koray Kavukcuoglu and David\n",
    "                  Silver and Andrei A. Rusu and Joel Veness and Marc\n",
    "                  G. Bellemare and Alex Graves and Martin Riedmiller\n",
    "                  and Andreas K. Fidjeland and Georg Ostrovski and\n",
    "                  Stig Petersen and Charles Beattie and Amir Sadik and\n",
    "                  Ioannis Antonoglou and Helen King and Dharshan\n",
    "                  Kumaran and Daan Wierstra and Shane Legg and Demis\n",
    "                  Hassabis},\n",
    "    title =\t {Human-Level Control Through Deep Reinforcement\n",
    "                  Learning},\n",
    "    journal =\t {Nature},\n",
    "    volume =\t 518,\n",
    "    number =\t 7540,\n",
    "    pages =\t {529-533},\n",
    "    year =\t 2015,\n",
    "    doi =        {10.1038/nature14236},\n",
    "    url =\t {http://dx.doi.org/10.1038/nature14236},\n",
    "    }\n",
    "\n",
    "    You may also want to max over frames to remove flickering. Some\n",
    "    games require this (based on animations and the limited sprite\n",
    "    drawing capabilities of the original Atari).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    new_size: 2 element tuple\n",
    "      The size that each image in the state should be scaled to. e.g\n",
    "      (84, 84) will make each image in the output have shape (84, 84).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,new_state_shape):\n",
    "        self.new_shape = new_state_shape\n",
    "    def process_state_for_memory(self, state):\n",
    "        \"\"\"Scale, convert to greyscale and store as uint8.\n",
    "\n",
    "        We don't want to save floating point numbers in the replay\n",
    "        memory. We get the same resolution as uint8, but use a quarter\n",
    "        to an eigth of the bytes (depending on float32 or float64)\n",
    "\n",
    "        We recommend using the Python Image Library (PIL) to do the\n",
    "        image conversions.\n",
    "        \"\"\"\n",
    "        img = Image.fromarray(state).convert('L').resize(self.new_shape[:2], Image.BILINEAR)\n",
    "        state = np.array(img)\n",
    "        return np.expand_dims(state,-1)\n",
    "    def process_for_network(self, state):\n",
    "        \"\"\"Scale, convert to greyscale and store as float32.\n",
    "\n",
    "        Basically same as process state for memory, but this time\n",
    "        outputs float32 images.\n",
    "        \"\"\"\n",
    "        state = np.float32(state / 255.0)\n",
    "        return state\n",
    "\n",
    "    def process_batch(self, samples):\n",
    "        \"\"\"The batches from replay memory will be uint8, convert to float32.\n",
    "\n",
    "        Same as process_state_for_network but works on a batch of\n",
    "        samples from the replay memory. Meaning you need to convert\n",
    "        both state and next state values.\n",
    "        \"\"\"\n",
    "        return np.float32(samples / 255.0)\n",
    "            \n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"Clip reward between -1 and 1.\"\"\"\n",
    "        #return np.clip(reward, -1, 1) \n",
    "    \n",
    "    def reset(self):\n",
    "        self.last_state = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "20cc2f69-169c-487f-b7cb-831d4ac30817"
    }
   },
   "source": [
    "## Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:29.916818Z",
     "start_time": "2018-01-23T18:31:29.734802Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "29a1fd90-66aa-40ce-b601-b841593ae45d"
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self,stateCnt,actionCnt,recurrent,mode,learning_rate):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "        self.learning_rate = learning_rate\n",
    "        #self.batch_size = batch_size\n",
    "        \n",
    "        self.recurrent = recurrent\n",
    "        self.mode = mode\n",
    "        \n",
    "    def build(self):\n",
    "        model = Sequential()\n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3)),\n",
    "                          input_shape=self.stateCnt,batch_size=self.batch_size))\n",
    "        \n",
    "        #model.add(layers.TimeDistributed(layers.Conv2D(32,(8,8),input_shape=self.stateCnt,activation='relu')))\n",
    "        conv2 = layers.Conv2D(64,(4,4),activation='relu')\n",
    "        conv3 = layers.Conv2D(64,(3,3),activation='relu')\n",
    "        model.add(layers.TimeDistributed(conv2))\n",
    "        model.add(layers.TimeDistributed(conv3))\n",
    "        model.add(layers.TimeDistributed(layers.Flatten()))\n",
    "        #model.add(layers.Flatten())\n",
    "        #model.add(layers.Permute((0,2,1)))\n",
    "        #self.add(Reshape(input_width, num_filters))\n",
    "        model.add(layers.GRU(units=70,stateful=True))\n",
    "        #model.add(layers.Dense(256,activation='relu')\n",
    "        model.add(layers.Dense(output_dim=self.actionCnt))\n",
    "        \n",
    "        model.compile(loss=self._huber_loss,optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    def build2(self):\n",
    "        inpt = Input(shape = self.stateCnt, name = \"input\")\n",
    "        \n",
    "        if self.mode == \"linear\":\n",
    "            \n",
    "            flatten_hidden = Flatten(name = \"flatten\")(input_data)\n",
    "            output = Dense(num_actions, name = \"output\")(flatten_hidden)\n",
    "        else:\n",
    "            if self.recurrent:\n",
    "                # shape should be (timesteps,height,width,color)\n",
    "                conv1 = TimeDistributed(Conv2D(32, (8, 8), strides = 4, activation = \"relu\", name = \"conv1\"))(inpt)\n",
    "                conv2 = TimeDistributed(Conv2D(64, (4, 4), strides = 2, activation = \"relu\", name = \"conv2\"))(conv1)\n",
    "                conv3 = TimeDistributed(Conv2D(64, (3, 3), strides = 1, activation = \"relu\", name = \"conv3\"))(conv2)\n",
    "                flatten_hidden = TimeDistributed(Flatten())(conv3)\n",
    "                hidden_input = TimeDistributed(Dense(512, activation = 'relu', name = 'flat_to_512')) (flatten_hidden)\n",
    "                context = GRU(512, return_sequences=False, stateful=False) (hidden_input)\n",
    "                \n",
    "            if self.mode == \"dqn\":\n",
    "                h4 = Dense(512, activation='relu', name = \"fc\")(context)\n",
    "                output = Dense(num_actions, name = \"output\")(h4)\n",
    "            elif self.mode == \"duel\":\n",
    "                value_hidden = Dense(512, activation = 'relu', name = 'value_fc')(context)\n",
    "                value = Dense(1, name = \"value\")(value_hidden)\n",
    "                \n",
    "                action_hidden = Dense(512, activation = 'relu', name = 'action_fc')(context)\n",
    "                action = Dense(self.actionCnt, name = \"action\")(action_hidden)\n",
    "                \n",
    "                action_mean = Lambda(lambda x: K.mean(x, axis = 1, keepdims = True), name = 'action_mean')(action) \n",
    "                output = Lambda(lambda x: x[0] + x[1] - x[2], name = 'output')([action, value, action_mean])\n",
    "        model = Model(inputs = inpt, outputs = output)\n",
    "        model.compile(loss=self._huber_loss,optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "            \n",
    "        \n",
    "    def _huber_loss(self, target, prediction):\n",
    "        # sqrt(1+error^2)-1\n",
    "        error = prediction - target\n",
    "        return K.mean(K.sqrt(1+K.square(error))-1, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "2a48d25d-ea4e-4339-9eab-8e3cb59b6a9c"
    }
   },
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:29.958224Z",
     "start_time": "2018-01-23T18:31:29.938922Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "883b5a10-c77c-47e9-9f09-c5088e06248a"
    }
   },
   "outputs": [],
   "source": [
    "class ShortMemory():\n",
    "    def __init__(self,hist_len,state_dim):\n",
    "        self.history_length = hist_len\n",
    "        self.state_dim = state_dim\n",
    "        self.mem_hist = np.zeros((hist_len,) + state_dim , dtype = np.float32)\n",
    "        self.current = 0\n",
    "    def add(self,state):\n",
    "        self.mem_hist[self.current % self.history_length] = state\n",
    "        self.current += 1\n",
    "    def get(self):\n",
    "        '''\n",
    "        This function provides the recent history of length history_length.\n",
    "        The sample in the beginning will be padded at the beginning. (0,0,0..data)\n",
    "        '''\n",
    "        return self.mem_hist[::-1]\n",
    "    \n",
    "    def forget(self):\n",
    "        self.mem_hist = np.zeros((self.history_length,) + self.state_dim, dtype = np.float32)\n",
    "        self.current = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:30.121271Z",
     "start_time": "2018-01-23T18:31:29.982088Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "bb1e39bd-fade-4967-a887-72616013cea1"
    }
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self,capacity,hist_len,s_dim):\n",
    "        '''\n",
    "        capacity: how many episodes to store?\n",
    "        hist_len: what is the history length of each episode?\n",
    "        s_dim: the size of your state in a tuple ex. (80,80,1) \n",
    "        '''\n",
    "        self.memory_size = capacity\n",
    "        self.history_length = hist_len\n",
    "        self.state_dim = s_dim\n",
    "        self.mem_a = np.zeros(self.memory_size, dtype = np.int8)\n",
    "        self.mem_r = np.zeros(self.memory_size, dtype = np.int8)\n",
    "        self.mem_s = np.zeros((self.memory_size,) + s_dim , dtype = np.uint8)\n",
    "        self.dones = np.zeros(self.memory_size, dtype = np.bool)\n",
    "        self.current = 0\n",
    "    def get_state(self,idx):\n",
    "        state = self.mem_s[idx - self.history_length + 1:idx + 1, :, :]\n",
    "        assert len(state) <= self.history_length\n",
    "        #print(len(state))\n",
    "        if len(state) < self.history_length:\n",
    "            pad = self.history_length - len(state)\n",
    "            pad_shape = (pad,) + (80,80,1)\n",
    "            #print(\"pad {}\".format(pad_shape))\n",
    "            pad_arr = np.zeros((pad,) + (80,80,1))\n",
    "\n",
    "            state = np.concatenate((pad_arr,state),axis=0)\n",
    "            #print(state.shape)\n",
    "\n",
    "        return state\n",
    "    def add(self,s,a,r,done):\n",
    "        self.mem_a[self.current % self.memory_size] = a\n",
    "        self.mem_r[self.current % self.memory_size] = r\n",
    "        self.mem_s[self.current % self.memory_size] = s\n",
    "        self.dones[self.current % self.memory_size] = done\n",
    "        self.current += 1 \n",
    "    def sample(self, batch_size):\n",
    "        indexes = []\n",
    "        # ensure enough frames to sample\n",
    "        assert self.current > self.history_length\n",
    "        # -1 because still need next frame\n",
    "        end = min(self.current, self.memory_size) - 1\n",
    "\n",
    "        while len(indexes) < batch_size: \n",
    "            index = np.random.randint(self.history_length - 1, end)\n",
    "            # sampled state shouldn't contain episode end\n",
    "            if self.dones[index - self.history_length + 1: index + 1].any():\n",
    "                continue\n",
    "            indexes.append(index)\n",
    "\n",
    "        smp_s = []\n",
    "        smp_a = [] \n",
    "        smp_r = []\n",
    "        smp_s_ = []\n",
    "        smp_done = []\n",
    "        for idx in indexes:\n",
    "            smp_s.append(self.get_state(idx))\n",
    "            smp_a.append(self.mem_a[idx])\n",
    "            smp_r.append(self.mem_r[idx])\n",
    "            smp_s_.append(self.get_state(idx + 1))\n",
    "            smp_done.append(self.dones[idx])\n",
    "        return np.array(smp_s),np.array(smp_a),np.array(smp_r),np.array(smp_s_),np.array(smp_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c2d1f6aa-9fb6-441d-ab31-9f6db9ed767d"
    }
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:30.494605Z",
     "start_time": "2018-01-23T18:31:30.475946Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are way too many parameters for the agent to be within the init arguments\n",
    "class AgentConfig():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        stateCnt \n",
    "        actionCnt \n",
    "        mem_size\n",
    "        epsilon_policy \n",
    "        gamma \n",
    "        num_frames\n",
    "        learning_rate \n",
    "        train_start \n",
    "        train_freq \n",
    "        target_update_freq  \n",
    "        batch_size \n",
    "        preprocessor \n",
    "        log_path \n",
    "        name      \n",
    "        '''\n",
    "        self.stateCnt = None\n",
    "        self.actionCnt = None\n",
    "        self.mem_size = None\n",
    "        self.epsilon_policy = None\n",
    "        self.gamma = None\n",
    "        self.num_frames = None\n",
    "        self.learning_rate = None\n",
    "        self.train_start = None\n",
    "        self.train_freq = None\n",
    "        self.target_update_freq = None \n",
    "        self.batch_size = None\n",
    "        self.preprocessor = None\n",
    "        self.log_path = None\n",
    "        self.name = None\n",
    "    def validate(self):\n",
    "        # validate parameters\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:13:52.762437Z",
     "start_time": "2018-01-23T17:13:52.753525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg = AgentConfig()\n",
    "cfg.stateCnt = \n",
    "cfg.actionCnt = None\n",
    "cfg.mem_size = None\n",
    "cfg.epsilon_policy = None\n",
    "cfg.gamma = None\n",
    "cfg.num_frames = None\n",
    "cfg.learning_rate = None\n",
    "cfg.train_start = None\n",
    "cfg.train_freq = None\n",
    "cfg.target_update_freq = None \n",
    "cfg.batch_size = None\n",
    "cfg.preprocessor = None\n",
    "cfg.log_path = None\n",
    "cfg.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T02:59:56.347278Z",
     "start_time": "2018-01-24T02:59:55.739851Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "c617fab6-f945-4e2d-b4f1-5a157d02485c"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self,cfg):\n",
    "        self.stateCnt = cfg.stateCnt\n",
    "        if cfg.preprocessor:\n",
    "            self.preprocessor = cfg.preprocessor\n",
    "            self.stateCnt = cfg.preprocessor.new_shape\n",
    "        \n",
    "        self.actionCnt = cfg.actionCnt\n",
    "        \n",
    "        # build network\n",
    "        inpt = (cfg.num_frames,) + self.stateCnt\n",
    "        self.model = Network(inpt,self.actionCnt,True,'duel',cfg.learning_rate).build2() # model\n",
    "        self.target_model = Network(inpt,self.actionCnt,True,'duel',cfg.learning_rate).build2() # target model\n",
    "        \n",
    "        # initialize memory\n",
    "        self.longmem = Memory(cfg.mem_size,cfg.num_frames,self.stateCnt)\n",
    "        self.shortmem = ShortMemory(cfg.num_frames,self.stateCnt)\n",
    "        \n",
    "        \n",
    "        self.epsilon_policy = cfg.epsilon_policy\n",
    "        self.gamma = cfg.gamma\n",
    "        self.target_update_freq = cfg.target_update_freq\n",
    "        \n",
    "        self.name = cfg.name\n",
    "        self.learning_rate = cfg.learning_rate \n",
    "        self.train_start = cfg.train_start\n",
    "        self.train_freq = cfg.train_freq\n",
    "        self.batch_size = cfg.batch_size\n",
    "        #logging\n",
    "        if cfg.log_path:\n",
    "            self.writer = tf.summary.FileWriter(cfg.log_path)\n",
    "        \n",
    "        # init counters for logging purpose\n",
    "        self.loss_count = 0\n",
    "        self.counter = 0\n",
    "    def save_model(self,name):\n",
    "        self.model.save_weights(self.log_path + \"/\" + name)\n",
    "    def remember(self,s,a,r,done):\n",
    "        self.longmem.add(s,a,r,done)\n",
    "    def save_scalar(self,step, name, value):\n",
    "        \"\"\"Save a scalar value to tensorboard.\n",
    "          Parameters\n",
    "          ----------\n",
    "          step: int\n",
    "            Training step (sets the position on x-axis of tensorboard graph.\n",
    "          name: str\n",
    "            Name of variable. Will be the name of the graph in tensorboard.\n",
    "          value: float\n",
    "            The value of the variable at this step.\n",
    "          writer: tf.FileWriter\n",
    "            The tensorboard FileWriter instance.\n",
    "          \"\"\"\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = float(value)\n",
    "        summary_value.tag = name\n",
    "        self.writer.add_summary(summary, step)\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    def act(self, s):\n",
    "        # Epsilon greedy action selection\n",
    "        s = s[None] # increase the rank of tensor to have a batch_size of 1 and length 1\n",
    "        if np.random.rand() <= self.epsilon_policy.epsilon:\n",
    "            return random.randrange(self.actionCnt)\n",
    "        act_values = self.model.predict(s)\n",
    "        return np.argmax(act_values[0]) # returns action\n",
    "    def replay2(self,batch_size):\n",
    "        prebatch_s,batch_a,batch_r,prebatch_s_,batch_done = self.longmem.sample(batch_size)# a batch of episode of parameter length\n",
    "        \n",
    "        batch_s = self.preprocessor.process_batch(prebatch_s)\n",
    "        batch_s_ = self.preprocessor.process_batch(prebatch_s_)\n",
    "        \n",
    "        a_ = self.model.predict(batch_s_)\n",
    "        a_idx = np.argmax(a_,axis=1)\n",
    "        behaviour_q = self.target_model.predict(batch_s_)\n",
    "        #target = self.model.predict(batch_s) [range(batch_size),batch_a.astype('int')]\n",
    "        \n",
    "        target = batch_r + self.gamma * (behaviour_q[range(batch_size),a_idx]) \n",
    "        loss = self.model.train_on_batch(batch_s,target)\n",
    "        self.save_scalar(self.loss_count,\"/loss\",loss)\n",
    "        self.loss_count +=1\n",
    "        \n",
    "        self.epsilon_policy.update()\n",
    "        return loss\n",
    "    def replay(self, batch_size,length):\n",
    "        #print(\"sample\")\n",
    "        batch = self.memory.old_sample_episode(batch_size,length) # tensor size batch_size,length,frame\n",
    "        #print(np.array(batch).size)\n",
    "        for mini in batch:\n",
    "            #print(\"reset\")\n",
    "            self.model.reset_states() # we do this because the RNN is stateful\n",
    "            self.target_model.reset_states()\n",
    "            \n",
    "            for s,a,r,s_,done in mini:\n",
    "                s = s[None][None]\n",
    "                s_ = s_[None][None]\n",
    "                #print(\"predict\")\n",
    "                target = self.model.predict(s)\n",
    "                if done:\n",
    "                    target[0][a] = r\n",
    "                else:\n",
    "                    a_ = self.model.predict(s_)[0]\n",
    "                    t = self.target_model.predict(s)[0]\n",
    "                    target[0][a] = r + self.gamma * t[np.argmax(a_)]\n",
    "                #print(\"fit\")\n",
    "                res = self.model.fit(s, target, epochs=1,batch_size=1, verbose=0,shuffle=False)\n",
    "                self.save_scalar(self.loss_count,\"Loss\",res.history['loss'][0])\n",
    "                \n",
    "        '''     \n",
    "        for s, a, r, s_, done in minibatch:\n",
    "            state = state\n",
    "            next_state = next_state\n",
    "            \n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "            self.model.fit(state, target, epochs=1, verbose=0,shuffle=False)\n",
    "        '''\n",
    "\n",
    "    def train(self,env,episodes):\n",
    "        # play loop\n",
    "        for e in range(episodes):\n",
    "            R = [0.0]\n",
    "            #agent.model.reset_states()\n",
    "            pre_s = env.reset()\n",
    "            mem_s = self.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "            net_s = self.preprocessor.process_for_network(mem_s) # normalized\n",
    "            done = False\n",
    "            self.shortmem.forget # forget short term memory for recurrent network \n",
    "            self.shortmem.add(net_s) \n",
    "            for t in itertools.count():\n",
    "                hist_s = self.shortmem.get()\n",
    "                # take action using net_s and receive a\n",
    "                a = self.act(hist_s)\n",
    "                #env.render()\n",
    "\n",
    "                pre_s_, r, done, info = env.step(a)\n",
    "                mem_s_ = self.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "                net_s_ = self.preprocessor.process_for_network(mem_s) # normalized\n",
    "\n",
    "                agent.shortmem.add(net_s_)\n",
    "                agent.remember(mem_s,a,r,done)\n",
    "                R[-1] += r\n",
    "                if done:\n",
    "                    agent.save_scalar(e,\"/reward per episode\",R[-1])\n",
    "                    #print(\"episode: {}/{}, score: {}, e: {}\".format(e, episodes, t, agent.epsilon))\n",
    "                    #exp.metric(\"reward\",R[-1])\n",
    "                    #update(e,R[-1],handle1,rplot)\n",
    "                    R = [0.0]\n",
    "                    break\n",
    "            if e % self.target_update_freq == 0:\n",
    "                self.update_target_model()\n",
    "            #if e % 100 == 0:\n",
    "            #    agent.save_model(\"Exp00-CNN\")\n",
    "            if e % 4 == 0 and e > self.train_start:\n",
    "                #print(\"replay\")\n",
    "                self.replay2(self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "ad276c2f-20e3-46f9-b200-f42edb02cac8"
    }
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-29T07:36:50.138Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "2c99af50-d7d2-4a1b-8c59-bfd85685951b"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(rgb_array,scale = 1/12):\n",
    "    frame_shape = rgb_array.shape\n",
    "    \n",
    "    frame = np.array(rgb_array)\n",
    "    gray_frame = np.dot(frame[...,:3],[0.299,0.587,0.114]).reshape((frame_shape[0],frame_shape[1]))\n",
    "    smaller = scimisc.imresize(gray_frame,scale,mode='L').astype('float64')\n",
    "    smaller /= 255.0\n",
    "    smaller = np.expand_dims(smaller,2) # convert to a 3D array of shape (height,width,grayscale)\n",
    "    smaller = np.reshape(smaller, [1, *(smaller.shape)])\n",
    "    return smaller.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-19T00:05:33.116500Z",
     "start_time": "2017-12-19T00:05:33.107652Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "2cd1233d-5e8d-49ee-8aea-14490651bbc7"
    }
   },
   "outputs": [],
   "source": [
    "def render(obs,root,canvas):\n",
    "    obs = np.squeeze(obs,2)\n",
    "    image = Image.fromarray(obs.astype('int8'),mode='L')\n",
    "    photo = ImageTk.PhotoImage(image)\n",
    "    root.one = photo\n",
    "    canvas.delete(\"all\")\n",
    "    canvas.create_image(frame_height,frame_width, image=photo)\n",
    "    root.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T16:42:10.666400Z",
     "start_time": "2018-01-23T16:42:10.630553Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "5983da69-59f0-4dfe-9150-4afb28b37ad4"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_output_folder(args, parent_dir, env_name, task_name):\n",
    "    \"\"\"Return save folder.\n",
    "    Assumes folders in the parent_dir have suffix -run{run\n",
    "    number}. Finds the highest run number and sets the output folder\n",
    "    to that number + 1. This is just convenient so that if you run the\n",
    "    same script multiple times tensorboard can plot all of the results\n",
    "    on the same plots with different names.\n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_dir: str\n",
    "      Path of the directory containing all experiment runs.\n",
    "    Returns\n",
    "    -------\n",
    "    parent_dir/run_dir\n",
    "      Path to this run's save directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "        print('===== Folder did not exist; creating... %s'%parent_dir)\n",
    "    experiment_id = 0\n",
    "    for folder_name in os.listdir(parent_dir):\n",
    "        if not os.path.isdir(os.path.join(parent_dir, folder_name)):\n",
    "            continue\n",
    "        try:\n",
    "            folder_name = int(folder_name.split('-run')[-1])\n",
    "            if folder_name > experiment_id:\n",
    "                experiment_id = folder_name\n",
    "        except:\n",
    "            pass\n",
    "    experiment_id += 1\n",
    "\n",
    "    parent_dir = os.path.join(parent_dir, env_name)\n",
    "    parent_dir = parent_dir + '-run{}'.format(experiment_id) + '-' + task_name\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "        print('===== Folder did not exist; creating... %s'%parent_dir)\n",
    "    else:\n",
    "        print('===== Folder exists; delete? %s'%parent_dir)\n",
    "        input(\"Press Enter to continue...\")\n",
    "        os.system('rm -rf %s/' % (parent_dir))\n",
    "    os.makedirs(parent_dir+'/videos/')\n",
    "    os.makedirs(parent_dir+'/images/')\n",
    "    return parent_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "44f0a6b3-671e-4a05-85fc-c52d1e45922c"
    }
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "cadab8d7-29a8-48e9-831c-4c8e61aee554"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "root = Tk()\n",
    "root_frame = Frame(root)\n",
    "canvas = Canvas(root_frame, borderwidth=0, highlightthickness=0, width=200, height=130, bg=\"black\" )\n",
    "root_frame.pack()\n",
    "canvas.pack()\n",
    "\n",
    "frame_height = 25\n",
    "frame_width = 35\n",
    "\n",
    "\n",
    "env = gym.make(\"MinecraftBasic-v0\")\n",
    "env.load_mission_file(\"./CliffWalking.xml\")\n",
    "env.init(videoResolution=[420,300],allowContinuousMovement=[\"move\", \"turn\", \"strafe\"])\n",
    "\n",
    "\n",
    "scale = 1/12 # scale image down by 1/12\n",
    "newshape = (env.video_height*scale,env.video_width*scale,1) # dimension of 1 for grayscale\n",
    "newshape = tuple(map(int,newshape))\n",
    "\n",
    "# the pre processor will adjust the observation space therefore we will edit the property of the environment to take the pre processor into accoutn\n",
    "env.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "shape=newshape)\n",
    "\n",
    "done = False\n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        env.reset()\n",
    "        while True:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            proc_obs = preprocess(obs)\n",
    "            \n",
    "            render(proc_obs,root_frame,canvas)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "    except:\n",
    "        root.destroy()\n",
    "        env.close()\n",
    "        raise\n",
    "env.close()\n",
    "root.destroy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "3ee7c012-4d9e-449a-b088-39ddbb831955"
    }
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T23:32:27.244253Z",
     "start_time": "2018-01-05T23:32:27.238751Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "c2c4723e-f544-4411-90e5-aaeeea755a2d"
    }
   },
   "outputs": [],
   "source": [
    "def update(x,y,handle,plot):\n",
    "    plot.data_source.data['x'] += [x]\n",
    "    plot.data_source.data['y'] += [y]\n",
    "    push_notebook(handle=handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T23:49:20.416109Z",
     "start_time": "2018-01-05T23:49:20.320487Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "6299a033-d360-4634-ad44-ed66e2c07e54"
    }
   },
   "outputs": [],
   "source": [
    "inferno = bokeh.palettes.Inferno9\n",
    "fig1 = figure(plot_width=400, plot_height=400,title=\"rewards\",\n",
    "                      x_axis_label=\"x\",\n",
    "                      y_axis_label=\"y\")\n",
    "rplot = fig1.line([],[],line_width=2)\n",
    "# make a grid\n",
    "handle1 = show(fig1, notebook_handle=True)\n",
    "\n",
    "reward_plot = {\"handle\":handle1,\"plot\":rplot}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "62f8ce75-9484-49e4-9d37-bf7f29257235"
    }
   },
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:41.036004Z",
     "start_time": "2018-01-23T18:31:40.985929Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "6669fd58-0676-4d6b-bbb3-77a644fd0f12"
    }
   },
   "outputs": [],
   "source": [
    "pre_env = gym.make(\"MinecraftCliffWalking1-v0\")\n",
    "pre_env.init(videoResolution=[400,400],allowContinuousMovement=[\"move\", \"turn\", \"strafe\"],observeGrid=[20,-1,20,20,-1,20],observeDistance=[4,45,12])\n",
    "env = MinecraftWrapper(pre_env,1/5,(41,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T03:00:05.535774Z",
     "start_time": "2018-01-24T03:00:05.324363Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "104c625c-304d-4849-9a6f-d1341f497b04"
    }
   },
   "outputs": [],
   "source": [
    "atari_env = gym.make('Seaquest-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T04:50:53.003771Z",
     "start_time": "2018-01-22T04:50:52.916839Z"
    },
    "code_folding": [],
    "collapsed": true,
    "nbpresent": {
     "id": "a50054f6-b180-4ee1-ba2b-a92f9f94b5ca"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(env,agent,episodes):\n",
    "    # play loop\n",
    "    batch_size = 32\n",
    "    train_start = 50\n",
    "    for e in range(episodes):\n",
    "        R = [0.0]\n",
    "        #agent.model.reset_states()\n",
    "        pre_s = env.reset()\n",
    "        mem_s = pre.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "        net_s = pre.process_for_network(mem_s) # normalized\n",
    "        done = False\n",
    "        agent.shortmem.forget # forget short term memory for recurrent network \n",
    "        agent.shortmem.add(net_s) \n",
    "        for t in itertools.count():\n",
    "            hist_s = agent.shortmem.get()\n",
    "            # take action using net_s and receive a\n",
    "            a = agent.act(hist_s)\n",
    "            env.render()\n",
    "            \n",
    "            pre_s_, r, done, info = env.step(a)\n",
    "            mem_s_ = agent.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "            net_s_ = agent.preprocessor.process_for_network(mem_s) # normalized\n",
    "            \n",
    "            agent.shortmem.add(net_s_)\n",
    "            agent.remember(mem_s,a,r,done)\n",
    "            R[-1] += r\n",
    "            if done:\n",
    "                \n",
    "                agent.save_scalar(e,\"/reward per episode\",R[-1])\n",
    "                #print(\"episode: {}/{}, score: {}, e: {}\".format(e, episodes, t, agent.epsilon))\n",
    "                #exp.metric(\"reward\",R[-1])\n",
    "                #update(e,R[-1],handle1,rplot)\n",
    "                R = [0.0]\n",
    "                break\n",
    "        #if e % 100 == 0:\n",
    "        #    agent.save_model(\"Exp00-CNN\")\n",
    "        if e % 4 == 0 and e > 32:\n",
    "            #print(\"replay\")\n",
    "            agent.replay2(batch_size)\n",
    "            agent.update_target_model()\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:31:44.134951Z",
     "start_time": "2018-01-23T18:31:44.097090Z"
    },
    "code_folding": [],
    "collapsed": true,
    "nbpresent": {
     "id": "81b9ad0f-1383-46bd-a91b-42cc299f789e"
    }
   },
   "outputs": [],
   "source": [
    "def get_output_folder(args, parent_dir, env_name, task_name):\n",
    "    \"\"\"Return save folder.\n",
    "    Assumes folders in the parent_dir have suffix -run{run\n",
    "    number}. Finds the highest run number and sets the output folder\n",
    "    to that number + 1. This is just convenient so that if you run the\n",
    "    same script multiple times tensorboard can plot all of the results\n",
    "    on the same plots with different names.\n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_dir: str\n",
    "      Path of the directory containing all experiment runs.\n",
    "    Returns\n",
    "    -------\n",
    "    parent_dir/run_dir\n",
    "      Path to this run's save directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "        print('===== Folder did not exist; creating... %s'%parent_dir)\n",
    "    experiment_id = 0\n",
    "    for folder_name in os.listdir(parent_dir):\n",
    "        if not os.path.isdir(os.path.join(parent_dir, folder_name)):\n",
    "            continue\n",
    "        try:\n",
    "            folder_name = int(folder_name.split('-run')[-1])\n",
    "            if folder_name > experiment_id:\n",
    "                experiment_id = folder_name\n",
    "        except:\n",
    "            pass\n",
    "    experiment_id += 1\n",
    "\n",
    "    parent_dir = os.path.join(parent_dir, env_name)\n",
    "    parent_dir = parent_dir + '-run{}'.format(experiment_id) + '-' + task_name\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "        print('===== Folder did not exist; creating... %s'%parent_dir)\n",
    "    else:\n",
    "        print('===== Folder exists; delete? %s'%parent_dir)\n",
    "        input(\"Press Enter to continue...\")\n",
    "        os.system('rm -rf %s/' % (parent_dir))\n",
    "    #s.makedirs(parent_dir+'/videos/')\n",
    "    #os.makedirs(parent_dir+'/images/')\n",
    "    return parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T03:00:14.635502Z",
     "start_time": "2018-01-24T03:00:14.631819Z"
    },
    "nbpresent": {
     "id": "4a589315-8054-4f3d-b48b-04156bd3c48b"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Folder did not exist; creating... ./logs/Seaquest-v3-run1-DQRN-Batch\n"
     ]
    }
   ],
   "source": [
    "doc = get_output_folder(None,\"./logs\",\"Seaquest-v3\",\"DQRN-Batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T18:49:19.560267Z",
     "start_time": "2018-01-23T18:49:19.556358Z"
    },
    "nbpresent": {
     "id": "218afe9f-e400-466c-994b-d87dd8016173"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logs/Seaquest-v2-run1-DQRN-Batch'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T03:00:49.482509Z",
     "start_time": "2018-01-24T03:00:48.171955Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "58744aa0-4cf4-41c2-8179-84e149038b06"
    }
   },
   "outputs": [],
   "source": [
    "episodes = 1000000\n",
    "cfg = AgentConfig()\n",
    "cfg.stateCnt = atari_env.observation_space.shape\n",
    "cfg.actionCnt = atari_env.action_space.n\n",
    "cfg.mem_size = 1000000\n",
    "cfg.epsilon_policy = LinearDecayGreedyEpsilonPolicy(1.0,0.05,episodes)\n",
    "cfg.gamma = 0.99\n",
    "cfg.num_frames = 10\n",
    "cfg.learning_rate = 0.0001\n",
    "cfg.train_start = 200\n",
    "cfg.train_freq = 4\n",
    "cfg.target_update_freq = 1000\n",
    "cfg.batch_size = 32\n",
    "cfg.preprocessor = AtariPreprocessor((84,84,1))\n",
    "cfg.log_path = doc\n",
    "cfg.name = \"A2\"\n",
    "agent = Agent(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-24T03:00:50.136Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "e44c5e29-4d2f-41ec-a1fb-7c42dd7113f6"
    }
   },
   "outputs": [],
   "source": [
    "agent.train(atari_env,episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ac9c66ef-4bd0-422a-af51-7dc81aac8a57"
    }
   },
   "source": [
    " Chasewind hyper parameters for seaquest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T19:15:14.192482Z",
     "start_time": "2018-01-06T19:15:00.875277Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "cb9f9ba4-e7b0-4b99-841b-4c4bca2c07f8"
    }
   },
   "outputs": [],
   "source": [
    "for mini in batch:\n",
    "    #print(\"reset\")\n",
    "    agent.model.reset_states() # we do this because the RNN is stateful\n",
    "    agent.target_model.reset_states()\n",
    "\n",
    "    for s,a,r,s_,done in mini:\n",
    "        s = s[None][None]\n",
    "        s_ = s_[None][None]\n",
    "        #print(\"predict\")\n",
    "        target = agent.model.predict(s)[0]\n",
    "        #print(\"target {}\".format(target))\n",
    "        if done:\n",
    "            target[a] = r\n",
    "        else:\n",
    "            a_ = agent.model.predict(s_)[0]\n",
    "            t = agent.target_model.predict(s)[0]\n",
    "            target[a] = r + agent.gamma * t[np.argmax(a_)]\n",
    "        #print(\"fit\")\n",
    "        '''\n",
    "        print(\"target + tderr: {}\".format(target))\n",
    "        print(\"action: {}\".format(a))\n",
    "        print(\"argmax next_action: {}\".format(np.argmax(a_)))\n",
    "        print(\"reward: {}\".format(r))\n",
    "        print(\"next_action {}\".format(a_))\n",
    "        print(\"behavioural {}\".format(t))\n",
    "        '''\n",
    "        res = self.model.fit(s, target, epochs=1,batch_size=1, verbose=0,shuffle=False)\n",
    "        self.save_scalar(self.loss_count,\"Loss\",res.history['loss'][0])\n",
    "        self.loss_count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c6932bdb-43d2-4043-a468-026656b62d44"
    }
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T22:17:22.803152Z",
     "start_time": "2018-01-21T22:17:22.790311Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "0acbc885-e5b8-445e-b136-a75f3ff12bac"
    }
   },
   "outputs": [],
   "source": [
    "b_s,b_a,b_r,b_s_,b_done = agent.memory.sample(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T22:25:46.116684Z",
     "start_time": "2018-01-21T22:25:46.113701Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "8fbdca94-112f-4da0-9f6f-c605b20b0503"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:20:57.261848Z",
     "start_time": "2018-01-23T17:20:55.775127Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "4b457670-d5a1-4bc0-96ef-0a93c15d6fdb"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cfg = AgentConfig()\n",
    "cfg.stateCnt = atari_env.observation_space.shape\n",
    "cfg.actionCnt = atari_env.action_space.n\n",
    "cfg.mem_size = 1000\n",
    "cfg.epsilon_policy = LinearDecayGreedyEpsilonPolicy(1.0,0.05,100)\n",
    "cfg.gamma = 0.99\n",
    "cfg.num_frames = 5\n",
    "cfg.learning_rate = 0.0001\n",
    "cfg.train_start = 20\n",
    "cfg.train_freq = 4\n",
    "cfg.target_update_freq = 100 \n",
    "cfg.batch_size = 32\n",
    "cfg.preprocessor = AtariPreprocessor((84,84,1))\n",
    "cfg.log_path = None\n",
    "cfg.name = \"Tester\"\n",
    "t_agent = Agent(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:31:02.831928Z",
     "start_time": "2018-01-23T03:31:02.762131Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "6ab35772-9289-48fb-83ff-77539c6c495e"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "prebatch_s,batch_a,batch_r,prebatch_s_,batch_done = agent.longmem.sample(batch_size)# a batch of episode of parameter length\n",
    "\n",
    "batch_s = agent.preprocessor.process_batch(prebatch_s)\n",
    "batch_s_ = agent.preprocessor.process_batch(prebatch_s_)\n",
    "a_ = agent.model.predict(batch_s_)\n",
    "a_idx = np.argmax(a_,axis=1)\n",
    "behaviour_q = agent.target_model.predict(batch_s_)\n",
    "target = agent.model.predict(batch_s)\n",
    "\n",
    "target[range(batch_size),batch_a.astype('int')] = batch_r + agent.gamma * (behaviour_q[range(batch_size),a_idx]) \n",
    "#loss = agent.model.train_on_batch(batch_s,target)\n",
    "#self.save_scalar(self.loss_count,\"/Loss\",loss)\n",
    "#self.loss_count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:16:10.037547Z",
     "start_time": "2018-01-23T03:16:10.030345Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "552de618-bfb1-4865-b226-b8724db50ffa"
    }
   },
   "outputs": [],
   "source": [
    "batch_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "3271e9cd-9ac8-4464-8314-899e6ae068bb"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:28:07.718308Z",
     "start_time": "2018-01-23T03:28:07.713695Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "ba63aa82-b239-4375-8250-d9f1c2a5ea93"
    }
   },
   "outputs": [],
   "source": [
    "t_img = np.squeeze(batch_s[0][2]*255.0,-1)\n",
    "img = Image.fromarray(t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:27:58.194330Z",
     "start_time": "2018-01-23T03:27:58.050764Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "1c567421-37a4-4984-a8b7-9cbb3a19d23b"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:28:13.904304Z",
     "start_time": "2018-01-23T03:28:13.765997Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "33fe9928-22d4-42c6-aa86-b337962bd466"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:25:58.203246Z",
     "start_time": "2018-01-23T17:25:58.184795Z"
    },
    "code_folding": [],
    "collapsed": true,
    "nbpresent": {
     "id": "921291b9-8562-4a32-9a55-ed5eb935233f"
    }
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/soply/f3eec2e79c165e39c9d540e916142ae1\n",
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(np.squeeze(image,-1) )\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:31:54.103494Z",
     "start_time": "2018-01-23T03:31:54.095818Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "c94a820f-0093-45c1-881f-85d1843d3d82"
    }
   },
   "outputs": [],
   "source": [
    "prebatch_s,batch_a,batch_r,prebatch_s_,batch_done = agent.longmem.sample(batch_size)# a batch of episode of parameter length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:34:48.819264Z",
     "start_time": "2018-01-23T03:34:48.814485Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "6c465604-205c-453b-91c3-f5c0b33527df"
    }
   },
   "outputs": [],
   "source": [
    "batch_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:36:03.129384Z",
     "start_time": "2018-01-23T03:36:03.124937Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "c04a6c58-0771-400f-9a6a-edfab2448ca4"
    }
   },
   "outputs": [],
   "source": [
    "shortmem_s = agent.shortmem.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:36:12.716016Z",
     "start_time": "2018-01-23T03:36:11.777880Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "2043a96b-477e-4697-a236-cdcae2a9cce9"
    }
   },
   "outputs": [],
   "source": [
    "show_images(shortmem_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:40:33.428923Z",
     "start_time": "2018-01-23T03:40:12.757314Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "092c8db1-ba8e-4a3f-82eb-79086faf7042"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:32:50.862129Z",
     "start_time": "2018-01-23T17:32:50.827553Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "7aaa1daa-e47b-4f0a-94b7-4d373d93e09c"
    }
   },
   "outputs": [],
   "source": [
    "def dry(env,agent,episodes):\n",
    "    # used to populate test data into agent, useful for debugging\n",
    "    for i in range(episodes):\n",
    "        # play loop\n",
    "        pre_s = env.reset()\n",
    "        mem_s = agent.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "        net_s = agent.preprocessor.process_for_network(mem_s) # normalized\n",
    "        done = False\n",
    "        #agent.shortmem.forget # forget short term memory for recurrent network \n",
    "        agent.shortmem.add(net_s)\n",
    "        for t in itertools.count():\n",
    "            hist_s = agent.shortmem.get()\n",
    "            # take action using net_s and receive a\n",
    "            a = agent.act(hist_s)\n",
    "            env.render()\n",
    "\n",
    "            pre_s_, r, done, info = env.step(a)\n",
    "\n",
    "            mem_s_ = agent.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "            net_s_ = agent.preprocessor.process_for_network(mem_s) # normalized\n",
    "            #test.append(mem_s_)\n",
    "            agent.shortmem.add(net_s_)\n",
    "            agent.remember(mem_s,a,r,done)\n",
    "\n",
    "            #R[-1] += r\n",
    "            if done:\n",
    "\n",
    "                #agent.save_scalar(e,\"/reward per episode\",R[-1])\n",
    "                #print(\"episode: {}/{}, score: {}, e: {}\".format(e, episodes, t, agent.epsilon))\n",
    "                #exp.metric(\"reward\",R[-1])\n",
    "                #update(e,R[-1],handle1,rplot)\n",
    "                #R = [0.0]\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:33:15.150713Z",
     "start_time": "2018-01-23T17:32:53.960999Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dry(atari_env,t_agent,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:33:48.878341Z",
     "start_time": "2018-01-23T17:33:47.515784Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "95aa46a5-86f8-47fa-b3c6-640ffa3e37e2"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prebatch_s,batch_a,batch_r,prebatch_s_,batch_done = t_agent.longmem.sample(32)# a batch of episode of parameter length\n",
    "idx = 30\n",
    "show_images(prebatch_s[idx])\n",
    "show_images(prebatch_s_[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:33:28.004376Z",
     "start_time": "2018-01-23T17:33:27.317382Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "750dfe9a-5744-4432-9aa5-449bd0f66991"
    }
   },
   "outputs": [],
   "source": [
    "show_images(t_agent.shortmem.get()*255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:26:28.868964Z",
     "start_time": "2018-01-23T17:26:27.492844Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "74fef843-0ba3-4f17-896a-509fc594e8e9"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:27:22.970013Z",
     "start_time": "2018-01-23T17:27:22.952579Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "aa94547f-cd66-4549-8cf3-0630c7ac2e1b"
    }
   },
   "outputs": [],
   "source": [
    "pre_s = atari_env.reset()\n",
    "mem_s = t_agent.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "net_s = t_agent.preprocessor.process_for_network(mem_s) # normalized\n",
    "done = False\n",
    "t_agent.shortmem.forget() # forget short term memory for recurrent network \n",
    "t_agent.shortmem.add(net_s)\n",
    "atari_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:32:07.600283Z",
     "start_time": "2018-01-23T17:32:07.588372Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "daafc7e0-ce98-483d-a33a-da2b3d9feeef"
    }
   },
   "outputs": [],
   "source": [
    "hist_s = t_agent.shortmem.get()\n",
    "a = t_agent.act(hist_s)\n",
    "atari_env.render()\n",
    "\n",
    "pre_s_, r, done, info = atari_env.step(a)\n",
    "\n",
    "mem_s_ = t_agent.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "net_s_ = t_agent.preprocessor.process_for_network(mem_s) # normalized\n",
    "t_agent.shortmem.add(net_s_)\n",
    "t_agent.remember(mem_s,a,r,done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:10:10.483097Z",
     "start_time": "2018-01-23T04:10:10.478315Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "d76678cf-4bab-42a0-b941-5066e9964473"
    }
   },
   "outputs": [],
   "source": [
    "t_agent.act(hist_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:31:45.672733Z",
     "start_time": "2018-01-23T17:31:45.558118Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "37cd9700-e5a9-4598-a4e7-d2aa995fd2e6"
    }
   },
   "outputs": [],
   "source": [
    "show_images(net_s[None]*255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:09:09.964710Z",
     "start_time": "2018-01-23T04:09:09.958517Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "090dd334-d198-4c4e-8c73-d6490eaca5d3"
    }
   },
   "outputs": [],
   "source": [
    "hist_s = t_agent.shortmem.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:09:11.505633Z",
     "start_time": "2018-01-23T04:09:10.475291Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "6414e12e-1912-4dcd-aaaf-ea360bc14b36"
    }
   },
   "outputs": [],
   "source": [
    "show_images(hist_s*255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:01:25.960783Z",
     "start_time": "2018-01-23T04:01:25.951488Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "64245d72-35bb-4228-b095-0d617d8e882f"
    }
   },
   "outputs": [],
   "source": [
    "t_agent.shortmem.mem_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:02:48.070304Z",
     "start_time": "2018-01-23T04:02:48.063977Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "feaede64-4bf5-4220-b318-08fb54880dbf"
    }
   },
   "outputs": [],
   "source": [
    "t_agent.shortmem.current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:36:29.683751Z",
     "start_time": "2018-01-23T17:36:29.681106Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:36:50.444753Z",
     "start_time": "2018-01-23T17:36:50.340747Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "82adcbcc-878c-48e4-b6bb-734d1d7e3bef"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Run DQN on Atari Breakout')\n",
    "parser.add_argument('--env', default='Seaquest-v0', help='Atari env name')\n",
    "parser.add_argument('-o', '--output', default='./log/', help='Directory to save data to')\n",
    "parser.add_argument('--seed', default=0, type=int, help='Random seed')\n",
    "parser.add_argument('--gamma', default=0.99, type=float, help='Discount factor')\n",
    "parser.add_argument('--batch_size', default=32, type=int, help='Minibatch size')\n",
    "parser.add_argument('--learning_rate', default=0.0001, type=float, help='Learning rate')\n",
    "parser.add_argument('--initial_epsilon', default=1.0, type=float, help='Initial exploration probability in epsilon-greedy')\n",
    "parser.add_argument('--final_epsilon', default=0.05, type=float, help='Final exploration probability in epsilon-greedy')\n",
    "parser.add_argument('--exploration_steps', default=1000000, type=int, help='Number of steps over which the initial value of epsilon is linearly annealed to its final value')\n",
    "parser.add_argument('--num_samples', default=100000000, type=int, help='Number of training samples from the environment in training')\n",
    "parser.add_argument('--num_frames', default=4, type=int, help='Number of frames to feed to Q-Network')\n",
    "parser.add_argument('--frame_width', default=84, type=int, help='Resized frame width')\n",
    "parser.add_argument('--frame_height', default=84, type=int, help='Resized frame height')\n",
    "parser.add_argument('--replay_memory_size', default=1000000, type=int, help='Number of replay memory the agent uses for training')\n",
    "parser.add_argument('--target_update_freq', default=10000, type=int, help='The frequency with which the target network is updated')\n",
    "parser.add_argument('--train_freq', default=4, type=int, help='The frequency of actions wrt Q-network update')\n",
    "parser.add_argument('--save_freq', default=50000, type=int, help='The frequency with which the network is saved')\n",
    "parser.add_argument('--eval_freq', default=50000, type=int, help='The frequency with which the policy is evlauted')    \n",
    "parser.add_argument('--num_burn_in', default=50000, type=int, help='Number of steps to populate the replay memory before training starts')\n",
    "parser.add_argument('--load_network', default=False, action='store_true', help='Load trained mode')\n",
    "parser.add_argument('--load_network_path', default='', help='the path to the trained mode file')\n",
    "parser.add_argument('--net_mode', default='dqn', help='choose the mode of net, can be linear, dqn, duel')\n",
    "parser.add_argument('--max_episode_length', default = 10000, type=int, help = 'max length of each episode')\n",
    "parser.add_argument('--num_episodes_at_test', default = 20, type=int, help='Number of episodes the agent plays at test')\n",
    "parser.add_argument('--ddqn', default=False, dest='ddqn', action='store_true', help='enable ddqn')\n",
    "parser.add_argument('--train', default=True, dest='train', action='store_true', help='Train mode')\n",
    "parser.add_argument('--test', dest='train', action='store_false', help='Test mode')\n",
    "parser.add_argument('--no_experience', default=False, action='store_true', help='do not use experience replay')\n",
    "parser.add_argument('--no_target', default=False, action='store_true', help='do not use target fixing')\n",
    "parser.add_argument('--no_monitor', default=False, action='store_true', help='do not record video')\n",
    "parser.add_argument('--task_name', default='', help='task name')\n",
    "parser.add_argument('--recurrent', default=False, dest='recurrent', action='store_true', help='enable recurrent DQN')\n",
    "parser.add_argument('--a_t', default=False, dest='a_t', action='store_true', help='enable temporal/spatial attention')\n",
    "parser.add_argument('--global_a_t', default=False, dest='global_a_t', action='store_true', help='enable global temporal attention')\n",
    "parser.add_argument('--selector', default=False, dest='selector', action='store_true', help='enable selector for spatial attention')\n",
    "parser.add_argument('--bidir', default=False, dest='bidir', action='store_true', help='enable two layer bidirectional lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:39:14.975165Z",
     "start_time": "2018-01-23T17:39:14.968164Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--gamma 0.9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:casper]",
   "language": "python",
   "name": "conda-env-casper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
