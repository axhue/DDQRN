{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:37.967313Z",
     "start_time": "2018-01-28T17:08:28.288533Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "21eb868f-3165-4224-b1b0-3d2040a59b82"
    }
   },
   "outputs": [],
   "source": [
    "import pdb;\n",
    "import scipy.misc as scimisc\n",
    "\n",
    "from tkinter import *\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "from PIL import Image\n",
    "\n",
    "import MalmoPython\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output,display\n",
    "import logging\n",
    "import math\n",
    "\n",
    "\n",
    "import gym\n",
    "import gym_minecraft\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import baselines.common.tf_util as U\n",
    "\n",
    "from baselines import logger\n",
    "from baselines import deepq\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer,PrioritizedReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:40.667783Z",
     "start_time": "2018-01-28T17:08:40.262196Z"
    },
    "nbpresent": {
     "id": "6394dbd2-aa3d-438b-bddc-de0c2b263ff0"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"f2d31a10-07c6-48db-9052-498815e4bef4\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(`.${CLASS_NAME.split(' ')[0]}`);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"f2d31a10-07c6-48db-9052-498815e4bef4\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"f2d31a10-07c6-48db-9052-498815e4bef4\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'f2d31a10-07c6-48db-9052-498815e4bef4' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"f2d31a10-07c6-48db-9052-498815e4bef4\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"f2d31a10-07c6-48db-9052-498815e4bef4\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"f2d31a10-07c6-48db-9052-498815e4bef4\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'f2d31a10-07c6-48db-9052-498815e4bef4' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"f2d31a10-07c6-48db-9052-498815e4bef4\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, push_notebook, show\n",
    "from bokeh.driving import linear\n",
    "from bokeh.layouts import row,gridplot\n",
    "from IPython.display import clear_output,display\n",
    "import bokeh\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:40.753462Z",
     "start_time": "2018-01-28T17:08:40.727442Z"
    },
    "nbpresent": {
     "id": "7073f323-45ba-43d8-b4fa-0f20a917264d"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential,model_from_json,Model\n",
    "from keras.layers import Conv2D,LSTM,GRU,TimeDistributed,Dense,Flatten,Input,Lambda,multiply\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:43.280118Z",
     "start_time": "2018-01-28T17:08:43.276474Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "7c192fe1-b95e-4b2c-a11e-da3e9884f788"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_minecraft\n",
    "from MinecraftGym import MinecraftWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "2ec6aafa-1163-4bca-87f2-2a31e99c5214"
    }
   },
   "source": [
    "## Epsilon Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:45.553405Z",
     "start_time": "2018-01-28T17:08:45.548398Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "4369636f-c1de-48c6-98f8-04c1ebf8313e"
    }
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\"Base class representing an MDP policy.\n",
    "\n",
    "    Policies are used by the agent to choose actions.\n",
    "\n",
    "    Policies are designed to be stacked to get interesting behaviors\n",
    "    of choices. For instances in a discrete action space the lowest\n",
    "    level policy may take in Q-Values and select the action index\n",
    "    corresponding to the largest value. If this policy is wrapped in\n",
    "    an epsilon greedy policy then with some probability epsilon, a\n",
    "    random action will be chosen.\n",
    "    \"\"\"\n",
    "\n",
    "    def select_action(self, **kwargs):\n",
    "        \"\"\"Used by agents to select actions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Any:\n",
    "          An object representing the chosen action. Type depends on\n",
    "          the hierarchy of policy instances.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This method should be overriden.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:45.743683Z",
     "start_time": "2018-01-28T17:08:45.723040Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "b604e303-7495-4a01-8636-df75fa9a576e"
    }
   },
   "outputs": [],
   "source": [
    "class LinearDecayGreedyEpsilonPolicy(Policy):\n",
    "    \"\"\"Policy with a parameter that decays linearly.\n",
    "\n",
    "    Like GreedyEpsilonPolicy but the epsilon decays from a start value\n",
    "    to an end value over k steps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_value: int, float\n",
    "      The initial value of the parameter\n",
    "    end_value: int, float\n",
    "      The value of the policy at the end of the decay.\n",
    "    num_steps: int\n",
    "      The number of steps over which to decay the value.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start_value, end_value, num_steps):  # noqa: D102\n",
    "        self.start_value = start_value\n",
    "        self.decay_rate = float(end_value - start_value) / num_steps\n",
    "        self.end_value = end_value\n",
    "        self.step = 0\n",
    "        self.epsilon = start_value\n",
    "\n",
    "    def update(self,is_training = True):\n",
    "        \"\"\"Decay parameter and select action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        q_values: np.array\n",
    "          The Q-values for each action.\n",
    "        is_training: bool, optional\n",
    "          If true then parameter will be decayed. Defaults to true.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Any:\n",
    "          Selected action.\n",
    "        \"\"\"\n",
    "        epsilon = self.start_value\n",
    "        if is_training:\n",
    "            epsilon += self.decay_rate * self.step\n",
    "            self.step += 1\n",
    "        self.epsilon = max(epsilon, self.end_value)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Start the decay over at the start value.\"\"\"\n",
    "        self.step = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "ecf79551-5703-4d22-85ad-83ea56d2ec06"
    }
   },
   "source": [
    "## Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:46.649609Z",
     "start_time": "2018-01-28T17:08:46.624562Z"
    },
    "code_folding": [],
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "89d86297-2eed-43a8-b9b5-d506dff95ecc"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"Preprocessor base class.\n",
    "\n",
    "    This is a suggested interface for the preprocessing steps. \n",
    "\n",
    "    Preprocessor can be used to perform some fixed operations on the\n",
    "    raw state from an environment. For example, in ConvNet based\n",
    "    networks which use image as the raw state, it is often useful to\n",
    "    convert the image to greyscale or downsample the image.\n",
    "\n",
    "    Preprocessors are implemented as class so that they can have\n",
    "    internal state. This can be useful for things like the\n",
    "    AtariPreproccessor which maxes over k frames.\n",
    "\n",
    "    If you're using internal states, such as for keeping a sequence of\n",
    "    inputs like in Atari, you should probably call reset when a new\n",
    "    episode begins so that state doesn't leak in from episode to\n",
    "    episode.\n",
    "    \"\"\"\n",
    "\n",
    "    def process_state_for_network(self, state):\n",
    "        \"\"\"Preprocess the given state before giving it to the network.\n",
    "\n",
    "        Should be called just before the action is selected.\n",
    "\n",
    "        This is a different method from the process_state_for_memory\n",
    "        because the replay memory may require a different storage\n",
    "        format to reduce memory usage. For example, storing images as\n",
    "        uint8 in memory is a lot more efficient thant float32, but the\n",
    "        networks work better with floating point images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: np.ndarray\n",
    "          Generally a numpy array. A single state from an environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        processed_state: np.ndarray\n",
    "          Generally a numpy array. The state after processing. Can be\n",
    "          modified in anyway.\n",
    "        \"\"\"\n",
    "        return state\n",
    "\n",
    "    def process_state_for_memory(self, state):\n",
    "        \"\"\"Preprocess the given state before giving it to the replay memory.\n",
    "\n",
    "        Should be called just before appending this to the replay memory.\n",
    "\n",
    "        This is a different method from the process_state_for_network\n",
    "        because the replay memory may require a different storage\n",
    "        format to reduce memory usage. For example, storing images as\n",
    "        uint8 in memory and the network expecting images in floating\n",
    "        point.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: np.ndarray\n",
    "          A single state from an environmnet. Generally a numpy array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        processed_state: np.ndarray\n",
    "          Generally a numpy array. The state after processing. Can be\n",
    "          modified in any manner.\n",
    "        \"\"\"\n",
    "        return state\n",
    "\n",
    "    def process_batch(self, samples):\n",
    "        \"\"\"Process batch of samples.\n",
    "\n",
    "        If your replay memory storage format is different than your\n",
    "        network input, you may want to apply this function to your\n",
    "        sampled batch before running it through your update function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples: list(tensorflow_rl.core.Sample)\n",
    "          List of samples to process\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        processed_samples: list(tensorflow_rl.core.Sample)\n",
    "          Samples after processing. Can be modified in anyways, but\n",
    "          the list length will generally stay the same.\n",
    "        \"\"\"\n",
    "        return samples\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"Process the reward.\n",
    "\n",
    "        Useful for things like reward clipping. The Atari environments\n",
    "        from DQN paper do this. Instead of taking real score, they\n",
    "        take the sign of the delta of the score.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward: float\n",
    "          Reward to process\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        processed_reward: float\n",
    "          The processed reward\n",
    "        \"\"\"\n",
    "        return reward\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset any internal state.\n",
    "\n",
    "        Will be called at the start of every new episode. Makes it\n",
    "        possible to do history snapshots.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:47.321322Z",
     "start_time": "2018-01-28T17:08:47.283395Z"
    },
    "code_folding": [
     6,
     14,
     31
    ],
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "da6bfe6a-4ee4-4856-be91-af43a9686319"
    }
   },
   "outputs": [],
   "source": [
    "class AtariPreprocessor(Preprocessor):\n",
    "    \"\"\"Converts images to greyscale and downscales.\n",
    "\n",
    "    Based on the preprocessing step described in:\n",
    "\n",
    "    @article{mnih15_human_level_contr_throug_deep_reinf_learn,\n",
    "    author =\t {Volodymyr Mnih and Koray Kavukcuoglu and David\n",
    "                  Silver and Andrei A. Rusu and Joel Veness and Marc\n",
    "                  G. Bellemare and Alex Graves and Martin Riedmiller\n",
    "                  and Andreas K. Fidjeland and Georg Ostrovski and\n",
    "                  Stig Petersen and Charles Beattie and Amir Sadik and\n",
    "                  Ioannis Antonoglou and Helen King and Dharshan\n",
    "                  Kumaran and Daan Wierstra and Shane Legg and Demis\n",
    "                  Hassabis},\n",
    "    title =\t {Human-Level Control Through Deep Reinforcement\n",
    "                  Learning},\n",
    "    journal =\t {Nature},\n",
    "    volume =\t 518,\n",
    "    number =\t 7540,\n",
    "    pages =\t {529-533},\n",
    "    year =\t 2015,\n",
    "    doi =        {10.1038/nature14236},\n",
    "    url =\t {http://dx.doi.org/10.1038/nature14236},\n",
    "    }\n",
    "\n",
    "    You may also want to max over frames to remove flickering. Some\n",
    "    games require this (based on animations and the limited sprite\n",
    "    drawing capabilities of the original Atari).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    new_size: 2 element tuple\n",
    "      The size that each image in the state should be scaled to. e.g\n",
    "      (84, 84) will make each image in the output have shape (84, 84).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,new_state_shape):\n",
    "        self.new_shape = new_state_shape\n",
    "    def process_state_for_memory(self, state):\n",
    "        \"\"\"Scale, convert to greyscale and store as uint8.\n",
    "\n",
    "        We don't want to save floating point numbers in the replay\n",
    "        memory. We get the same resolution as uint8, but use a quarter\n",
    "        to an eigth of the bytes (depending on float32 or float64)\n",
    "\n",
    "        We recommend using the Python Image Library (PIL) to do the\n",
    "        image conversions.\n",
    "        \"\"\"\n",
    "        img = Image.fromarray(state).convert('L').resize(self.new_shape[:2], Image.BILINEAR)\n",
    "        state = np.array(img)\n",
    "        return np.expand_dims(state,-1)\n",
    "    def process_for_network(self, state):\n",
    "        \"\"\"Scale, convert to greyscale and store as float32.\n",
    "\n",
    "        Basically same as process state for memory, but this time\n",
    "        outputs float32 images.\n",
    "        \"\"\"\n",
    "        state = np.float32(state / 255.0)\n",
    "        return state\n",
    "\n",
    "    def process_batch(self, samples):\n",
    "        \"\"\"The batches from replay memory will be uint8, convert to float32.\n",
    "\n",
    "        Same as process_state_for_network but works on a batch of\n",
    "        samples from the replay memory. Meaning you need to convert\n",
    "        both state and next state values.\n",
    "        \"\"\"\n",
    "        return np.float32(samples / 255.0)\n",
    "            \n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"Clip reward between -1 and 1.\"\"\"\n",
    "        return np.clip(reward, -1, 1) \n",
    "    \n",
    "    def reset(self):\n",
    "        self.last_state = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:48.578631Z",
     "start_time": "2018-01-28T17:08:48.567073Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, max_grad=1.):\n",
    "    \"\"\"Calculate the huber loss.\n",
    "\n",
    "    See https://en.wikipedia.org/wiki/Huber_loss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.array, tf.Tensor\n",
    "      Target value.\n",
    "    y_pred: np.array, tf.Tensor\n",
    "      Predicted value.\n",
    "    max_grad: float, optional\n",
    "      Positive floating point value. Represents the maximum possible\n",
    "      gradient magnitude.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "      The huber loss.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"HuberLoss\"):\n",
    "        delta = max_grad \n",
    "        diff = tf.abs(y_true - y_pred, name = \"diff\")\n",
    "        mask = diff < delta\n",
    "        return tf.where(mask, 0.5 * tf.square(diff), delta * (diff - 0.5 * delta))\n",
    "\n",
    "def mean_huber_loss(y_true, y_pred, max_grad=1.):\n",
    "    \"\"\"Return mean huber loss.\n",
    "\n",
    "    Same as huber_loss, but takes the mean over all values in the\n",
    "    output tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.array, tf.Tensor\n",
    "      Target value.\n",
    "    y_pred: np.array, tf.Tensor\n",
    "      Predicted value.\n",
    "    max_grad: float, optional\n",
    "      Positive floating point value. Represents the maximum possible\n",
    "      gradient magnitude.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "      The mean huber loss.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(huber_loss(y_true, y_pred, max_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "20cc2f69-169c-487f-b7cb-831d4ac30817"
    }
   },
   "source": [
    "## Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:50.361113Z",
     "start_time": "2018-01-28T17:08:50.108516Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "29a1fd90-66aa-40ce-b601-b841593ae45d"
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self,stateCnt,actionCnt,recurrent,mode,learning_rate):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "        self.learning_rate = learning_rate\n",
    "        #self.batch_size = batch_size\n",
    "        \n",
    "        self.recurrent = recurrent\n",
    "        self.mode = mode\n",
    "        \n",
    "    def build(self):\n",
    "        model = Sequential()\n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3)),\n",
    "                          input_shape=self.stateCnt,batch_size=self.batch_size))\n",
    "        \n",
    "        #model.add(layers.TimeDistributed(layers.Conv2D(32,(8,8),input_shape=self.stateCnt,activation='relu')))\n",
    "        conv2 = layers.Conv2D(64,(4,4),activation='relu')\n",
    "        conv3 = layers.Conv2D(64,(3,3),activation='relu')\n",
    "        model.add(layers.TimeDistributed(conv2))\n",
    "        model.add(layers.TimeDistributed(conv3))\n",
    "        model.add(layers.TimeDistributed(layers.Flatten()))\n",
    "        #model.add(layers.Flatten())\n",
    "        #model.add(layers.Permute((0,2,1)))\n",
    "        #self.add(Reshape(input_width, num_filters))\n",
    "        model.add(layers.GRU(units=70,stateful=True))\n",
    "        #model.add(layers.Dense(256,activation='relu')\n",
    "        model.add(layers.Dense(output_dim=self.actionCnt))\n",
    "        \n",
    "        model.compile(loss=self._huber_loss,optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    def build2(self):\n",
    "        inpt = Input(shape = self.stateCnt, name = \"input\")\n",
    "        \n",
    "        if self.mode == \"linear\":\n",
    "            \n",
    "            flatten_hidden = Flatten(name = \"flatten\")(input_data)\n",
    "            output = Dense(num_actions, name = \"output\")(flatten_hidden)\n",
    "        else:\n",
    "            if self.recurrent:\n",
    "                # shape should be (timesteps,height,width,color)\n",
    "                conv1 = TimeDistributed(Conv2D(32, (8, 8), strides = 4, activation = \"relu\", name = \"conv1\"))(inpt)\n",
    "                conv2 = TimeDistributed(Conv2D(64, (4, 4), strides = 2, activation = \"relu\", name = \"conv2\"))(conv1)\n",
    "                conv3 = TimeDistributed(Conv2D(64, (3, 3), strides = 1, activation = \"relu\", name = \"conv3\"))(conv2)\n",
    "                flatten_hidden = TimeDistributed(Flatten())(conv3)\n",
    "                hidden_input = TimeDistributed(Dense(512, activation = 'relu', name = 'flat_to_512')) (flatten_hidden)\n",
    "                context = LSTM(512, return_sequences=False, stateful=False) (hidden_input)\n",
    "                \n",
    "            if self.mode == \"dqn\":\n",
    "                h4 = Dense(512, activation='relu', name = \"fc\")(context)\n",
    "                output = Dense(num_actions, name = \"output\")(h4)\n",
    "            elif self.mode == \"duel\":\n",
    "                value_hidden = Dense(512, activation = 'relu', name = 'value_fc')(context)\n",
    "                value = Dense(1, name = \"value\")(value_hidden)\n",
    "                \n",
    "                action_hidden = Dense(512, activation = 'relu', name = 'action_fc')(context)\n",
    "                action = Dense(self.actionCnt, name = \"action\")(action_hidden)\n",
    "                \n",
    "                action_mean = Lambda(lambda x: K.mean(x, axis = 1, keepdims = True), name = 'action_mean')(action) \n",
    "                output = Lambda(lambda x: x[0] + x[1] - x[2], name = 'output')([action, value, action_mean])\n",
    "        model = Model(inputs = inpt, outputs = output)\n",
    "        model.compile(loss=self.mean_huber_loss,optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "            \n",
    "        \n",
    "    def pseudo_huber_loss(self, target, prediction):\n",
    "        # don't use this as a loss function, keeping this as an example of what not to do\n",
    "        error = prediction - target\n",
    "        return K.mean(K.sqrt(1+K.square(error))-1, axis=-1)\n",
    "    def huber_loss(self,y_true, y_pred, max_grad=1.):\n",
    "        \"\"\"Calculate the huber loss.\n",
    "\n",
    "        See https://en.wikipedia.org/wiki/Huber_loss\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true: np.array, tf.Tensor\n",
    "          Target value.\n",
    "        y_pred: np.array, tf.Tensor\n",
    "          Predicted value.\n",
    "        max_grad: float, optional\n",
    "          Positive floating point value. Represents the maximum possible\n",
    "          gradient magnitude.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tf.Tensor\n",
    "          The huber loss.\n",
    "        \"\"\"\n",
    "        with K.name_scope(\"HuberLoss\"):\n",
    "            delta = max_grad \n",
    "            diff = tf.abs(y_true - y_pred, name = \"diff\")\n",
    "            mask = diff < delta\n",
    "            return tf.where(mask, 0.5 * tf.square(diff), delta * (diff - 0.5 * delta))\n",
    "\n",
    "    def mean_huber_loss(self,y_true, y_pred, max_grad=1.):\n",
    "        \"\"\"Return mean huber loss.\n",
    "\n",
    "        Same as huber_loss, but takes the mean over all values in the\n",
    "        output tensor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true: np.array, tf.Tensor\n",
    "          Target value.\n",
    "        y_pred: np.array, tf.Tensor\n",
    "          Predicted value.\n",
    "        max_grad: float, optional\n",
    "          Positive floating point value. Represents the maximum possible\n",
    "          gradient magnitude.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tf.Tensor\n",
    "          The mean huber loss.\n",
    "        \"\"\"\n",
    "        return tf.reduce_mean(self.huber_loss(y_true, y_pred, max_grad))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "2a48d25d-ea4e-4339-9eab-8e3cb59b6a9c"
    }
   },
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:53.272051Z",
     "start_time": "2018-01-28T17:08:53.255355Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "883b5a10-c77c-47e9-9f09-c5088e06248a"
    }
   },
   "outputs": [],
   "source": [
    "class ShortMemory():\n",
    "    def __init__(self,hist_len,state_dim):\n",
    "        self.history_length = hist_len\n",
    "        self.state_dim = state_dim\n",
    "        self.mem_hist = np.zeros((hist_len,) + state_dim , dtype = np.float32)\n",
    "        self.current = 0\n",
    "    def add(self,state):\n",
    "        self.mem_hist[self.current % self.history_length] = state\n",
    "        self.current += 1\n",
    "    def get(self):\n",
    "        '''\n",
    "        This function provides the recent history of length history_length.\n",
    "        The sample in the beginning will be padded at the beginning. (0,0,0..data)\n",
    "        '''\n",
    "        return self.mem_hist\n",
    "    \n",
    "    def forget(self):\n",
    "        self.mem_hist = np.zeros((self.history_length,) + self.state_dim, dtype = np.float32)\n",
    "        self.current = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:08:55.271656Z",
     "start_time": "2018-01-28T17:08:55.136097Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "bb1e39bd-fade-4967-a887-72616013cea1"
    }
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self,capacity,hist_len,s_dim):\n",
    "        '''\n",
    "        capacity: how many episodes to store?\n",
    "        hist_len: what is the history length of each episode?\n",
    "        s_dim: the size of your state in a tuple ex. (80,80,1) \n",
    "        '''\n",
    "        self.memory_size = capacity\n",
    "        self.history_length = hist_len\n",
    "        self.state_dim = s_dim\n",
    "        self.mem_a = np.zeros(self.memory_size, dtype = np.int8)\n",
    "        self.mem_r = np.zeros(self.memory_size, dtype = np.int8)\n",
    "        self.mem_s = np.zeros((self.memory_size,) + s_dim , dtype = np.uint8)\n",
    "        self.dones = np.zeros(self.memory_size, dtype = np.bool)\n",
    "        self.current = 0\n",
    "    def get_state(self,idx):\n",
    "        state = self.mem_s[idx - self.history_length + 1:idx + 1, :, :]\n",
    "        assert len(state) <= self.history_length\n",
    "        #print(len(state))\n",
    "        if len(state) < self.history_length:\n",
    "            pad = self.history_length - len(state)\n",
    "            pad_shape = (pad,) + self.state_dim\n",
    "            #print(\"pad {}\".format(pad_shape))\n",
    "            pad_arr = np.zeros((pad,) + self.state_dim)\n",
    "\n",
    "            state = np.concatenate((pad_arr,state),axis=0)\n",
    "            #print(state.shape)\n",
    "\n",
    "        return state\n",
    "    def add(self,s,a,r,done):\n",
    "        self.mem_a[self.current % self.memory_size] = a\n",
    "        self.mem_r[self.current % self.memory_size] = r\n",
    "        self.mem_s[self.current % self.memory_size] = s\n",
    "        self.dones[self.current % self.memory_size] = done\n",
    "        self.current += 1 \n",
    "    def sample(self, batch_size):\n",
    "        indexes = []\n",
    "        # ensure enough frames to sample\n",
    "        assert self.current > self.history_length\n",
    "        # -1 because still need next frame\n",
    "        end = min(self.current, self.memory_size) - 1\n",
    "\n",
    "        while len(indexes) < batch_size: \n",
    "            index = np.random.randint(self.history_length - 1, end)\n",
    "            # sampled state shouldn't contain episode end\n",
    "            if self.dones[index - self.history_length + 1: index + 1].any():\n",
    "                continue\n",
    "            indexes.append(index)\n",
    "\n",
    "        smp_s = []\n",
    "        smp_a = [] \n",
    "        smp_r = []\n",
    "        smp_s_ = []\n",
    "        smp_done = []\n",
    "        for idx in indexes:\n",
    "            smp_s.append(self.get_state(idx))\n",
    "            smp_a.append(self.mem_a[idx])\n",
    "            smp_r.append(self.mem_r[idx])\n",
    "            smp_s_.append(self.get_state(idx + 1))\n",
    "            smp_done.append(self.dones[idx])\n",
    "        return np.array(smp_s),np.array(smp_a),np.array(smp_r),np.array(smp_s_),np.array(smp_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c2d1f6aa-9fb6-441d-ab31-9f6db9ed767d"
    }
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:09:01.593123Z",
     "start_time": "2018-01-28T17:09:01.575792Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are way too many parameters for the agent to be within the init arguments\n",
    "class AgentConfig():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        stateCnt \n",
    "        actionCnt \n",
    "        mem_size\n",
    "        epsilon_policy \n",
    "        gamma \n",
    "        num_frames\n",
    "        learning_rate \n",
    "        train_start \n",
    "        train_freq \n",
    "        target_update_freq  \n",
    "        batch_size \n",
    "        preprocessor \n",
    "        log_path \n",
    "        name      \n",
    "        '''\n",
    "        self.stateCnt = None\n",
    "        self.actionCnt = None\n",
    "        self.mem_size = None\n",
    "        self.epsilon_policy = None\n",
    "        self.gamma = None\n",
    "        self.num_frames = None\n",
    "        self.learning_rate = None\n",
    "        self.train_start = None\n",
    "        self.train_freq = None\n",
    "        self.target_update_freq = None \n",
    "        self.batch_size = None\n",
    "        self.preprocessor = None\n",
    "        self.log_path = None\n",
    "        self.name = None\n",
    "    def validate(self):\n",
    "        # validate parameters\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:13:52.762437Z",
     "start_time": "2018-01-23T17:13:52.753525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg = AgentConfig()\n",
    "cfg.stateCnt = \n",
    "cfg.actionCnt = None\n",
    "cfg.mem_size = None\n",
    "cfg.epsilon_policy = None\n",
    "cfg.gamma = None\n",
    "cfg.num_frames = None\n",
    "cfg.learning_rate = None\n",
    "cfg.train_start = None\n",
    "cfg.train_freq = None\n",
    "cfg.target_update_freq = None \n",
    "cfg.batch_size = None\n",
    "cfg.preprocessor = None\n",
    "cfg.log_path = None\n",
    "cfg.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:14:13.701092Z",
     "start_time": "2018-01-28T17:14:11.624083Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "c617fab6-f945-4e2d-b4f1-5a157d02485c"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self,cfg):\n",
    "        self.stateCnt = cfg.stateCnt\n",
    "        if cfg.preprocessor:\n",
    "            self.preprocessor = cfg.preprocessor\n",
    "            self.stateCnt = cfg.preprocessor.new_shape\n",
    "        \n",
    "        self.actionCnt = cfg.actionCnt\n",
    "        \n",
    "\n",
    "\n",
    "        # initialize memory\n",
    "        self.longmem = Memory(cfg.mem_size,cfg.num_frames,self.stateCnt)\n",
    "        self.shortmem = ShortMemory(cfg.num_frames,self.stateCnt)\n",
    "        \n",
    "        \n",
    "        self.epsilon_policy = cfg.epsilon_policy\n",
    "        self.gamma = cfg.gamma\n",
    "        self.target_update_freq = cfg.target_update_freq\n",
    "        \n",
    "        self.name = cfg.name\n",
    "        self.learning_rate = cfg.learning_rate \n",
    "        self.train_start = cfg.train_start\n",
    "        self.train_freq = cfg.train_freq\n",
    "        self.batch_size = cfg.batch_size\n",
    "        \n",
    "        # build network\n",
    "        inpt = (cfg.num_frames,) + self.stateCnt\n",
    "        self.net_inpt = inpt\n",
    "        self.model = Network(inpt,self.actionCnt,True,'duel',cfg.learning_rate).build2() # model\n",
    "        self.target_model = Network(inpt,self.actionCnt,True,'duel',cfg.learning_rate).build2() # target model\n",
    "        self.final_model = None\n",
    "        self.compile()\n",
    "        #logging\n",
    "        if cfg.log_path:\n",
    "            self.writer = tf.summary.FileWriter(cfg.log_path)\n",
    "            self.log_path = cfg.log_path\n",
    "        # init counters for logging purpose\n",
    "        self.loss_count = 0\n",
    "        self.counter = 0\n",
    "    def save_model(self):\n",
    "        self.model.save_weights(self.log_path + \"/\" + self.name)\n",
    "    def remember(self,s,a,r,done):\n",
    "        self.longmem.add(s,a,r,done)\n",
    "    def save_scalar(self,step, name, value):\n",
    "        \"\"\"Save a scalar value to tensorboard.\n",
    "          Parameters\n",
    "          ----------\n",
    "          step: int\n",
    "            Training step (sets the position on x-axis of tensorboard graph.\n",
    "          name: str\n",
    "            Name of variable. Will be the name of the graph in tensorboard.\n",
    "          value: float\n",
    "            The value of the variable at this step.\n",
    "          writer: tf.FileWriter\n",
    "            The tensorboard FileWriter instance.\n",
    "          \"\"\"\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = float(value)\n",
    "        summary_value.tag = name\n",
    "        self.writer.add_summary(summary, step)\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def compile(self, optimizer = None, loss_func = None):\n",
    "        \"\"\"Setup all of the TF graph variables/ops.\n",
    "\n",
    "        This is inspired by the compile method on the\n",
    "        keras.models.Model class.\n",
    "\n",
    "        This is the place to create the target network, setup \n",
    "        loss function and any placeholders.\n",
    "        \"\"\"\n",
    "        if loss_func is None:\n",
    "            loss_func = mean_huber_loss\n",
    "            # loss_func = 'mse'\n",
    "        if optimizer is None:\n",
    "            optimizer = Adam(lr = self.learning_rate)\n",
    "            # optimizer = RMSprop(lr=0.00025)\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            state = Input(shape = (*self.net_inpt, ) , name = \"states\")\n",
    "            action_mask = Input(shape = (self.actionCnt,), name = \"actions\")\n",
    "            qa_value = self.model(state)\n",
    "            qa_value = multiply([qa_value, action_mask], name = \"multiply\")\n",
    "            qa_value = Lambda(lambda x: tf.reduce_sum(x, axis=1, keep_dims = True), name = \"sum\")(qa_value)\n",
    "\n",
    "        self.final_model = Model(inputs = [state, action_mask], outputs = qa_value)\n",
    "        self.final_model.compile(loss=loss_func, optimizer=optimizer)\n",
    "\n",
    "    def act(self, s, episode=0):\n",
    "        # Epsilon greedy action selection\n",
    "        s = s[None] # increase the rank of tensor to have a batch_size of 1 and length 1\n",
    "        if episode >= self.train_start:\n",
    "            self.epsilon_policy.update()\n",
    "        if np.random.rand() <= self.epsilon_policy.epsilon:\n",
    "            return random.randrange(self.actionCnt)\n",
    "        act_values = self.model.predict_on_batch(s)\n",
    "        return np.argmax(act_values[0]) # returns action\n",
    "    def replay(self,batch_size):\n",
    "        prebatch_s,batch_a,batch_r,prebatch_s_,batch_done = self.longmem.sample(batch_size)# a batch of episode of parameter length\n",
    "        \n",
    "        batch_s = self.preprocessor.process_batch(prebatch_s)\n",
    "        batch_s_ = self.preprocessor.process_batch(prebatch_s_)\n",
    "        \n",
    "        a_ = self.model.predict(batch_s_)\n",
    "        a_idx = np.argmax(a_,axis=1)\n",
    "        behaviour_q = self.target_model.predict(batch_s_)\n",
    "        action_mask = np.zeros((batch_size, self.actionCnt))\n",
    "        action_mask[range(batch_size), batch_a] = 1.0\n",
    "        #target = self.model.predict(batch_s) [range(batch_size),batch_a.astype('int')] # smaller update for other residuals? research if this actually makes in impact?\n",
    "        \n",
    "        target = batch_r + self.gamma * (behaviour_q[range(batch_size),a_idx]) \n",
    "        loss = self.final_model.train_on_batch([batch_s,action_mask],target)\n",
    "        \n",
    "        return loss, np.mean(target)\n",
    "\n",
    "    def train2(self, env, num_iterations, max_episode_length=None):\n",
    "        \"\"\"Fit your model to the provided environment.\n",
    "\n",
    "        This is where you sample actions from your network,\n",
    "        collect experience samples and add them to your replay memory,\n",
    "        and update your network parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: gym.Env\n",
    "          This is the Atari environment. \n",
    "        num_iterations: int\n",
    "          How many samples/updates to perform.\n",
    "        max_episode_length: int\n",
    "          How long a single episode should last before the agent\n",
    "          resets. Can help exploration.\n",
    "        \"\"\"\n",
    "        is_training = True\n",
    "        print(\"Training starts.\")\n",
    "        #self.save_model(0)\n",
    "        eval_count = 0\n",
    "\n",
    "        pre_s = env.reset()\n",
    "        mem_s = self.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "        net_s = self.preprocessor.process_for_network(mem_s) # normalized\n",
    "        burn_in = True\n",
    "        idx_episode = 1\n",
    "        episode_loss = .0\n",
    "        episode_frames = 0\n",
    "        episode_reward = .0\n",
    "        episode_raw_reward = .0\n",
    "        episode_target_value = .0\n",
    "        for t in range(self.num_burn_in + num_iterations):\n",
    "            self.shortmem.add(net_s)\n",
    "            hist_\n",
    "            action_state = self.history_processor.process_state_for_network(\n",
    "                self.atari_processor.process_state_for_network(state))\n",
    "            policy_type = \"UniformRandomPolicy\" if burn_in else \"LinearDecayGreedyEpsilonPolicy\"\n",
    "            action = self.select_action(action_state, is_training, policy_type = policy_type)\n",
    "            processed_state = self.atari_processor.process_state_for_memory(state)\n",
    "\n",
    "            state, reward, done, info = env.step(action)\n",
    "\n",
    "            processed_next_state = self.atari_processor.process_state_for_network(state)\n",
    "            action_next_state = np.dstack((action_state, processed_next_state))\n",
    "            action_next_state = action_next_state[:, :, 1:]\n",
    "\n",
    "            processed_reward = self.atari_processor.process_reward(reward)\n",
    "\n",
    "            self.memory.append(processed_state, action, processed_reward, done)\n",
    "            current_sample = Sample(action_state, action, processed_reward, action_next_state, done)\n",
    "            \n",
    "            if not burn_in: \n",
    "                episode_frames += 1\n",
    "                episode_reward += processed_reward\n",
    "                episode_raw_reward += reward\n",
    "                if episode_frames > max_episode_length:\n",
    "                    done = True\n",
    "\n",
    "            if done:\n",
    "                # adding last frame only to save last state\n",
    "                last_frame = self.atari_processor.process_state_for_memory(state)\n",
    "                # action, reward, done doesn't matter here\n",
    "                self.memory.append(last_frame, action, 0, done)\n",
    "                if not burn_in:\n",
    "                    avg_target_value = episode_target_value / episode_frames\n",
    "                    print(\">>> Training: time %d, episode %d, length %d, reward %.0f, raw_reward %.0f, loss %.4f, target value %.4f, policy step %d, memory cap %d\" % \n",
    "                        (t, idx_episode, episode_frames, episode_reward, episode_raw_reward, episode_loss, \n",
    "                        avg_target_value, self.policy.step, self.memory.current))\n",
    "                    sys.stdout.flush()\n",
    "                    save_scalar(idx_episode, 'train/episode_frames', episode_frames, self.writer)\n",
    "                    save_scalar(idx_episode, 'train/episode_reward', episode_reward, self.writer)\n",
    "                    save_scalar(idx_episode, 'train/episode_raw_reward', episode_raw_reward, self.writer)\n",
    "                    save_scalar(idx_episode, 'train/episode_loss', episode_loss, self.writer)\n",
    "                    save_scalar(idx_episode, 'train_avg/avg_reward', episode_reward / episode_frames, self.writer)\n",
    "                    save_scalar(idx_episode, 'train_avg/avg_target_value', avg_target_value, self.writer)\n",
    "                    save_scalar(idx_episode, 'train_avg/avg_loss', episode_loss / episode_frames, self.writer)\n",
    "                    episode_frames = 0\n",
    "                    episode_reward = .0\n",
    "                    episode_raw_reward = .0\n",
    "                    episode_loss = .0\n",
    "                    episode_target_value = .0\n",
    "                    idx_episode += 1\n",
    "                burn_in = (t < self.num_burn_in)\n",
    "                state = env.reset()\n",
    "                self.atari_processor.reset()\n",
    "                self.history_processor.reset()\n",
    "\n",
    "            if not burn_in:\n",
    "                if t % self.train_freq == 0:\n",
    "                    loss, target_value = self.update_policy(current_sample)\n",
    "                    episode_loss += loss\n",
    "                    episode_target_value += target_value\n",
    "                # update freq is based on train_freq\n",
    "                if t % (self.train_freq * self.target_update_freq) == 0:\n",
    "                    # target updates can have the option to be hard or soft\n",
    "                    # related functions are defined in deeprl_prj.utils\n",
    "                    # here we use hard target update as default\n",
    "                    self.target_network.set_weights(self.q_network.get_weights())\n",
    "                if t % self.save_freq == 0:\n",
    "                    self.save_model(idx_episode)\n",
    "                if t % (self.eval_freq * self.train_freq) == 0:\n",
    "                    episode_reward_mean, episode_reward_std, eval_count = self.evaluate(env, 20, eval_count, max_episode_length, True)\n",
    "                    save_scalar(t, 'eval/eval_episode_reward_mean', episode_reward_mean, self.writer)\n",
    "                    save_scalar(t, 'eval/eval_episode_reward_std', episode_reward_std, self.writer)\n",
    "\n",
    "    def train(self,env,episodes,render = False):\n",
    "        e_idx = 0\n",
    "        # play loop\n",
    "        print(\"Training\")\n",
    "        for e in range(episodes):\n",
    "            # stats\n",
    "            e_loss = .0\n",
    "            e_length = .0\n",
    "            e_r = .0\n",
    "            e_raw_r = .0\n",
    "            e_target_val = .0\n",
    "            \n",
    "            \n",
    "            #agent.model.reset_states()\n",
    "            pre_s = env.reset()\n",
    "            mem_s = self.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "            net_s = self.preprocessor.process_for_network(mem_s) # normalized\n",
    "            done = False\n",
    "            self.shortmem.forget # forget short term memory for recurrent network \n",
    "            self.shortmem.add(net_s)\n",
    "            if e % 40 == 0:\n",
    "                self.save_model()\n",
    "            for t in itertools.count():\n",
    "                hist_s = self.shortmem.get() # receive the last frames including the most recent frame to make a tensor shape (num_frames,frame_dim)\n",
    "                # take action using net_s and receive a\n",
    "                a = self.act(hist_s,e)\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                pre_s_, raw_r, done, info = env.step(a)\n",
    "                \n",
    "                r = self.preprocessor.process_reward(raw_r) # reward clipping\n",
    "                mem_s_ = self.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "                net_s_ = self.preprocessor.process_for_network(mem_s) # normalized\n",
    "        \n",
    "                agent.shortmem.add(net_s_)\n",
    "                agent.remember(mem_s,a,r,done)\n",
    "                \n",
    "                # collect stats\n",
    "                e_raw_r += raw_r\n",
    "                e_r += r\n",
    "                \n",
    "\n",
    "                if e > self.train_start:\n",
    "                    \n",
    "                    if t % (self.train_freq *self.target_update_freq) == 0:\n",
    "                        self.update_target_model()\n",
    "                    \n",
    "                    if t % self.train_freq == 0  :\n",
    "                        loss, avg_target = self.replay(self.batch_size)\n",
    "                        e_loss += loss\n",
    "                        e_target_val += avg_target\n",
    "            \n",
    "                if done:\n",
    "                    print(\"Training: episode %d, length %d, reward %.0f, raw_reward %.0f, loss %.4f, target value %.4f,, memory cap %d\" % \n",
    "                        (e, t, e_r, e_raw_r, e_loss, e_target_val/t, self.longmem.current))\n",
    "                    if e > self.train_start:\n",
    "                        e_idx += 1\n",
    "                        self.save_scalar(e_idx,\"train/episode_reward\",e_r)\n",
    "                        self.save_scalar(e_idx,\"train/episode_raw_reward\",e_raw_r)\n",
    "                        self.save_scalar(e_idx,\"train/episode_frames\",t)\n",
    "                        self.save_scalar(e_idx,\"train/episode_loss\",e_loss)\n",
    "                        self.save_scalar(e_idx,\"train_avg/avg_target_value\",e_target_val/t)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "ad276c2f-20e3-46f9-b200-f42edb02cac8"
    }
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-29T07:36:50.138Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "2c99af50-d7d2-4a1b-8c59-bfd85685951b"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(rgb_array,scale = 1/12):\n",
    "    frame_shape = rgb_array.shape\n",
    "    \n",
    "    frame = np.array(rgb_array)\n",
    "    gray_frame = np.dot(frame[...,:3],[0.299,0.587,0.114]).reshape((frame_shape[0],frame_shape[1]))\n",
    "    smaller = scimisc.imresize(gray_frame,scale,mode='L').astype('float64')\n",
    "    smaller /= 255.0\n",
    "    smaller = np.expand_dims(smaller,2) # convert to a 3D array of shape (height,width,grayscale)\n",
    "    smaller = np.reshape(smaller, [1, *(smaller.shape)])\n",
    "    return smaller.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-19T00:05:33.116500Z",
     "start_time": "2017-12-19T00:05:33.107652Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "2cd1233d-5e8d-49ee-8aea-14490651bbc7"
    }
   },
   "outputs": [],
   "source": [
    "def render(obs,root,canvas):\n",
    "    obs = np.squeeze(obs,2)\n",
    "    image = Image.fromarray(obs.astype('int8'),mode='L')\n",
    "    photo = ImageTk.PhotoImage(image)\n",
    "    root.one = photo\n",
    "    canvas.delete(\"all\")\n",
    "    canvas.create_image(frame_height,frame_width, image=photo)\n",
    "    root.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T16:42:10.666400Z",
     "start_time": "2018-01-23T16:42:10.630553Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "5983da69-59f0-4dfe-9150-4afb28b37ad4"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_output_folder(args, parent_dir, env_name, task_name):\n",
    "    \"\"\"Return save folder.\n",
    "    Assumes folders in the parent_dir have suffix -run{run\n",
    "    number}. Finds the highest run number and sets the output folder\n",
    "    to that number + 1. This is just convenient so that if you run the\n",
    "    same script multiple times tensorboard can plot all of the results\n",
    "    on the same plots with different names.\n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_dir: str\n",
    "      Path of the directory containing all experiment runs.\n",
    "    Returns\n",
    "    -------\n",
    "    parent_dir/run_dir\n",
    "      Path to this run's save directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "        print('===== Folder did not exist; creating... %s'%parent_dir)\n",
    "    experiment_id = 0\n",
    "    for folder_name in os.listdir(parent_dir):\n",
    "        if not os.path.isdir(os.path.join(parent_dir, folder_name)):\n",
    "            continue\n",
    "        try:\n",
    "            folder_name = int(folder_name.split('-run')[-1])\n",
    "            if folder_name > experiment_id:\n",
    "                experiment_id = folder_name\n",
    "        except:\n",
    "            pass\n",
    "    experiment_id += 1\n",
    "\n",
    "    parent_dir = os.path.join(parent_dir, env_name)\n",
    "    parent_dir = parent_dir + '-run{}'.format(experiment_id) + '-' + task_name\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "        print('===== Folder did not exist; creating... %s'%parent_dir)\n",
    "    else:\n",
    "        print('===== Folder exists; delete? %s'%parent_dir)\n",
    "        input(\"Press Enter to continue...\")\n",
    "        os.system('rm -rf %s/' % (parent_dir))\n",
    "    os.makedirs(parent_dir+'/videos/')\n",
    "    os.makedirs(parent_dir+'/images/')\n",
    "    return parent_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "44f0a6b3-671e-4a05-85fc-c52d1e45922c"
    }
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "cadab8d7-29a8-48e9-831c-4c8e61aee554"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "root = Tk()\n",
    "root_frame = Frame(root)\n",
    "canvas = Canvas(root_frame, borderwidth=0, highlightthickness=0, width=200, height=130, bg=\"black\" )\n",
    "root_frame.pack()\n",
    "canvas.pack()\n",
    "\n",
    "frame_height = 25\n",
    "frame_width = 35\n",
    "\n",
    "\n",
    "env = gym.make(\"MinecraftBasic-v0\")\n",
    "env.load_mission_file(\"./CliffWalking.xml\")\n",
    "env.init(videoResolution=[420,300],allowContinuousMovement=[\"move\", \"turn\", \"strafe\"])\n",
    "\n",
    "\n",
    "scale = 1/12 # scale image down by 1/12\n",
    "newshape = (env.video_height*scale,env.video_width*scale,1) # dimension of 1 for grayscale\n",
    "newshape = tuple(map(int,newshape))\n",
    "\n",
    "# the pre processor will adjust the observation space therefore we will edit the property of the environment to take the pre processor into accoutn\n",
    "env.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "shape=newshape)\n",
    "\n",
    "done = False\n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        env.reset()\n",
    "        while True:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            proc_obs = preprocess(obs)\n",
    "            \n",
    "            render(proc_obs,root_frame,canvas)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "    except:\n",
    "        root.destroy()\n",
    "        env.close()\n",
    "        raise\n",
    "env.close()\n",
    "root.destroy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "3ee7c012-4d9e-449a-b088-39ddbb831955"
    }
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T23:32:27.244253Z",
     "start_time": "2018-01-05T23:32:27.238751Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "c2c4723e-f544-4411-90e5-aaeeea755a2d"
    }
   },
   "outputs": [],
   "source": [
    "def update(x,y,handle,plot):\n",
    "    plot.data_source.data['x'] += [x]\n",
    "    plot.data_source.data['y'] += [y]\n",
    "    push_notebook(handle=handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T23:49:20.416109Z",
     "start_time": "2018-01-05T23:49:20.320487Z"
    },
    "collapsed": true,
    "hidden": true,
    "nbpresent": {
     "id": "6299a033-d360-4634-ad44-ed66e2c07e54"
    }
   },
   "outputs": [],
   "source": [
    "inferno = bokeh.palettes.Inferno9\n",
    "fig1 = figure(plot_width=400, plot_height=400,title=\"rewards\",\n",
    "                      x_axis_label=\"x\",\n",
    "                      y_axis_label=\"y\")\n",
    "rplot = fig1.line([],[],line_width=2)\n",
    "# make a grid\n",
    "handle1 = show(fig1, notebook_handle=True)\n",
    "\n",
    "reward_plot = {\"handle\":handle1,\"plot\":rplot}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "62f8ce75-9484-49e4-9d37-bf7f29257235"
    }
   },
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:09:07.052720Z",
     "start_time": "2018-01-28T17:09:07.005015Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "6669fd58-0676-4d6b-bbb3-77a644fd0f12"
    }
   },
   "outputs": [],
   "source": [
    "pre_env = gym.make(\"MinecraftCliffWalking1-v0\")\n",
    "pre_env.init(videoResolution=[400,400],allowContinuousMovement=[\"move\", \"turn\", \"strafe\"],observeGrid=[20,-1,20,20,-1,20],observeDistance=[4,45,12])\n",
    "env = MinecraftWrapper(pre_env,1/5,(41,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:09:07.455170Z",
     "start_time": "2018-01-28T17:09:07.237057Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "104c625c-304d-4849-9a6f-d1341f497b04"
    }
   },
   "outputs": [],
   "source": [
    "atari_env = gym.make('Seaquest-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:09:09.172460Z",
     "start_time": "2018-01-28T17:09:09.136443Z"
    },
    "code_folding": [],
    "collapsed": true,
    "nbpresent": {
     "id": "81b9ad0f-1383-46bd-a91b-42cc299f789e"
    }
   },
   "outputs": [],
   "source": [
    "def get_output_folder(args, parent_dir, env_name, task_name):\n",
    "    \"\"\"Return save folder.\n",
    "    Assumes folders in the parent_dir have suffix -run{run\n",
    "    number}. Finds the highest run number and sets the output folder\n",
    "    to that number + 1. This is just convenient so that if you run the\n",
    "    same script multiple times tensorboard can plot all of the results\n",
    "    on the same plots with different names.\n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_dir: str\n",
    "      Path of the directory containing all experiment runs.\n",
    "    Returns\n",
    "    -------\n",
    "    parent_dir/run_dir\n",
    "      Path to this run's save directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "        print('===== Folder did not exist; creating... %s'%parent_dir)\n",
    "    experiment_id = 0\n",
    "    for folder_name in os.listdir(parent_dir):\n",
    "        if not os.path.isdir(os.path.join(parent_dir, folder_name)):\n",
    "            continue\n",
    "        try:\n",
    "            folder_name = int(folder_name.split('-run')[-1])\n",
    "            if folder_name > experiment_id:\n",
    "                experiment_id = folder_name\n",
    "        except:\n",
    "            pass\n",
    "    experiment_id += 1\n",
    "\n",
    "    parent_dir = os.path.join(parent_dir, env_name)\n",
    "    parent_dir = parent_dir + '-run{}'.format(experiment_id) + '-' + task_name\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "        print('===== Folder did not exist; creating... %s'%parent_dir)\n",
    "    else:\n",
    "        print('===== Folder exists; delete? %s'%parent_dir)\n",
    "        input(\"Press Enter to continue...\")\n",
    "        os.system('rm -rf %s/' % (parent_dir))\n",
    "    #s.makedirs(parent_dir+'/videos/')\n",
    "    #os.makedirs(parent_dir+'/images/')\n",
    "    return parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:14:18.762780Z",
     "start_time": "2018-01-28T17:14:17.571100Z"
    },
    "nbpresent": {
     "id": "4a589315-8054-4f3d-b48b-04156bd3c48b"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Folder exists; delete? ./logs/Seaquest-v8-run1-DQRN-Batch\n",
      "Press Enter to continue...\n"
     ]
    }
   ],
   "source": [
    "doc = get_output_folder(None,\"./logs\",\"Seaquest-v8\",\"DQRN-Batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:14:19.025041Z",
     "start_time": "2018-01-28T17:14:19.021274Z"
    },
    "nbpresent": {
     "id": "218afe9f-e400-466c-994b-d87dd8016173"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logs/Seaquest-v8-run1-DQRN-Batch'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T17:14:21.833008Z",
     "start_time": "2018-01-28T17:14:19.794696Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "58744aa0-4cf4-41c2-8179-84e149038b06"
    }
   },
   "outputs": [],
   "source": [
    "episodes = 100000\n",
    "cfg = AgentConfig()\n",
    "cfg.stateCnt = atari_env.observation_space.shape\n",
    "cfg.actionCnt = atari_env.action_space.n\n",
    "cfg.mem_size = 1000000\n",
    "cfg.epsilon_policy = LinearDecayGreedyEpsilonPolicy(1.0,0.05,episodes)\n",
    "cfg.gamma = 0.99\n",
    "cfg.num_frames = 10\n",
    "cfg.learning_rate = 0.001\n",
    "cfg.train_start = 200\n",
    "cfg.train_freq = 3\n",
    "cfg.target_update_freq = 70\n",
    "cfg.batch_size = 32\n",
    "cfg.preprocessor = AtariPreprocessor((84,84,1))\n",
    "cfg.log_path = doc\n",
    "cfg.name = \"A3\"\n",
    "agent = Agent(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-28T17:14:23.269Z"
    },
    "nbpresent": {
     "id": "e44c5e29-4d2f-41ec-a1fb-7c42dd7113f6"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Training: episode 0, length 775, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 776\n",
      "Training: episode 1, length 583, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 1360\n",
      "Training: episode 2, length 662, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 2023\n",
      "Training: episode 3, length 870, reward 7, raw_reward 140, loss 0.0000, target value 0.0000,, memory cap 2894\n",
      "Training: episode 4, length 585, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 3480\n",
      "Training: episode 5, length 1016, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 4497\n",
      "Training: episode 6, length 640, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 5138\n",
      "Training: episode 7, length 498, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 5637\n",
      "Training: episode 8, length 733, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 6371\n",
      "Training: episode 9, length 667, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 7039\n",
      "Training: episode 10, length 585, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 7625\n",
      "Training: episode 11, length 623, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 8249\n",
      "Training: episode 12, length 771, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 9021\n",
      "Training: episode 13, length 558, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 9580\n",
      "Training: episode 14, length 692, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 10273\n",
      "Training: episode 15, length 731, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 11005\n",
      "Training: episode 16, length 516, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 11522\n",
      "Training: episode 17, length 549, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 12072\n",
      "Training: episode 18, length 533, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 12606\n",
      "Training: episode 19, length 680, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 13287\n",
      "Training: episode 20, length 723, reward 6, raw_reward 120, loss 0.0000, target value 0.0000,, memory cap 14011\n",
      "Training: episode 21, length 601, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 14613\n",
      "Training: episode 22, length 937, reward 6, raw_reward 120, loss 0.0000, target value 0.0000,, memory cap 15551\n",
      "Training: episode 23, length 1032, reward 8, raw_reward 160, loss 0.0000, target value 0.0000,, memory cap 16584\n",
      "Training: episode 24, length 547, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 17132\n",
      "Training: episode 25, length 677, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 17810\n",
      "Training: episode 26, length 774, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 18585\n",
      "Training: episode 27, length 647, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 19233\n",
      "Training: episode 28, length 689, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 19923\n",
      "Training: episode 29, length 1238, reward 12, raw_reward 240, loss 0.0000, target value 0.0000,, memory cap 21162\n",
      "Training: episode 30, length 419, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 21582\n",
      "Training: episode 31, length 629, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 22212\n",
      "Training: episode 32, length 798, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 23011\n",
      "Training: episode 33, length 998, reward 7, raw_reward 140, loss 0.0000, target value 0.0000,, memory cap 24010\n",
      "Training: episode 34, length 693, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 24704\n",
      "Training: episode 35, length 840, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 25545\n",
      "Training: episode 36, length 461, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 26007\n",
      "Training: episode 37, length 524, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 26532\n",
      "Training: episode 38, length 771, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 27304\n",
      "Training: episode 39, length 975, reward 7, raw_reward 140, loss 0.0000, target value 0.0000,, memory cap 28280\n",
      "Training: episode 40, length 476, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 28757\n",
      "Training: episode 41, length 777, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 29535\n",
      "Training: episode 42, length 442, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 29978\n",
      "Training: episode 43, length 692, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 30671\n",
      "Training: episode 44, length 679, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 31351\n",
      "Training: episode 45, length 588, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 31940\n",
      "Training: episode 46, length 730, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 32671\n",
      "Training: episode 47, length 627, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 33299\n",
      "Training: episode 48, length 415, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 33715\n",
      "Training: episode 49, length 555, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 34271\n",
      "Training: episode 50, length 628, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 34900\n",
      "Training: episode 51, length 856, reward 6, raw_reward 120, loss 0.0000, target value 0.0000,, memory cap 35757\n",
      "Training: episode 52, length 1139, reward 10, raw_reward 200, loss 0.0000, target value 0.0000,, memory cap 36897\n",
      "Training: episode 53, length 698, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 37596\n",
      "Training: episode 54, length 552, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 38149\n",
      "Training: episode 55, length 933, reward 8, raw_reward 160, loss 0.0000, target value 0.0000,, memory cap 39083\n",
      "Training: episode 56, length 643, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 39727\n",
      "Training: episode 57, length 432, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 40160\n",
      "Training: episode 58, length 928, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 41089\n",
      "Training: episode 59, length 606, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 41696\n",
      "Training: episode 60, length 596, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 42293\n",
      "Training: episode 61, length 526, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 42820\n",
      "Training: episode 62, length 631, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 43452\n",
      "Training: episode 63, length 482, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 43935\n",
      "Training: episode 64, length 1091, reward 11, raw_reward 220, loss 0.0000, target value 0.0000,, memory cap 45027\n",
      "Training: episode 65, length 447, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 45475\n",
      "Training: episode 66, length 614, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 46090\n",
      "Training: episode 67, length 708, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 46799\n",
      "Training: episode 68, length 678, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 47478\n",
      "Training: episode 69, length 559, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 48038\n",
      "Training: episode 70, length 512, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 48551\n",
      "Training: episode 71, length 482, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 49034\n",
      "Training: episode 72, length 463, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 49498\n",
      "Training: episode 73, length 662, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 50161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: episode 74, length 649, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 50811\n",
      "Training: episode 75, length 423, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 51235\n",
      "Training: episode 76, length 592, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 51828\n",
      "Training: episode 77, length 684, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 52513\n",
      "Training: episode 78, length 460, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 52974\n",
      "Training: episode 79, length 713, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 53688\n",
      "Training: episode 80, length 916, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 54605\n",
      "Training: episode 81, length 609, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 55215\n",
      "Training: episode 82, length 498, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 55714\n",
      "Training: episode 83, length 1183, reward 12, raw_reward 240, loss 0.0000, target value 0.0000,, memory cap 56898\n",
      "Training: episode 84, length 930, reward 8, raw_reward 160, loss 0.0000, target value 0.0000,, memory cap 57829\n",
      "Training: episode 85, length 687, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 58517\n",
      "Training: episode 86, length 820, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 59338\n",
      "Training: episode 87, length 481, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 59820\n",
      "Training: episode 88, length 934, reward 6, raw_reward 120, loss 0.0000, target value 0.0000,, memory cap 60755\n",
      "Training: episode 89, length 1002, reward 7, raw_reward 140, loss 0.0000, target value 0.0000,, memory cap 61758\n",
      "Training: episode 90, length 508, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 62267\n",
      "Training: episode 91, length 781, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 63049\n",
      "Training: episode 92, length 505, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 63555\n",
      "Training: episode 93, length 583, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 64139\n",
      "Training: episode 94, length 937, reward 8, raw_reward 160, loss 0.0000, target value 0.0000,, memory cap 65077\n",
      "Training: episode 95, length 434, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 65512\n",
      "Training: episode 96, length 1433, reward 15, raw_reward 300, loss 0.0000, target value 0.0000,, memory cap 66946\n",
      "Training: episode 97, length 962, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 67909\n",
      "Training: episode 98, length 793, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 68703\n",
      "Training: episode 99, length 456, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 69160\n",
      "Training: episode 100, length 693, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 69854\n",
      "Training: episode 101, length 546, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 70401\n",
      "Training: episode 102, length 732, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 71134\n",
      "Training: episode 103, length 774, reward 6, raw_reward 120, loss 0.0000, target value 0.0000,, memory cap 71909\n",
      "Training: episode 104, length 513, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 72423\n",
      "Training: episode 105, length 737, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 73161\n",
      "Training: episode 106, length 919, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 74081\n",
      "Training: episode 107, length 383, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 74465\n",
      "Training: episode 108, length 557, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 75023\n",
      "Training: episode 109, length 646, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 75670\n",
      "Training: episode 110, length 530, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 76201\n",
      "Training: episode 111, length 610, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 76812\n",
      "Training: episode 112, length 493, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 77306\n",
      "Training: episode 113, length 1069, reward 9, raw_reward 180, loss 0.0000, target value 0.0000,, memory cap 78376\n",
      "Training: episode 114, length 848, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 79225\n",
      "Training: episode 115, length 689, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 79915\n",
      "Training: episode 116, length 599, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 80515\n",
      "Training: episode 117, length 418, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 80934\n",
      "Training: episode 118, length 698, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 81633\n",
      "Training: episode 119, length 672, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 82306\n",
      "Training: episode 120, length 853, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 83160\n",
      "Training: episode 121, length 1242, reward 14, raw_reward 280, loss 0.0000, target value 0.0000,, memory cap 84403\n",
      "Training: episode 122, length 715, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 85119\n",
      "Training: episode 123, length 784, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 85904\n",
      "Training: episode 124, length 1004, reward 6, raw_reward 120, loss 0.0000, target value 0.0000,, memory cap 86909\n",
      "Training: episode 125, length 552, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 87462\n",
      "Training: episode 126, length 818, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 88281\n",
      "Training: episode 127, length 1512, reward 17, raw_reward 360, loss 0.0000, target value 0.0000,, memory cap 89794\n",
      "Training: episode 128, length 533, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 90328\n",
      "Training: episode 129, length 846, reward 6, raw_reward 120, loss 0.0000, target value 0.0000,, memory cap 91175\n",
      "Training: episode 130, length 550, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 91726\n",
      "Training: episode 131, length 598, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 92325\n",
      "Training: episode 132, length 622, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 92948\n",
      "Training: episode 133, length 615, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 93564\n",
      "Training: episode 134, length 1382, reward 17, raw_reward 340, loss 0.0000, target value 0.0000,, memory cap 94947\n",
      "Training: episode 135, length 1062, reward 11, raw_reward 220, loss 0.0000, target value 0.0000,, memory cap 96010\n",
      "Training: episode 136, length 494, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 96505\n",
      "Training: episode 137, length 1031, reward 5, raw_reward 120, loss 0.0000, target value 0.0000,, memory cap 97537\n",
      "Training: episode 138, length 570, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 98108\n",
      "Training: episode 139, length 756, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 98865\n",
      "Training: episode 140, length 922, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 99788\n",
      "Training: episode 141, length 675, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 100464\n",
      "Training: episode 142, length 610, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 101075\n",
      "Training: episode 143, length 534, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 101610\n",
      "Training: episode 144, length 555, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 102166\n",
      "Training: episode 145, length 652, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 102819\n",
      "Training: episode 146, length 843, reward 7, raw_reward 140, loss 0.0000, target value 0.0000,, memory cap 103663\n",
      "Training: episode 147, length 549, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 104213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: episode 148, length 733, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 104947\n",
      "Training: episode 149, length 562, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 105510\n",
      "Training: episode 150, length 911, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 106422\n",
      "Training: episode 151, length 555, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 106978\n",
      "Training: episode 152, length 597, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 107576\n",
      "Training: episode 153, length 494, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 108071\n",
      "Training: episode 154, length 714, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 108786\n",
      "Training: episode 155, length 669, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 109456\n",
      "Training: episode 156, length 570, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 110027\n",
      "Training: episode 157, length 461, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 110489\n",
      "Training: episode 158, length 547, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 111037\n",
      "Training: episode 159, length 763, reward 8, raw_reward 160, loss 0.0000, target value 0.0000,, memory cap 111801\n",
      "Training: episode 160, length 462, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 112264\n",
      "Training: episode 161, length 528, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 112793\n",
      "Training: episode 162, length 832, reward 6, raw_reward 120, loss 0.0000, target value 0.0000,, memory cap 113626\n",
      "Training: episode 163, length 783, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 114410\n",
      "Training: episode 164, length 836, reward 6, raw_reward 120, loss 0.0000, target value 0.0000,, memory cap 115247\n",
      "Training: episode 165, length 642, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 115890\n",
      "Training: episode 166, length 494, reward 1, raw_reward 20, loss 0.0000, target value 0.0000,, memory cap 116385\n",
      "Training: episode 167, length 554, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 116940\n",
      "Training: episode 168, length 596, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 117537\n",
      "Training: episode 169, length 1417, reward 15, raw_reward 300, loss 0.0000, target value 0.0000,, memory cap 118955\n",
      "Training: episode 170, length 496, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 119452\n",
      "Training: episode 171, length 758, reward 7, raw_reward 140, loss 0.0000, target value 0.0000,, memory cap 120211\n",
      "Training: episode 172, length 541, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 120753\n",
      "Training: episode 173, length 747, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 121501\n",
      "Training: episode 174, length 1051, reward 6, raw_reward 120, loss 0.0000, target value 0.0000,, memory cap 122553\n",
      "Training: episode 175, length 452, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 123006\n",
      "Training: episode 176, length 799, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 123806\n",
      "Training: episode 177, length 640, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 124447\n",
      "Training: episode 178, length 778, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 125226\n",
      "Training: episode 179, length 552, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 125779\n",
      "Training: episode 180, length 586, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 126366\n",
      "Training: episode 181, length 816, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 127183\n",
      "Training: episode 182, length 815, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 127999\n",
      "Training: episode 183, length 677, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 128677\n",
      "Training: episode 184, length 701, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 129379\n",
      "Training: episode 185, length 489, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 129869\n",
      "Training: episode 186, length 740, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 130610\n",
      "Training: episode 187, length 695, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 131306\n",
      "Training: episode 188, length 572, reward 0, raw_reward 0, loss 0.0000, target value 0.0000,, memory cap 131879\n",
      "Training: episode 189, length 485, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 132365\n",
      "Training: episode 190, length 662, reward 5, raw_reward 100, loss 0.0000, target value 0.0000,, memory cap 133028\n",
      "Training: episode 191, length 990, reward 9, raw_reward 180, loss 0.0000, target value 0.0000,, memory cap 134019\n",
      "Training: episode 192, length 1276, reward 12, raw_reward 240, loss 0.0000, target value 0.0000,, memory cap 135296\n",
      "Training: episode 193, length 746, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 136043\n",
      "Training: episode 194, length 781, reward 4, raw_reward 80, loss 0.0000, target value 0.0000,, memory cap 136825\n",
      "Training: episode 195, length 1076, reward 11, raw_reward 220, loss 0.0000, target value 0.0000,, memory cap 137902\n",
      "Training: episode 196, length 1423, reward 13, raw_reward 260, loss 0.0000, target value 0.0000,, memory cap 139326\n",
      "Training: episode 197, length 970, reward 9, raw_reward 180, loss 0.0000, target value 0.0000,, memory cap 140297\n",
      "Training: episode 198, length 932, reward 7, raw_reward 140, loss 0.0000, target value 0.0000,, memory cap 141230\n",
      "Training: episode 199, length 648, reward 2, raw_reward 40, loss 0.0000, target value 0.0000,, memory cap 141879\n",
      "Training: episode 200, length 726, reward 3, raw_reward 60, loss 0.0000, target value 0.0000,, memory cap 142606\n",
      "Training: episode 201, length 777, reward 3, raw_reward 60, loss 0.7364, target value 0.0003,, memory cap 143384\n",
      "Training: episode 202, length 615, reward 4, raw_reward 80, loss 0.5996, target value 0.0070,, memory cap 144000\n",
      "Training: episode 203, length 906, reward 3, raw_reward 60, loss 0.9260, target value 0.0314,, memory cap 144907\n",
      "Training: episode 204, length 645, reward 4, raw_reward 80, loss 0.6638, target value 0.0456,, memory cap 145553\n",
      "Training: episode 205, length 690, reward 4, raw_reward 80, loss 0.5218, target value 0.0561,, memory cap 146244\n",
      "Training: episode 206, length 1240, reward 12, raw_reward 240, loss 0.9192, target value 0.0644,, memory cap 147485\n",
      "Training: episode 207, length 511, reward 1, raw_reward 20, loss 0.3531, target value 0.0728,, memory cap 147997\n",
      "Training: episode 208, length 575, reward 2, raw_reward 40, loss 0.4777, target value 0.0858,, memory cap 148573\n",
      "Training: episode 209, length 442, reward 0, raw_reward 0, loss 0.4010, target value 0.0960,, memory cap 149016\n",
      "Training: episode 210, length 518, reward 1, raw_reward 20, loss 0.4804, target value 0.1035,, memory cap 149535\n",
      "Training: episode 211, length 583, reward 3, raw_reward 60, loss 0.5580, target value 0.1064,, memory cap 150119\n",
      "Training: episode 212, length 479, reward 3, raw_reward 60, loss 0.4432, target value 0.1096,, memory cap 150599\n",
      "Training: episode 213, length 644, reward 3, raw_reward 60, loss 0.4284, target value 0.1185,, memory cap 151244\n",
      "Training: episode 214, length 685, reward 1, raw_reward 20, loss 0.6340, target value 0.1245,, memory cap 151930\n",
      "Training: episode 215, length 627, reward 4, raw_reward 80, loss 0.6050, target value 0.1255,, memory cap 152558\n",
      "Training: episode 216, length 458, reward 0, raw_reward 0, loss 0.3485, target value 0.1250,, memory cap 153017\n",
      "Training: episode 217, length 717, reward 7, raw_reward 140, loss 0.7104, target value 0.1415,, memory cap 153735\n",
      "Training: episode 218, length 920, reward 7, raw_reward 140, loss 0.8713, target value 0.1522,, memory cap 154656\n",
      "Training: episode 219, length 532, reward 0, raw_reward 0, loss 0.5376, target value 0.1536,, memory cap 155189\n",
      "Training: episode 220, length 475, reward 1, raw_reward 20, loss 0.4272, target value 0.1557,, memory cap 155665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: episode 221, length 636, reward 5, raw_reward 100, loss 0.5252, target value 0.1661,, memory cap 156302\n",
      "Training: episode 222, length 838, reward 6, raw_reward 140, loss 0.9317, target value 0.1690,, memory cap 157141\n",
      "Training: episode 223, length 568, reward 2, raw_reward 40, loss 0.4744, target value 0.1715,, memory cap 157710\n",
      "Training: episode 224, length 905, reward 6, raw_reward 120, loss 0.9264, target value 0.1746,, memory cap 158616\n",
      "Training: episode 225, length 677, reward 4, raw_reward 80, loss 0.5884, target value 0.1881,, memory cap 159294\n",
      "Training: episode 226, length 643, reward 3, raw_reward 60, loss 0.5530, target value 0.1918,, memory cap 159938\n",
      "Training: episode 227, length 727, reward 5, raw_reward 100, loss 0.7204, target value 0.2111,, memory cap 160666\n",
      "Training: episode 228, length 772, reward 5, raw_reward 100, loss 0.6165, target value 0.2057,, memory cap 161439\n",
      "Training: episode 229, length 639, reward 4, raw_reward 80, loss 0.6483, target value 0.2057,, memory cap 162079\n",
      "Training: episode 230, length 936, reward 10, raw_reward 200, loss 0.8384, target value 0.2123,, memory cap 163016\n",
      "Training: episode 231, length 529, reward 2, raw_reward 40, loss 0.4628, target value 0.2189,, memory cap 163546\n",
      "Training: episode 232, length 791, reward 2, raw_reward 40, loss 0.9019, target value 0.2196,, memory cap 164338\n",
      "Training: episode 233, length 591, reward 3, raw_reward 60, loss 0.6512, target value 0.2259,, memory cap 164930\n",
      "Training: episode 234, length 980, reward 10, raw_reward 200, loss 0.9795, target value 0.2230,, memory cap 165911\n",
      "Training: episode 235, length 698, reward 5, raw_reward 100, loss 0.6384, target value 0.2245,, memory cap 166610\n",
      "Training: episode 236, length 1576, reward 14, raw_reward 280, loss 1.2820, target value 0.2282,, memory cap 168187\n",
      "Training: episode 237, length 997, reward 11, raw_reward 220, loss 0.8272, target value 0.2431,, memory cap 169185\n",
      "Training: episode 238, length 894, reward 6, raw_reward 120, loss 0.8534, target value 0.2318,, memory cap 170080\n",
      "Training: episode 239, length 1261, reward 7, raw_reward 140, loss 1.4417, target value 0.2364,, memory cap 171342\n",
      "Training: episode 240, length 922, reward 5, raw_reward 100, loss 0.9914, target value 0.2276,, memory cap 172265\n",
      "Training: episode 241, length 516, reward 2, raw_reward 40, loss 0.5028, target value 0.2287,, memory cap 172782\n",
      "Training: episode 242, length 800, reward 3, raw_reward 60, loss 0.6977, target value 0.2330,, memory cap 173583\n",
      "Training: episode 243, length 836, reward 9, raw_reward 180, loss 1.0302, target value 0.2396,, memory cap 174420\n",
      "Training: episode 244, length 905, reward 6, raw_reward 120, loss 0.7744, target value 0.2259,, memory cap 175326\n",
      "Training: episode 245, length 514, reward 1, raw_reward 20, loss 0.5354, target value 0.2235,, memory cap 175841\n",
      "Training: episode 246, length 693, reward 3, raw_reward 60, loss 0.7892, target value 0.2231,, memory cap 176535\n",
      "Training: episode 247, length 917, reward 2, raw_reward 40, loss 0.8116, target value 0.2215,, memory cap 177453\n",
      "Training: episode 248, length 622, reward 2, raw_reward 40, loss 0.5349, target value 0.2185,, memory cap 178076\n",
      "Training: episode 249, length 642, reward 6, raw_reward 140, loss 0.4603, target value 0.2150,, memory cap 178719\n",
      "Training: episode 250, length 1037, reward 2, raw_reward 40, loss 0.8980, target value 0.2176,, memory cap 179757\n",
      "Training: episode 251, length 1022, reward 9, raw_reward 180, loss 1.0719, target value 0.2213,, memory cap 180780\n",
      "Training: episode 252, length 1174, reward 11, raw_reward 220, loss 0.9527, target value 0.2316,, memory cap 181955\n",
      "Training: episode 253, length 1272, reward 12, raw_reward 240, loss 1.2814, target value 0.2376,, memory cap 183228\n",
      "Training: episode 254, length 1226, reward 13, raw_reward 260, loss 1.0793, target value 0.2404,, memory cap 184455\n",
      "Training: episode 255, length 1068, reward 10, raw_reward 200, loss 1.0101, target value 0.2372,, memory cap 185524\n",
      "Training: episode 256, length 403, reward 0, raw_reward 0, loss 0.3800, target value 0.2387,, memory cap 185928\n",
      "Training: episode 257, length 824, reward 5, raw_reward 100, loss 0.6382, target value 0.2331,, memory cap 186753\n",
      "Training: episode 258, length 1023, reward 6, raw_reward 120, loss 0.8999, target value 0.2343,, memory cap 187777\n",
      "Training: episode 259, length 974, reward 10, raw_reward 200, loss 1.0661, target value 0.2495,, memory cap 188752\n",
      "Training: episode 260, length 781, reward 7, raw_reward 140, loss 0.7577, target value 0.2569,, memory cap 189534\n",
      "Training: episode 261, length 1255, reward 11, raw_reward 220, loss 1.1701, target value 0.2689,, memory cap 190790\n",
      "Training: episode 262, length 1401, reward 14, raw_reward 280, loss 1.4348, target value 0.2787,, memory cap 192192\n",
      "Training: episode 263, length 605, reward 3, raw_reward 60, loss 0.5718, target value 0.2886,, memory cap 192798\n",
      "Training: episode 264, length 992, reward 4, raw_reward 80, loss 0.9026, target value 0.2935,, memory cap 193791\n",
      "Training: episode 265, length 989, reward 9, raw_reward 180, loss 0.6998, target value 0.2909,, memory cap 194781\n",
      "Training: episode 266, length 1324, reward 10, raw_reward 200, loss 1.0903, target value 0.2919,, memory cap 196106\n",
      "Training: episode 267, length 729, reward 5, raw_reward 100, loss 0.8524, target value 0.2943,, memory cap 196836\n",
      "Training: episode 268, length 1317, reward 9, raw_reward 180, loss 1.3588, target value 0.2932,, memory cap 198154\n",
      "Training: episode 269, length 894, reward 8, raw_reward 160, loss 0.9296, target value 0.2868,, memory cap 199049\n",
      "Training: episode 270, length 1425, reward 15, raw_reward 300, loss 1.5463, target value 0.2899,, memory cap 200475\n",
      "Training: episode 271, length 1337, reward 16, raw_reward 320, loss 1.1912, target value 0.2947,, memory cap 201813\n",
      "Training: episode 272, length 672, reward 3, raw_reward 60, loss 0.6004, target value 0.2890,, memory cap 202486\n",
      "Training: episode 273, length 1456, reward 9, raw_reward 180, loss 1.5258, target value 0.3045,, memory cap 203943\n",
      "Training: episode 274, length 1010, reward 9, raw_reward 180, loss 0.8831, target value 0.2955,, memory cap 204954\n",
      "Training: episode 275, length 1589, reward 14, raw_reward 280, loss 1.6879, target value 0.2945,, memory cap 206544\n",
      "Training: episode 276, length 950, reward 7, raw_reward 160, loss 0.8244, target value 0.3067,, memory cap 207495\n",
      "Training: episode 277, length 1607, reward 9, raw_reward 180, loss 1.4349, target value 0.2946,, memory cap 209103\n",
      "Training: episode 278, length 1469, reward 13, raw_reward 260, loss 1.3449, target value 0.2795,, memory cap 210573\n",
      "Training: episode 279, length 839, reward 3, raw_reward 60, loss 0.8083, target value 0.2821,, memory cap 211413\n",
      "Training: episode 280, length 1033, reward 5, raw_reward 120, loss 0.8859, target value 0.2811,, memory cap 212447\n",
      "Training: episode 281, length 851, reward 7, raw_reward 140, loss 0.9018, target value 0.2931,, memory cap 213299\n",
      "Training: episode 282, length 952, reward 6, raw_reward 120, loss 0.9799, target value 0.2883,, memory cap 214252\n",
      "Training: episode 283, length 941, reward 5, raw_reward 100, loss 1.0730, target value 0.2835,, memory cap 215194\n",
      "Training: episode 284, length 1007, reward 6, raw_reward 120, loss 0.8109, target value 0.2744,, memory cap 216202\n",
      "Training: episode 285, length 754, reward 7, raw_reward 140, loss 0.9373, target value 0.2752,, memory cap 216957\n",
      "Training: episode 286, length 637, reward 4, raw_reward 100, loss 0.6148, target value 0.2749,, memory cap 217595\n",
      "Training: episode 287, length 958, reward 11, raw_reward 220, loss 1.0100, target value 0.2790,, memory cap 218554\n",
      "Training: episode 288, length 477, reward 3, raw_reward 60, loss 0.6141, target value 0.2867,, memory cap 219032\n",
      "Training: episode 289, length 847, reward 5, raw_reward 100, loss 0.8183, target value 0.2836,, memory cap 219880\n",
      "Training: episode 290, length 1218, reward 10, raw_reward 200, loss 1.1697, target value 0.2910,, memory cap 221099\n",
      "Training: episode 291, length 1225, reward 8, raw_reward 160, loss 1.4497, target value 0.2966,, memory cap 222325\n",
      "Training: episode 292, length 1102, reward 4, raw_reward 80, loss 1.2007, target value 0.2947,, memory cap 223428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: episode 293, length 875, reward 4, raw_reward 80, loss 0.8365, target value 0.2902,, memory cap 224304\n",
      "Training: episode 294, length 651, reward 5, raw_reward 100, loss 0.8488, target value 0.2996,, memory cap 224956\n",
      "Training: episode 295, length 894, reward 2, raw_reward 40, loss 1.1387, target value 0.3200,, memory cap 225851\n",
      "Training: episode 296, length 1179, reward 10, raw_reward 200, loss 1.0723, target value 0.3234,, memory cap 227031\n",
      "Training: episode 297, length 860, reward 7, raw_reward 140, loss 0.6620, target value 0.3209,, memory cap 227892\n",
      "Training: episode 298, length 1142, reward 11, raw_reward 220, loss 1.0606, target value 0.3118,, memory cap 229035\n",
      "Training: episode 299, length 796, reward 5, raw_reward 100, loss 0.8720, target value 0.3186,, memory cap 229832\n",
      "Training: episode 300, length 1042, reward 3, raw_reward 80, loss 0.8670, target value 0.3169,, memory cap 230875\n",
      "Training: episode 301, length 678, reward 4, raw_reward 80, loss 0.6370, target value 0.3105,, memory cap 231554\n",
      "Training: episode 302, length 2257, reward 10, raw_reward 200, loss 2.2423, target value 0.3046,, memory cap 233812\n",
      "Training: episode 303, length 959, reward 7, raw_reward 140, loss 0.7263, target value 0.2972,, memory cap 234772\n",
      "Training: episode 304, length 1032, reward 5, raw_reward 100, loss 0.9055, target value 0.2902,, memory cap 235805\n",
      "Training: episode 305, length 1352, reward 8, raw_reward 160, loss 1.3817, target value 0.2851,, memory cap 237158\n",
      "Training: episode 306, length 1278, reward 11, raw_reward 220, loss 1.4042, target value 0.2931,, memory cap 238437\n",
      "Training: episode 307, length 984, reward 8, raw_reward 180, loss 1.1069, target value 0.3045,, memory cap 239422\n",
      "Training: episode 308, length 1290, reward 6, raw_reward 120, loss 1.1811, target value 0.3001,, memory cap 240713\n",
      "Training: episode 309, length 818, reward 3, raw_reward 60, loss 0.6484, target value 0.3037,, memory cap 241532\n",
      "Training: episode 310, length 1300, reward 5, raw_reward 100, loss 1.3150, target value 0.2947,, memory cap 242833\n",
      "Training: episode 311, length 1186, reward 11, raw_reward 220, loss 1.1985, target value 0.2970,, memory cap 244020\n",
      "Training: episode 312, length 854, reward 3, raw_reward 60, loss 0.7759, target value 0.3014,, memory cap 244875\n",
      "Training: episode 313, length 1234, reward 6, raw_reward 140, loss 1.0572, target value 0.3002,, memory cap 246110\n",
      "Training: episode 314, length 1083, reward 3, raw_reward 60, loss 1.3740, target value 0.3019,, memory cap 247194\n",
      "Training: episode 315, length 1300, reward 3, raw_reward 60, loss 1.4211, target value 0.2993,, memory cap 248495\n",
      "Training: episode 316, length 740, reward 4, raw_reward 80, loss 0.7865, target value 0.3051,, memory cap 249236\n",
      "Training: episode 317, length 1009, reward 4, raw_reward 80, loss 1.0068, target value 0.3052,, memory cap 250246\n",
      "Training: episode 318, length 955, reward 13, raw_reward 280, loss 1.3072, target value 0.3009,, memory cap 251202\n",
      "Training: episode 319, length 1211, reward 4, raw_reward 80, loss 1.3259, target value 0.2935,, memory cap 252414\n",
      "Training: episode 320, length 1207, reward 10, raw_reward 200, loss 1.1802, target value 0.2972,, memory cap 253622\n",
      "Training: episode 321, length 945, reward 4, raw_reward 80, loss 0.8333, target value 0.3072,, memory cap 254568\n",
      "Training: episode 322, length 779, reward 7, raw_reward 140, loss 0.6004, target value 0.3139,, memory cap 255348\n",
      "Training: episode 323, length 1658, reward 2, raw_reward 40, loss 1.6723, target value 0.3005,, memory cap 257007\n",
      "Training: episode 324, length 1030, reward 4, raw_reward 80, loss 0.9053, target value 0.2989,, memory cap 258038\n",
      "Training: episode 325, length 926, reward 3, raw_reward 60, loss 0.7827, target value 0.2998,, memory cap 258965\n",
      "Training: episode 326, length 1435, reward 5, raw_reward 100, loss 1.4575, target value 0.3037,, memory cap 260401\n",
      "Training: episode 327, length 653, reward 6, raw_reward 120, loss 0.8282, target value 0.2978,, memory cap 261055\n",
      "Training: episode 328, length 646, reward 6, raw_reward 120, loss 0.7624, target value 0.3034,, memory cap 261702\n",
      "Training: episode 329, length 1159, reward 5, raw_reward 100, loss 1.1784, target value 0.2913,, memory cap 262862\n",
      "Training: episode 330, length 1215, reward 2, raw_reward 60, loss 1.3417, target value 0.2912,, memory cap 264078\n",
      "Training: episode 331, length 1487, reward 4, raw_reward 100, loss 1.4887, target value 0.2839,, memory cap 265566\n",
      "Training: episode 332, length 1306, reward 8, raw_reward 160, loss 1.2337, target value 0.2810,, memory cap 266873\n",
      "Training: episode 333, length 1792, reward 6, raw_reward 120, loss 1.6609, target value 0.2820,, memory cap 268666\n",
      "Training: episode 334, length 867, reward 5, raw_reward 100, loss 0.9755, target value 0.2726,, memory cap 269534\n",
      "Training: episode 335, length 1420, reward 6, raw_reward 120, loss 1.4996, target value 0.2690,, memory cap 270955\n",
      "Training: episode 336, length 811, reward 3, raw_reward 60, loss 0.8358, target value 0.2627,, memory cap 271767\n",
      "Training: episode 337, length 1140, reward 6, raw_reward 120, loss 1.0387, target value 0.2671,, memory cap 272908\n",
      "Training: episode 338, length 727, reward 4, raw_reward 80, loss 0.7731, target value 0.2742,, memory cap 273636\n",
      "Training: episode 339, length 1019, reward 7, raw_reward 140, loss 0.9454, target value 0.2782,, memory cap 274656\n",
      "Training: episode 340, length 999, reward 6, raw_reward 120, loss 1.0527, target value 0.2818,, memory cap 275656\n",
      "Training: episode 341, length 1194, reward 11, raw_reward 220, loss 1.4558, target value 0.2879,, memory cap 276851\n",
      "Training: episode 342, length 1581, reward 8, raw_reward 160, loss 1.5425, target value 0.3019,, memory cap 278433\n",
      "Training: episode 343, length 1929, reward 4, raw_reward 80, loss 1.9195, target value 0.3156,, memory cap 280363\n",
      "Training: episode 344, length 1222, reward 9, raw_reward 180, loss 1.1529, target value 0.3264,, memory cap 281586\n",
      "Training: episode 345, length 869, reward 5, raw_reward 100, loss 0.9327, target value 0.3237,, memory cap 282456\n",
      "Training: episode 346, length 1301, reward 3, raw_reward 60, loss 1.4074, target value 0.3341,, memory cap 283758\n",
      "Training: episode 347, length 1171, reward 5, raw_reward 100, loss 1.2458, target value 0.3344,, memory cap 284930\n",
      "Training: episode 348, length 1264, reward 8, raw_reward 160, loss 1.2482, target value 0.3338,, memory cap 286195\n",
      "Training: episode 349, length 662, reward 2, raw_reward 60, loss 0.6323, target value 0.3386,, memory cap 286858\n",
      "Training: episode 350, length 882, reward 5, raw_reward 100, loss 0.8665, target value 0.3453,, memory cap 287741\n",
      "Training: episode 351, length 1079, reward 4, raw_reward 80, loss 0.9175, target value 0.3436,, memory cap 288821\n",
      "Training: episode 352, length 772, reward 4, raw_reward 80, loss 0.9482, target value 0.3423,, memory cap 289594\n",
      "Training: episode 353, length 880, reward 6, raw_reward 140, loss 0.8654, target value 0.3382,, memory cap 290475\n",
      "Training: episode 354, length 1305, reward 4, raw_reward 80, loss 1.1092, target value 0.3269,, memory cap 291781\n",
      "Training: episode 355, length 931, reward 7, raw_reward 160, loss 1.1462, target value 0.3275,, memory cap 292713\n",
      "Training: episode 356, length 1174, reward 4, raw_reward 80, loss 0.9630, target value 0.3352,, memory cap 293888\n"
     ]
    }
   ],
   "source": [
    "agent.train(atari_env,episodes,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-27T04:05:12.947Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ac9c66ef-4bd0-422a-af51-7dc81aac8a57"
    }
   },
   "source": [
    " Chasewind hyper parameters for seaquest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-06T19:15:14.192482Z",
     "start_time": "2018-01-06T19:15:00.875277Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "cb9f9ba4-e7b0-4b99-841b-4c4bca2c07f8"
    }
   },
   "outputs": [],
   "source": [
    "for mini in batch:\n",
    "    #print(\"reset\")\n",
    "    agent.model.reset_states() # we do this because the RNN is stateful\n",
    "    agent.target_model.reset_states()\n",
    "\n",
    "    for s,a,r,s_,done in mini:\n",
    "        s = s[None][None]\n",
    "        s_ = s_[None][None]\n",
    "        #print(\"predict\")\n",
    "        target = agent.model.predict(s)[0]\n",
    "        #print(\"target {}\".format(target))\n",
    "        if done:\n",
    "            target[a] = r\n",
    "        else:\n",
    "            a_ = agent.model.predict(s_)[0]\n",
    "            t = agent.target_model.predict(s)[0]\n",
    "            target[a] = r + agent.gamma * t[np.argmax(a_)]\n",
    "        #print(\"fit\")\n",
    "        '''\n",
    "        print(\"target + tderr: {}\".format(target))\n",
    "        print(\"action: {}\".format(a))\n",
    "        print(\"argmax next_action: {}\".format(np.argmax(a_)))\n",
    "        print(\"reward: {}\".format(r))\n",
    "        print(\"next_action {}\".format(a_))\n",
    "        print(\"behavioural {}\".format(t))\n",
    "        '''\n",
    "        res = self.model.fit(s, target, epochs=1,batch_size=1, verbose=0,shuffle=False)\n",
    "        self.save_scalar(self.loss_count,\"Loss\",res.history['loss'][0])\n",
    "        self.loss_count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c6932bdb-43d2-4043-a468-026656b62d44"
    }
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T22:25:46.116684Z",
     "start_time": "2018-01-21T22:25:46.113701Z"
    },
    "collapsed": true
   },
   "source": [
    "## Create competitor (chasewind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:48:40.728363Z",
     "start_time": "2018-01-29T03:48:40.680432Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from chasewind.deeprl_prj.dqn_keras import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:48:41.483597Z",
     "start_time": "2018-01-29T03:48:41.351082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--bidir'], dest='bidir', nargs=0, const=True, default=False, type=None, choices=None, help='enable two layer bidirectional lstm', metavar=None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Run DQN on Atari Breakout')\n",
    "parser.add_argument('--env', default='Seaquest-v0', help='Atari env name')\n",
    "parser.add_argument('-o', '--output', default='./log/', help='Directory to save data to')\n",
    "parser.add_argument('--seed', default=0, type=int, help='Random seed')\n",
    "parser.add_argument('--gamma', default=0.99, type=float, help='Discount factor')\n",
    "parser.add_argument('--batch_size', default=32, type=int, help='Minibatch size')\n",
    "parser.add_argument('--learning_rate', default=0.0001, type=float, help='Learning rate')\n",
    "parser.add_argument('--initial_epsilon', default=1.0, type=float, help='Initial exploration probability in epsilon-greedy')\n",
    "parser.add_argument('--final_epsilon', default=0.05, type=float, help='Final exploration probability in epsilon-greedy')\n",
    "parser.add_argument('--exploration_steps', default=1000000, type=int, help='Number of steps over which the initial value of epsilon is linearly annealed to its final value')\n",
    "parser.add_argument('--num_samples', default=100000000, type=int, help='Number of training samples from the environment in training')\n",
    "parser.add_argument('--num_frames', default=10, type=int, help='Number of frames to feed to Q-Network')\n",
    "parser.add_argument('--frame_width', default=84, type=int, help='Resized frame width')\n",
    "parser.add_argument('--frame_height', default=84, type=int, help='Resized frame height')\n",
    "parser.add_argument('--replay_memory_size', default=1000000, type=int, help='Number of replay memory the agent uses for training')\n",
    "parser.add_argument('--target_update_freq', default=10000, type=int, help='The frequency with which the target network is updated')\n",
    "parser.add_argument('--train_freq', default=4, type=int, help='The frequency of actions wrt Q-network update')\n",
    "parser.add_argument('--save_freq', default=50000, type=int, help='The frequency with which the network is saved')\n",
    "parser.add_argument('--eval_freq', default=50000, type=int, help='The frequency with which the policy is evlauted')    \n",
    "parser.add_argument('--num_burn_in', default=50000, type=int, help='Number of steps to populate the replay memory before training starts')\n",
    "parser.add_argument('--load_network', default=False, action='store_true', help='Load trained mode')\n",
    "parser.add_argument('--load_network_path', default='', help='the path to the trained mode file')\n",
    "parser.add_argument('--net_mode', default='dqn', help='choose the mode of net, can be linear, dqn, duel')\n",
    "parser.add_argument('--max_episode_length', default = 10000, type=int, help = 'max length of each episode')\n",
    "parser.add_argument('--num_episodes_at_test', default = 20, type=int, help='Number of episodes the agent plays at test')\n",
    "parser.add_argument('--ddqn', default=True, dest='ddqn', action='store_true', help='enable ddqn')\n",
    "parser.add_argument('--train', default=True, dest='train', action='store_true', help='Train mode')\n",
    "parser.add_argument('--test', dest='train', action='store_false', help='Test mode')\n",
    "parser.add_argument('--no_experience', default=False, action='store_true', help='do not use experience replay')\n",
    "parser.add_argument('--no_target', default=False, action='store_true', help='do not use target fixing')\n",
    "parser.add_argument('--no_monitor', default=False, action='store_true', help='do not record video')\n",
    "parser.add_argument('--task_name', default='', help='task name')\n",
    "parser.add_argument('--recurrent', default=False, dest='recurrent', action='store_true', help='enable recurrent DQN')\n",
    "parser.add_argument('--a_t', default=False, dest='a_t', action='store_true', help='enable temporal/spatial attention')\n",
    "parser.add_argument('--global_a_t', default=False, dest='global_a_t', action='store_true', help='enable global temporal attention')\n",
    "parser.add_argument('--selector', default=False, dest='selector', action='store_true', help='enable selector for spatial attention')\n",
    "parser.add_argument('--bidir', default=False, dest='bidir', action='store_true', help='enable two layer bidirectional lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:53:28.171209Z",
     "start_time": "2018-01-29T03:53:28.165312Z"
    }
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--recurrent','--task_name=test','--net_mode=duel','--replay_memory_size=5000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:53:35.415428Z",
     "start_time": "2018-01-29T03:53:30.705884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Defining Recurrent Modules...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input (InputLayer)               (None, 84, 84, 10)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)              (None, 84, 84, 10, 1) 0           input[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "permute_5 (Permute)              (None, 10, 84, 84, 1) 0           reshape_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_91 (TimeDistrib (None, 10, 20, 20, 32 2080        permute_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_92 (TimeDistrib (None, 10, 9, 9, 64)  32832       time_distributed_91[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_93 (TimeDistrib (None, 10, 7, 7, 64)  36928       time_distributed_92[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_94 (TimeDistrib (None, 10, 3136)      0           time_distributed_93[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_95 (TimeDistrib (None, 10, 512)       1606144     time_distributed_94[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                   (None, 512)           2099200     time_distributed_95[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "action_fc (Dense)                (None, 512)           262656      lstm_19[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "action (Dense)                   (None, 18)            9234        action_fc[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "value_fc (Dense)                 (None, 512)           262656      lstm_19[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "value (Dense)                    (None, 1)             513         value_fc[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "action_mean (Lambda)             (None, 1)             0           action[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "output (Lambda)                  (None, 18)            0           action[0][0]                     \n",
      "                                                                   value[0][0]                      \n",
      "                                                                   action_mean[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 4,312,243\n",
      "Trainable params: 4,312,243\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      ">>>> Defining Recurrent Modules...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input (InputLayer)               (None, 84, 84, 10)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)              (None, 84, 84, 10, 1) 0           input[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "permute_6 (Permute)              (None, 10, 84, 84, 1) 0           reshape_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_96 (TimeDistrib (None, 10, 20, 20, 32 2080        permute_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_97 (TimeDistrib (None, 10, 9, 9, 64)  32832       time_distributed_96[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_98 (TimeDistrib (None, 10, 7, 7, 64)  36928       time_distributed_97[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_99 (TimeDistrib (None, 10, 3136)      0           time_distributed_98[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_100 (TimeDistri (None, 10, 512)       1606144     time_distributed_99[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                   (None, 512)           2099200     time_distributed_100[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "action_fc (Dense)                (None, 512)           262656      lstm_20[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "action (Dense)                   (None, 18)            9234        action_fc[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "value_fc (Dense)                 (None, 512)           262656      lstm_20[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "value (Dense)                    (None, 1)             513         value_fc[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "action_mean (Lambda)             (None, 1)             0           action[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "output (Lambda)                  (None, 18)            0           action[0][0]                     \n",
      "                                                                   value[0][0]                      \n",
      "                                                                   action_mean[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 4,312,243\n",
      "Trainable params: 4,312,243\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      ">>>> Net mode: duel, Using double dqn: True\n",
      ">>>> Target fixing: True, Experience replay: True\n"
     ]
    }
   ],
   "source": [
    "chase = DQNAgent(args,atari_env.action_space.n);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create my agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:53:38.461922Z",
     "start_time": "2018-01-29T03:53:35.547636Z"
    },
    "nbpresent": {
     "id": "4b457670-d5a1-4bc0-96ef-0a93c15d6fdb"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cfg = AgentConfig()\n",
    "cfg.stateCnt = atari_env.observation_space.shape\n",
    "cfg.actionCnt = atari_env.action_space.n\n",
    "cfg.mem_size = 5000\n",
    "cfg.epsilon_policy = LinearDecayGreedyEpsilonPolicy(1.0,0.05,100)\n",
    "cfg.gamma = 0.99\n",
    "cfg.num_frames = 5\n",
    "cfg.learning_rate = 0.0001\n",
    "cfg.train_start = 20\n",
    "cfg.train_freq = 4\n",
    "cfg.target_update_freq = 100 \n",
    "cfg.batch_size = 32\n",
    "cfg.preprocessor = AtariPreprocessor((84,84,1))\n",
    "cfg.log_path = None\n",
    "cfg.name = \"Tester\"\n",
    "r3 = Agent(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:31:02.831928Z",
     "start_time": "2018-01-23T03:31:02.762131Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "6ab35772-9289-48fb-83ff-77539c6c495e"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "prebatch_s,batch_a,batch_r,prebatch_s_,batch_done = agent.longmem.sample(batch_size)# a batch of episode of parameter length\n",
    "\n",
    "batch_s = agent.preprocessor.process_batch(prebatch_s)\n",
    "batch_s_ = agent.preprocessor.process_batch(prebatch_s_)\n",
    "a_ = agent.model.predict(batch_s_)\n",
    "a_idx = np.argmax(a_,axis=1)\n",
    "behaviour_q = agent.target_model.predict(batch_s_)\n",
    "target = agent.model.predict(batch_s)\n",
    "\n",
    "target[range(batch_size),batch_a.astype('int')] = batch_r + agent.gamma * (behaviour_q[range(batch_size),a_idx]) \n",
    "#loss = agent.model.train_on_batch(batch_s,target)\n",
    "#self.save_scalar(self.loss_count,\"/Loss\",loss)\n",
    "#self.loss_count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:16:10.037547Z",
     "start_time": "2018-01-23T03:16:10.030345Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "552de618-bfb1-4865-b226-b8724db50ffa"
    }
   },
   "outputs": [],
   "source": [
    "batch_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:50:22.847713Z",
     "start_time": "2018-01-29T03:50:22.813430Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "3271e9cd-9ac8-4464-8314-899e6ae068bb"
    }
   },
   "outputs": [],
   "source": [
    "def mod_dry(env,episodes):\n",
    "    for i in range(episodes):\n",
    "        state = env.reset()\n",
    "        for t in itertools.count():\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            mem_state1 = chase.atari_processor.process_state_for_memory(state)\n",
    "            mem_state2 = r3.preprocessor.process_state_for_memory(state)\n",
    "\n",
    "            net_state1 = chase.history_processor.process_state_for_network(chase.atari_processor.process_state_for_network(state))\n",
    "            net_state2 = r3.shortmem.add(r3.preprocessor.process_for_network(mem_state2))\n",
    "\n",
    "            nxt_state,reward,done,info = env.step(action)\n",
    "\n",
    "            nxt_mem_state1 = chase.atari_processor.process_state_for_memory(nxt_state)\n",
    "            nxt_mem_state2 = r3.preprocessor.process_state_for_memory(nxt_state)\n",
    "\n",
    "            env.render()\n",
    "\n",
    "            chase.memory.append(mem_state1,action,reward,done)\n",
    "            r3.remember(mem_state2,action,reward,done)\n",
    "\n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:54:19.591053Z",
     "start_time": "2018-01-29T03:53:39.347855Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_dry(atari_env,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T04:13:12.594661Z",
     "start_time": "2018-01-29T04:13:12.498028Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "samples = chase.memory.sample(batch_size)\n",
    "samples = chase.atari_processor.process_batch(samples)\n",
    "\n",
    "states = np.stack([x.state for x in samples])\n",
    "actions = np.asarray([x.action for x in samples])\n",
    "action_mask = np.zeros((batch_size, chase.num_actions))\n",
    "action_mask[range(batch_size), actions] = 1.0\n",
    "\n",
    "next_states = np.stack([x.next_state for x in samples])\n",
    "mask = np.asarray([1 - int(x.is_terminal) for x in samples])\n",
    "rewards = np.asarray([x.reward for x in samples])\n",
    "\n",
    "next_qa_value = chase.target_network.predict_on_batch(next_states)\n",
    "\n",
    "qa_value = chase.q_network.predict_on_batch(next_states)\n",
    "max_actions = np.argmax(qa_value, axis = 1)\n",
    "next_qa_value = next_qa_value[range(batch_size), max_actions]\n",
    "c_target = rewards + chase.gamma * mask * next_qa_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T04:12:06.444188Z",
     "start_time": "2018-01-29T04:12:01.742380Z"
    },
    "nbpresent": {
     "id": "ba63aa82-b239-4375-8250-d9f1c2a5ea93"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60979033"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chase.final_model.train_on_batch([states,action_mask],c_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T04:15:28.645191Z",
     "start_time": "2018-01-29T04:15:28.494175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.60905778, 0.62124907718738542)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chase.update_policy(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T04:15:32.000204Z",
     "start_time": "2018-01-29T04:15:31.917137Z"
    },
    "nbpresent": {
     "id": "1c567421-37a4-4984-a8b7-9cbb3a19d23b"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.8816087e-05, 0.0017492085)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3.replay(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:28:13.904304Z",
     "start_time": "2018-01-23T03:28:13.765997Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "33fe9928-22d4-42c6-aa86-b337962bd466"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:10:39.136540Z",
     "start_time": "2018-01-29T03:10:39.108498Z"
    },
    "code_folding": [],
    "collapsed": true,
    "nbpresent": {
     "id": "921291b9-8562-4a32-9a55-ed5eb935233f"
    }
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/soply/f3eec2e79c165e39c9d540e916142ae1\n",
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(np.squeeze(image,-1) )\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:31:54.103494Z",
     "start_time": "2018-01-23T03:31:54.095818Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "c94a820f-0093-45c1-881f-85d1843d3d82"
    }
   },
   "outputs": [],
   "source": [
    "prebatch_s,batch_a,batch_r,prebatch_s_,batch_done = agent.longmem.sample(batch_size)# a batch of episode of parameter length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:34:48.819264Z",
     "start_time": "2018-01-23T03:34:48.814485Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "6c465604-205c-453b-91c3-f5c0b33527df"
    }
   },
   "outputs": [],
   "source": [
    "batch_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:36:03.129384Z",
     "start_time": "2018-01-23T03:36:03.124937Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "c04a6c58-0771-400f-9a6a-edfab2448ca4"
    }
   },
   "outputs": [],
   "source": [
    "shortmem_s = agent.shortmem.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:36:12.716016Z",
     "start_time": "2018-01-23T03:36:11.777880Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "2043a96b-477e-4697-a236-cdcae2a9cce9"
    }
   },
   "outputs": [],
   "source": [
    "show_images(shortmem_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T03:40:33.428923Z",
     "start_time": "2018-01-23T03:40:12.757314Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "092c8db1-ba8e-4a3f-82eb-79086faf7042"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:24:28.643416Z",
     "start_time": "2018-01-29T03:24:28.595472Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "7aaa1daa-e47b-4f0a-94b7-4d373d93e09c"
    }
   },
   "outputs": [],
   "source": [
    "def dry(env,agent,episodes):\n",
    "    # used to populate test data into agent, useful for debugging\n",
    "    for i in range(episodes):\n",
    "        # play loop\n",
    "        pre_s = env.reset()\n",
    "        mem_s = agent.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "        net_s = agent.preprocessor.process_for_network(mem_s) # normalized\n",
    "        done = False\n",
    "        #agent.shortmem.forget # forget short term memory for recurrent network \n",
    "        agent.shortmem.add(net_s)\n",
    "        for t in itertools.count():\n",
    "            hist_s = agent.shortmem.get()\n",
    "            # take action using net_s and receive a\n",
    "            a = agent.act(hist_s)\n",
    "            env.render()\n",
    "\n",
    "            pre_s_, r, done, info = env.step(a)\n",
    "\n",
    "            mem_s_ = agent.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "            net_s_ = agent.preprocessor.process_for_network(mem_s) # normalized\n",
    "            #test.append(mem_s_)\n",
    "            agent.shortmem.add(net_s_)\n",
    "            agent.remember(mem_s,a,r,done)\n",
    "\n",
    "            #R[-1] += r\n",
    "            if done:\n",
    "\n",
    "                #agent.save_scalar(e,\"/reward per episode\",R[-1])\n",
    "                #print(\"episode: {}/{}, score: {}, e: {}\".format(e, episodes, t, agent.epsilon))\n",
    "                #exp.metric(\"reward\",R[-1])\n",
    "                #update(e,R[-1],handle1,rplot)\n",
    "                #R = [0.0]\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:37:02.769800Z",
     "start_time": "2018-01-29T03:36:36.452993Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dry(atari_env,t_agent,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T04:04:52.773449Z",
     "start_time": "2018-01-29T04:04:52.768632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7428"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3.longmem.current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T04:06:41.489796Z",
     "start_time": "2018-01-29T04:06:39.405504Z"
    },
    "nbpresent": {
     "id": "95aa46a5-86f8-47fa-b3c6-640ffa3e37e2"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAADTkAAAFLCAYAAACnEWn8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8cb04269e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prebatch_s,batch_a,batch_r,prebatch_s_,batch_done = t_agent.longmem.sample(32)# a batch of episode of parameter length\n",
    "idx = 30\n",
    "show_images(np.expand_dims(chase.memory.screens[3300:3310],-1))\n",
    "#show_images(prebatch_s_[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:17:59.778345Z",
     "start_time": "2018-01-29T03:17:59.533319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:51:22.259405Z",
     "start_time": "2018-01-29T03:51:22.242700Z"
    },
    "nbpresent": {
     "id": "750dfe9a-5744-4432-9aa5-449bd0f66991"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def gif(filename, array, fps=10, scale=1.0):\n",
    "    \"\"\"Creates a gif given a stack of images using moviepy\n",
    "    Notes\n",
    "    -----\n",
    "    works with current Github version of moviepy (not the pip version)\n",
    "    https://github.com/Zulko/moviepy/commit/d4c9c37bc88261d8ed8b5d9b7c317d13b2cdf62e\n",
    "    Usage\n",
    "    -----\n",
    "    >>> X = randn(100, 64, 64)\n",
    "    >>> gif('test.gif', X)\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : string\n",
    "        The filename of the gif to write to\n",
    "    array : array_like\n",
    "        A numpy array that contains a sequence of images\n",
    "    fps : int\n",
    "        frames per second (default: 10)\n",
    "    scale : float\n",
    "        how much to rescale each image by (default: 1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    # ensure that the file has the .gif extension\n",
    "    #fname, _ = os.path.splitext(filename)\n",
    "    #filename = fname + '.gif'\n",
    "\n",
    "    # copy into the color dimension if the images are black and white\n",
    "    #if array.ndim == 3:\n",
    "    #    array = array[..., np.newaxis] * np.ones(3)\n",
    "\n",
    "    # make the moviepy clip\n",
    "    clip = mpy.ImageSequenceClip(list(array), fps=fps)\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T04:00:50.750632Z",
     "start_time": "2018-01-29T04:00:49.209898Z"
    },
    "nbpresent": {
     "id": "74fef843-0ba3-4f17-896a-509fc594e8e9"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_clip = gif(None,chase.memory.screens)\n",
    "r_clip = gif(None,np.squeeze(r3.longmem.mem_s,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T04:02:41.069609Z",
     "start_time": "2018-01-29T04:02:26.421561Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5001 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 37/5001 [00:00<00:13, 365.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏         | 74/5001 [00:00<00:13, 366.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 107/5001 [00:00<00:13, 352.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 140/5001 [00:00<00:14, 342.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 174/5001 [00:00<00:14, 338.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 213/5001 [00:00<00:13, 351.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 249/5001 [00:00<00:13, 353.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 287/5001 [00:00<00:13, 360.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 325/5001 [00:00<00:12, 364.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 361/5001 [00:01<00:12, 358.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 396/5001 [00:01<00:13, 352.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▊         | 432/5001 [00:01<00:12, 353.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 470/5001 [00:01<00:12, 360.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 506/5001 [00:01<00:12, 359.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 542/5001 [00:01<00:12, 359.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 578/5001 [00:01<00:12, 345.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 613/5001 [00:01<00:12, 344.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 650/5001 [00:01<00:12, 347.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▎        | 687/5001 [00:01<00:12, 354.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 726/5001 [00:02<00:11, 362.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 763/5001 [00:02<00:11, 357.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▌        | 800/5001 [00:02<00:11, 358.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 837/5001 [00:02<00:11, 361.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 874/5001 [00:02<00:11, 357.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 910/5001 [00:02<00:11, 354.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 947/5001 [00:02<00:11, 357.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|█▉        | 984/5001 [00:02<00:11, 359.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 1022/5001 [00:02<00:10, 363.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 1059/5001 [00:02<00:11, 352.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 1097/5001 [00:03<00:10, 358.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 1133/5001 [00:03<00:10, 354.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 1170/5001 [00:03<00:10, 358.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 1206/5001 [00:03<00:10, 356.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▍       | 1243/5001 [00:03<00:10, 359.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▌       | 1280/5001 [00:03<00:10, 362.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▋       | 1317/5001 [00:03<00:10, 359.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 1354/5001 [00:03<00:10, 349.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 1392/5001 [00:03<00:10, 355.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 1428/5001 [00:04<00:10, 348.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▉       | 1463/5001 [00:04<00:10, 345.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|██▉       | 1500/5001 [00:04<00:09, 351.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███       | 1537/5001 [00:04<00:09, 356.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███▏      | 1573/5001 [00:04<00:09, 356.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 1611/5001 [00:04<00:09, 359.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 1647/5001 [00:04<00:09, 355.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▎      | 1683/5001 [00:04<00:09, 356.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 1720/5001 [00:04<00:09, 359.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 1757/5001 [00:04<00:08, 360.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▌      | 1794/5001 [00:05<00:08, 361.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 1831/5001 [00:05<00:08, 361.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 1868/5001 [00:05<00:08, 351.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 1904/5001 [00:05<00:08, 350.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▉      | 1940/5001 [00:05<00:08, 351.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███▉      | 1977/5001 [00:05<00:08, 355.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 2013/5001 [00:05<00:08, 353.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 2049/5001 [00:05<00:08, 355.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 2086/5001 [00:05<00:08, 358.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 2122/5001 [00:05<00:08, 357.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 2158/5001 [00:06<00:08, 343.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 2194/5001 [00:06<00:08, 347.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▍     | 2229/5001 [00:06<00:08, 342.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▌     | 2264/5001 [00:06<00:08, 338.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▌     | 2300/5001 [00:06<00:07, 344.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 2335/5001 [00:06<00:07, 334.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 2370/5001 [00:06<00:07, 338.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 2404/5001 [00:06<00:07, 336.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 2438/5001 [00:06<00:07, 330.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 2475/5001 [00:07<00:07, 340.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 2510/5001 [00:07<00:07, 342.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████     | 2547/5001 [00:07<00:07, 349.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 2583/5001 [00:07<00:06, 347.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 2618/5001 [00:07<00:06, 346.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 2654/5001 [00:07<00:06, 347.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 2689/5001 [00:07<00:07, 320.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 2722/5001 [00:07<00:07, 322.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 2757/5001 [00:07<00:06, 328.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 2791/5001 [00:07<00:06, 324.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▋    | 2824/5001 [00:08<00:06, 313.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 2856/5001 [00:08<00:06, 307.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 2887/5001 [00:08<00:06, 306.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 2918/5001 [00:08<00:06, 301.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 2950/5001 [00:08<00:06, 305.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████▉    | 2983/5001 [00:08<00:06, 308.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 3014/5001 [00:08<00:06, 305.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████    | 3047/5001 [00:08<00:06, 311.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 3083/5001 [00:08<00:05, 322.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 3119/5001 [00:09<00:05, 331.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████▎   | 3153/5001 [00:09<00:05, 331.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▍   | 3189/5001 [00:09<00:05, 338.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▍   | 3224/5001 [00:09<00:05, 341.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 3260/5001 [00:09<00:05, 344.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▌   | 3295/5001 [00:09<00:05, 336.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 3329/5001 [00:09<00:04, 336.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 3364/5001 [00:09<00:04, 339.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 3399/5001 [00:09<00:04, 322.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▊   | 3433/5001 [00:09<00:04, 326.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 3468/5001 [00:10<00:04, 331.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 3502/5001 [00:10<00:04, 327.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████   | 3538/5001 [00:10<00:04, 334.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████▏  | 3573/5001 [00:10<00:04, 335.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 3607/5001 [00:10<00:04, 333.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 3642/5001 [00:10<00:04, 335.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▎  | 3677/5001 [00:10<00:03, 337.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 3712/5001 [00:10<00:03, 339.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▍  | 3747/5001 [00:10<00:03, 341.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 3782/5001 [00:10<00:03, 343.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▋  | 3818/5001 [00:11<00:03, 345.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 3853/5001 [00:11<00:03, 343.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 3888/5001 [00:11<00:03, 341.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 3923/5001 [00:11<00:03, 336.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 3957/5001 [00:11<00:03, 337.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|███████▉  | 3993/5001 [00:11<00:02, 342.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████  | 4028/5001 [00:11<00:02, 338.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████▏ | 4064/5001 [00:11<00:02, 341.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 4099/5001 [00:11<00:02, 340.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 4135/5001 [00:12<00:02, 343.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 4170/5001 [00:12<00:02, 340.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▍ | 4205/5001 [00:12<00:02, 331.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▍ | 4239/5001 [00:12<00:02, 333.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▌ | 4273/5001 [00:12<00:02, 331.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 4307/5001 [00:12<00:02, 333.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 4341/5001 [00:12<00:01, 331.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 4376/5001 [00:12<00:01, 334.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 4411/5001 [00:12<00:01, 338.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▉ | 4446/5001 [00:12<00:01, 341.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|████████▉ | 4481/5001 [00:13<00:01, 337.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 4516/5001 [00:13<00:01, 337.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 4550/5001 [00:13<00:01, 336.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 4584/5001 [00:13<00:01, 330.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 4618/5001 [00:13<00:01, 333.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 4652/5001 [00:13<00:01, 332.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▎| 4686/5001 [00:13<00:00, 327.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|█████████▍| 4720/5001 [00:13<00:00, 329.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 4753/5001 [00:13<00:00, 327.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▌| 4788/5001 [00:13<00:00, 331.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▋| 4822/5001 [00:14<00:00, 331.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 4856/5001 [00:14<00:00, 328.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 4890/5001 [00:14<00:00, 330.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████▊| 4924/5001 [00:14<00:00, 325.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████▉| 4959/5001 [00:14<00:00, 332.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████▉| 4994/5001 [00:14<00:00, 335.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 5001/5001 [00:14<00:00, 342.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "c_clip.write_gif(\"test.gif\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T03:39:51.736464Z",
     "start_time": "2018-01-29T03:39:07.832321Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time, position, color :  6.333, (38, 57), [31 31 31]\n",
      "time, position, color :  6.400, (38, 57), [31 31 31]\n",
      "time, position, color :  6.400, (38, 57), [31 31 31]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-1ed87c710b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtestclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.gif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtestclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-222>\u001b[0m in \u001b[0;36mpreview\u001b[0;34m(clip, fps, audio, audio_fps, audio_buffersize, audio_nbytes)\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/casper/lib/python3.5/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-221>\u001b[0m in \u001b[0;36mpreview\u001b[0;34m(clip, fps, audio, audio_fps, audio_buffersize, audio_nbytes)\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/casper/lib/python3.5/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/casper/lib/python3.5/site-packages/moviepy/video/io/preview.py\u001b[0m in \u001b[0;36mpreview\u001b[0;34m(clip, fps, audio, audio_fps, audio_buffersize, audio_nbytes)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mimdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/casper/lib/python3.5/site-packages/moviepy/video/io/preview.py\u001b[0m in \u001b[0;36mimdisplay\u001b[0;34m(imarray, screen)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "testclip = mpy.VideoFileClip(\"test.gif\")\n",
    "testclip.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:27:22.970013Z",
     "start_time": "2018-01-23T17:27:22.952579Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "aa94547f-cd66-4549-8cf3-0630c7ac2e1b"
    }
   },
   "outputs": [],
   "source": [
    "pre_s = atari_env.reset()\n",
    "mem_s = t_agent.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "net_s = t_agent.preprocessor.process_for_network(mem_s) # normalized\n",
    "done = False\n",
    "t_agent.shortmem.forget() # forget short term memory for recurrent network \n",
    "t_agent.shortmem.add(net_s)\n",
    "atari_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:32:07.600283Z",
     "start_time": "2018-01-23T17:32:07.588372Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "daafc7e0-ce98-483d-a33a-da2b3d9feeef"
    }
   },
   "outputs": [],
   "source": [
    "hist_s = t_agent.shortmem.get()\n",
    "a = t_agent.act(hist_s)\n",
    "atari_env.render()\n",
    "\n",
    "pre_s_, r, done, info = atari_env.step(a)\n",
    "\n",
    "mem_s_ = t_agent.preprocessor.process_state_for_memory(pre_s) #scaled and grayscaled\n",
    "net_s_ = t_agent.preprocessor.process_for_network(mem_s) # normalized\n",
    "t_agent.shortmem.add(net_s_)\n",
    "t_agent.remember(mem_s,a,r,done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:10:10.483097Z",
     "start_time": "2018-01-23T04:10:10.478315Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "d76678cf-4bab-42a0-b941-5066e9964473"
    }
   },
   "outputs": [],
   "source": [
    "t_agent.act(hist_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:31:45.672733Z",
     "start_time": "2018-01-23T17:31:45.558118Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "37cd9700-e5a9-4598-a4e7-d2aa995fd2e6"
    }
   },
   "outputs": [],
   "source": [
    "show_images(net_s[None]*255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:09:09.964710Z",
     "start_time": "2018-01-23T04:09:09.958517Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "090dd334-d198-4c4e-8c73-d6490eaca5d3"
    }
   },
   "outputs": [],
   "source": [
    "hist_s = t_agent.shortmem.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:09:11.505633Z",
     "start_time": "2018-01-23T04:09:10.475291Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "6414e12e-1912-4dcd-aaaf-ea360bc14b36"
    }
   },
   "outputs": [],
   "source": [
    "show_images(hist_s*255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:01:25.960783Z",
     "start_time": "2018-01-23T04:01:25.951488Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "64245d72-35bb-4228-b095-0d617d8e882f"
    }
   },
   "outputs": [],
   "source": [
    "t_agent.shortmem.mem_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T04:02:48.070304Z",
     "start_time": "2018-01-23T04:02:48.063977Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "feaede64-4bf5-4220-b318-08fb54880dbf"
    }
   },
   "outputs": [],
   "source": [
    "t_agent.shortmem.current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:36:29.683751Z",
     "start_time": "2018-01-23T17:36:29.681106Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:36:50.444753Z",
     "start_time": "2018-01-23T17:36:50.340747Z"
    },
    "collapsed": true,
    "nbpresent": {
     "id": "82adcbcc-878c-48e4-b6bb-734d1d7e3bef"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Run DQN on Atari Breakout')\n",
    "parser.add_argument('--env', default='Seaquest-v0', help='Atari env name')\n",
    "parser.add_argument('-o', '--output', default='./log/', help='Directory to save data to')\n",
    "parser.add_argument('--seed', default=0, type=int, help='Random seed')\n",
    "parser.add_argument('--gamma', default=0.99, type=float, help='Discount factor')\n",
    "parser.add_argument('--batch_size', default=32, type=int, help='Minibatch size')\n",
    "parser.add_argument('--learning_rate', default=0.0001, type=float, help='Learning rate')\n",
    "parser.add_argument('--initial_epsilon', default=1.0, type=float, help='Initial exploration probability in epsilon-greedy')\n",
    "parser.add_argument('--final_epsilon', default=0.05, type=float, help='Final exploration probability in epsilon-greedy')\n",
    "parser.add_argument('--exploration_steps', default=1000000, type=int, help='Number of steps over which the initial value of epsilon is linearly annealed to its final value')\n",
    "parser.add_argument('--num_samples', default=100000000, type=int, help='Number of training samples from the environment in training')\n",
    "parser.add_argument('--num_frames', default=4, type=int, help='Number of frames to feed to Q-Network')\n",
    "parser.add_argument('--frame_width', default=84, type=int, help='Resized frame width')\n",
    "parser.add_argument('--frame_height', default=84, type=int, help='Resized frame height')\n",
    "parser.add_argument('--replay_memory_size', default=1000000, type=int, help='Number of replay memory the agent uses for training')\n",
    "parser.add_argument('--target_update_freq', default=10000, type=int, help='The frequency with which the target network is updated')\n",
    "parser.add_argument('--train_freq', default=4, type=int, help='The frequency of actions wrt Q-network update')\n",
    "parser.add_argument('--save_freq', default=50000, type=int, help='The frequency with which the network is saved')\n",
    "parser.add_argument('--eval_freq', default=50000, type=int, help='The frequency with which the policy is evlauted')    \n",
    "parser.add_argument('--num_burn_in', default=50000, type=int, help='Number of steps to populate the replay memory before training starts')\n",
    "parser.add_argument('--load_network', default=False, action='store_true', help='Load trained mode')\n",
    "parser.add_argument('--load_network_path', default='', help='the path to the trained mode file')\n",
    "parser.add_argument('--net_mode', default='dqn', help='choose the mode of net, can be linear, dqn, duel')\n",
    "parser.add_argument('--max_episode_length', default = 10000, type=int, help = 'max length of each episode')\n",
    "parser.add_argument('--num_episodes_at_test', default = 20, type=int, help='Number of episodes the agent plays at test')\n",
    "parser.add_argument('--ddqn', default=False, dest='ddqn', action='store_true', help='enable ddqn')\n",
    "parser.add_argument('--train', default=True, dest='train', action='store_true', help='Train mode')\n",
    "parser.add_argument('--test', dest='train', action='store_false', help='Test mode')\n",
    "parser.add_argument('--no_experience', default=False, action='store_true', help='do not use experience replay')\n",
    "parser.add_argument('--no_target', default=False, action='store_true', help='do not use target fixing')\n",
    "parser.add_argument('--no_monitor', default=False, action='store_true', help='do not record video')\n",
    "parser.add_argument('--task_name', default='', help='task name')\n",
    "parser.add_argument('--recurrent', default=False, dest='recurrent', action='store_true', help='enable recurrent DQN')\n",
    "parser.add_argument('--a_t', default=False, dest='a_t', action='store_true', help='enable temporal/spatial attention')\n",
    "parser.add_argument('--global_a_t', default=False, dest='global_a_t', action='store_true', help='enable global temporal attention')\n",
    "parser.add_argument('--selector', default=False, dest='selector', action='store_true', help='enable selector for spatial attention')\n",
    "parser.add_argument('--bidir', default=False, dest='bidir', action='store_true', help='enable two layer bidirectional lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T17:39:14.975165Z",
     "start_time": "2018-01-23T17:39:14.968164Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--gamma 0.9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:casper]",
   "language": "python",
   "name": "conda-env-casper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
