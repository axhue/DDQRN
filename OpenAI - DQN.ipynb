{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-01T01:55:33.666323Z",
     "start_time": "2018-01-01T01:55:22.701852Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import baselines.common.tf_util as U\n",
    "\n",
    "from baselines import logger\n",
    "from baselines import deepq\n",
    "from baselines.deepq.replay_buffer import PrioritizedReplayBuffer,ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "from baselines.deepq.models import cnn_to_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-01T01:55:33.744798Z",
     "start_time": "2018-01-01T01:55:33.693413Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc as scimisc\n",
    "import gym_minecraft\n",
    "from MinecraftGym import MinecraftWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-01T01:55:33.795136Z",
     "start_time": "2018-01-01T01:55:33.759212Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential,model_from_json\n",
    "from keras.layers import Dense, Activation,GRU,Input,LSTM,Conv2D,Flatten\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-01T01:55:34.371861Z",
     "start_time": "2018-01-01T01:55:33.811884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"d1edc769-c348-4b68-b30c-cf39ed8dba9b\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(`.${CLASS_NAME.split(' ')[0]}`);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"d1edc769-c348-4b68-b30c-cf39ed8dba9b\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"d1edc769-c348-4b68-b30c-cf39ed8dba9b\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'd1edc769-c348-4b68-b30c-cf39ed8dba9b' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"d1edc769-c348-4b68-b30c-cf39ed8dba9b\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"d1edc769-c348-4b68-b30c-cf39ed8dba9b\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"d1edc769-c348-4b68-b30c-cf39ed8dba9b\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'd1edc769-c348-4b68-b30c-cf39ed8dba9b' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"d1edc769-c348-4b68-b30c-cf39ed8dba9b\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, push_notebook, show\n",
    "from bokeh.driving import linear\n",
    "from bokeh.layouts import row,gridplot\n",
    "from IPython.display import clear_output,display\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-01T01:55:34.391574Z",
     "start_time": "2018-01-01T01:55:34.385093Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.client import push_session\n",
    "from bokeh.driving import cosine\n",
    "from bokeh.plotting import figure, curdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-01T01:55:34.442837Z",
     "start_time": "2018-01-01T01:55:34.408763Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperdash import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T05:33:43.439121Z",
     "start_time": "2017-12-30T05:33:43.298713Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_env = gym.make(\"MinecraftCliffWalking1-v0\")\n",
    "pre_env.init(videoResolution=[400,400],allowContinuousMovement=[\"move\", \"turn\", \"strafe\"],observeGrid=[20,-1,20,20,-1,20],observeDistance=[4,45,12])\n",
    "env = MinecraftWrapper(pre_env,1/10,(41,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T06:32:00.743610Z",
     "start_time": "2017-12-28T06:32:00.731472Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proc2_reward(info):\n",
    "    if info is None:\n",
    "        return 0\n",
    "    elif 'observation' not in info.keys():\n",
    "        return 0\n",
    "    elif info['observation'] is None:\n",
    "        return 0\n",
    "    elif 'distanceFromdist' in info['observation'].keys():\n",
    "        return 10/(0.001 + info['observation']['distanceFromdist'])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T07:41:41.024575Z",
     "start_time": "2017-12-31T07:41:41.005637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(inpt, num_actions, scope, reuse=False):\n",
    "    \"\"\"This model takes as input an observation and returns values of all actions.\"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32,(8,8),input_shape=env.observation_space.shape,activation='relu'))\n",
    "        model.add(Conv2D(64,(4,4),activation='relu'))\n",
    "        model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256,activation='relu')\n",
    "        model.add(Dense(output_dim=num_actions,activation='softmax'))\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        a = Conv2D(32,(8,8),input_shape=env.observation_space.shape,activation='relu')(inpt)\n",
    "        b = Conv2D(64,(3,3),activation='relu')(a)\n",
    "        d = Flatten()(b)\n",
    "        c = Dense(output_dim=num_actions,activation='softmax')(d)\n",
    "        '''\n",
    "        return model(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T06:32:01.703389Z",
     "start_time": "2017-12-28T06:32:01.683271Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def old_model(inpt, num_actions, scope, reuse=False):\n",
    "    \"\"\"This model takes as input an observation and returns values of all actions.\"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = inpt\n",
    "\n",
    "        out =  tf.layers.conv2d(\n",
    "                inputs=out,\n",
    "                filters=32,\n",
    "                kernel_size=[8, 8],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "        out =  tf.layers.conv2d(\n",
    "                inputs=out,\n",
    "                filters=64,\n",
    "                kernel_size=[4, 4],\n",
    "                padding=\"same\",\n",
    "                activation=tf.nn.relu)\n",
    "        out2 = tf.contrib.layers.flatten(out)\n",
    "        out2 = tf.layers.dense(inputs=out2, units=64)\n",
    "        out2 = tf.layers.dense(inputs=out2, units=num_actions)\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-01T01:55:58.641464Z",
     "start_time": "2018-01-01T01:55:58.636087Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lib_model(inpt,num_action,scope,reuse = False):\n",
    "    with tf.variable_scope(scope,reuse = reuse):\n",
    "        return cnn_to_mlp([(32,8,4),(64,4,2),(64,3,1)],[256],True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-29T00:35:35.060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ notes: using MLP CNN 32,8x8,s4 - 64,4x4,s2 - 64,3x3,s1 }\n",
      "WARNING:tensorflow:From /home/ubuntu/baselines/baselines/deepq/build_graph.py:366: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda2/envs/casper/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ubuntu/miniconda2/envs/casper/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: 110.000000 |\n",
      "| reward: 150.000000 |\n",
      "| reward:  75.000000 |\n",
      "| reward: 185.000000 |\n",
      "| reward: 135.000000 |\n",
      "| reward: 120.000000 |\n",
      "| reward: 370.000000 |\n",
      "| reward:  55.000000 |\n",
      "| reward: 125.000000 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 43       |\n",
      "| episodes               | 10       |\n",
      "| mean episode reward    | 147      |\n",
      "| steps                  | 5769     |\n",
      "-------------------------------------\n",
      "| reward: 125.000000 |\n",
      "| reward: 175.000000 |\n",
      "| reward: 105.000000 |\n",
      "| reward:  85.000000 |\n",
      "| reward: 110.000000 |\n",
      "| reward: 225.000000 |\n"
     ]
    }
   ],
   "source": [
    "U.reset()\n",
    "exp  = Experiment(\"openAI DQN - Minecraft\")\n",
    "exp.param(\"notes\",\"using MLP CNN 32,8x8,s4 - 64,4x4,s2 - 64,3x3,s1\")\n",
    "#env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "with U.make_session(2) as sess:\n",
    "    # Create all the functions necessary to train the model\n",
    "    K.set_session(sess)\n",
    "    dqn_model = cnn_to_mlp(\n",
    "        convs=[(32, 8, 4), (64, 4, 2), (64, 3, 1)],\n",
    "        hiddens=[256],\n",
    "        dueling=True\n",
    "    )\n",
    "    act, train, update_target, debug = deepq.build_train(\n",
    "        make_obs_ph=lambda name: U.BatchInput(env.observation_space.shape, name=name),\n",
    "        q_func= model,\n",
    "        num_actions=env.action_space.n,\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    "    )\n",
    "    \n",
    "    # Create the replay buffer\n",
    "    replay_buffer = ReplayBuffer(1000000)\n",
    "    # Create the schedule for exploration starting from 1 (every action is random) down to\n",
    "    # 0.02 (98% of actions are selected according to values predicted by the model).\n",
    "    exploration = LinearSchedule(schedule_timesteps=1000, initial_p=1.0, final_p=0.02)\n",
    "\n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    U.initialize()\n",
    "    update_target()\n",
    "\n",
    "    episode_rewards = [0.0]\n",
    "    obs = env.reset()\n",
    "    for t in itertools.count():\n",
    "        # Take action and update exploration to the newest value\n",
    "        action = act(obs[None], update_eps=exploration.value(t))[0]\n",
    "        new_obs, rew, done, info = env.step(action)\n",
    "        #rew += proc2_reward(info)\n",
    "        # Store transition in the replay buffer.\n",
    "        replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "        obs = new_obs\n",
    "\n",
    "        episode_rewards[-1] += rew\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            exp.metric(\"reward\",episode_rewards[-1])\n",
    "            episode_rewards.append(0)\n",
    "\n",
    "        is_solved = t > 100 and np.mean(episode_rewards[-101:-1]) >= 200\n",
    "        if is_solved:\n",
    "            # Show off the result\n",
    "            #env.render()\n",
    "            #print(\"Solved\")\n",
    "            pass\n",
    "        else:\n",
    "            # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "            if t > 100:\n",
    "                obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(32)\n",
    "                train(obses_t, actions, rewards, obses_tp1, dones, np.ones_like(rewards))\n",
    "            # Update target network periodically.\n",
    "            if t % 1000 == 0:\n",
    "                update_target()\n",
    "\n",
    "        if done and len(episode_rewards) % 10 == 0:\n",
    "            logger.record_tabular(\"steps\", t)\n",
    "            logger.record_tabular(\"episodes\", len(episode_rewards))\n",
    "            logger.record_tabular(\"mean episode reward\", round(np.mean(episode_rewards[-101:-1]), 1))\n",
    "            logger.record_tabular(\"% time spent exploring\", int(100 * exploration.value(t)))\n",
    "            logger.dump_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T21:40:06.098091Z",
     "start_time": "2017-12-29T21:40:06.088394Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(rgb_array,scale = 1/12):\n",
    "    frame_shape = rgb_array.shape\n",
    "    \n",
    "    frame = np.array(rgb_array)\n",
    "    gray_frame = np.dot(frame[...,:3],[0.299,0.587,0.114]).reshape((frame_shape[0],frame_shape[1]))\n",
    "    smaller = scimisc.imresize(gray_frame,scale,mode='L').astype('float64')\n",
    "    smaller /= 255.0\n",
    "    smaller = np.expand_dims(smaller,2) # convert to a 3D array of shape (height,width,grayscale)\n",
    "    #smaller = np.reshape(smaller, [1, *(smaller.shape)])\n",
    "    return smaller.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-01T21:55:43.049410Z",
     "start_time": "2018-01-01T21:55:42.887547Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(env,exp,model,buffer_size,epsilon,learning_rate,preprocessor = None):\n",
    "    U.reset()\n",
    "    #exp.param(\"notes\",\"using MLP CNN 32,8x8,s4 - 64,4x4,s2 - 64,3x3,s1\")\n",
    "    \n",
    "\n",
    "    with U.make_session(2) as sess:\n",
    "        K.set_session(sess)\n",
    "        try:\n",
    "            # Create all the functions necessary to train the model\n",
    "            \n",
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput(env.observation_space.shape, name=name),\n",
    "                q_func= model,\n",
    "                num_actions=env.action_space.n,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate),\n",
    "            )\n",
    "    \n",
    "            # Create the replay buffer\n",
    "            replay_buffer = ReplayBuffer(buffer_size)\n",
    "            # Create the schedule for exploration starting from 1 (every action is random) down to\n",
    "            # 0.02 (98% of actions are selected according to values predicted by the model).\n",
    "            exploration = LinearSchedule(schedule_timesteps=1000, initial_p=1.0, final_p=0.02)\n",
    "\n",
    "            # Initialize the parameters and copy them to the target network.\n",
    "            U.initialize()\n",
    "            update_target()\n",
    "            \n",
    "            #\n",
    "\n",
    "            episode_rewards = [0.0]\n",
    "            obs = env.reset()\n",
    "            if preprocessor:\n",
    "                obs = preprocessor(obs)\n",
    "            for t in itertools.count():\n",
    "                # Take action and update exploration to the newest value\n",
    "                action = act(obs[None], update_eps=exploration.value(t))[0]\n",
    "                new_obs, rew, done, info = env.step(action)\n",
    "                if preprocessor:\n",
    "                    new_obs = preprocessor(new_obs)\n",
    "                #rew += proc2_reward(info)\n",
    "                # Store transition in the replay buffer.\n",
    "                replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "                obs = new_obs\n",
    "\n",
    "                episode_rewards[-1] += rew\n",
    "                if done:\n",
    "                    obs = env.reset()\n",
    "                    if preprocessor:\n",
    "                        obs = preprocessor(obs)\n",
    "                    if exp:\n",
    "                        exp.metric(\"reward\",episode_rewards[-1])\n",
    "                    episode_rewards.append(0)\n",
    "                    \n",
    "                # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "                if t > 100:\n",
    "                    obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(32)\n",
    "                    train(obses_t, actions, rewards, obses_tp1, dones, np.ones_like(rewards))\n",
    "                # Update target network periodically.\n",
    "                if t % 1000 == 0:\n",
    "                    update_target()\n",
    "\n",
    "                if done and len(episode_rewards) % 10 == 0:\n",
    "                    logger.record_tabular(\"steps\", t)\n",
    "                    logger.record_tabular(\"episodes\", len(episode_rewards))\n",
    "                    logger.record_tabular(\"mean episode reward\", round(np.mean(episode_rewards[-101:-1]), 1))\n",
    "                    logger.record_tabular(\"% time spent exploring\", int(100 * exploration.value(t)))\n",
    "                    logger.dump_tabular()\n",
    "        except:\n",
    "            if exp:\n",
    "                exp.end()\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-29T21:43:51.490Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "exp = Experiment(\"OpenAI - DQN - SpaceInvaders\")\n",
    "train(env,exp,(42,32,1),5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-01T01:58:55.141590Z",
     "start_time": "2018-01-01T01:58:52.223403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ learning rate: Adam Optimizer with lr = 0.001 }\n",
      "{ buffer_size: 1000000 }\n",
      "This run of OpenAI - DQN - SpaceInvaders ran for 0:00:01 and logs are available locally at: /home/ubuntu/.hyperdash/logs/openai-dqn-spaceinvaders/openai-dqn-spaceinvaders_2018-01-01t01-58-52-295523.log\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e4f776a05f77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"buffer_size\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a27ec82f0454>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, exp, model, buffer_size, epsilon, learning_rate, preprocessor)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mq_func\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mnum_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/baselines/baselines/deepq/build_graph.py\u001b[0m in \u001b[0;36mbuild_train\u001b[0;34m(make_obs_ph, q_func, num_actions, optimizer, grad_norm_clipping, gamma, double_q, scope, reuse, param_noise, param_noise_filter_func)\u001b[0m\n\u001b[1;32m    339\u001b[0m             param_noise_filter_func=param_noise_filter_func)\n\u001b[1;32m    340\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mact_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_obs_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/baselines/baselines/deepq/build_graph.py\u001b[0m in \u001b[0;36mbuild_act\u001b[0;34m(make_obs_ph, q_func, num_actions, scope, reuse)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations_ph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"q_func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mdeterministic_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "pre_env = gym.make(\"MinecraftCliffWalking1-v0\")\n",
    "pre_env.init(videoResolution=[400,400],allowContinuousMovement=[\"move\", \"turn\", \"strafe\"],observeGrid=[20,-1,20,20,-1,20],observeDistance=[4,45,12])\n",
    "env = MinecraftWrapper(pre_env,1/10,(41,41))\n",
    "\n",
    "exp = Experiment(\"OpenAI - DQN - SpaceInvaders\")\n",
    "learning_rate = 0.001\n",
    "epsilon = 1\n",
    "buffer_size = 1000000\n",
    "exp.param(\"learning rate\", \"Adam Optimizer with lr = {}\".format(learning_rate))\n",
    "exp.param(\"buffer_size\",buffer_size)\n",
    "\n",
    "train(env,exp,env.observation_space.shape,5000,buffer_size,epsilon,learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-01T22:30:57.178064Z",
     "start_time": "2018-01-01T22:01:09.529407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ memory capacity: 1000000 }\n",
      "WARNING:tensorflow:From /home/ubuntu/baselines/baselines/deepq/build_graph.py:366: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "| reward: -93.392687 |\n",
      "| reward: -96.212646 |\n",
      "| reward: -95.283120 |\n",
      "| reward: -93.379098 |\n",
      "| reward: -96.221016 |\n",
      "| reward: -95.259455 |\n",
      "| reward: -95.241061 |\n",
      "| reward: -96.228949 |\n",
      "| reward: -96.221624 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 94       |\n",
      "| episodes               | 10       |\n",
      "| mean episode reward    | -95.3    |\n",
      "| steps                  | 53       |\n",
      "-------------------------------------\n",
      "| reward: -91.468579 |\n",
      "| reward: -89.517490 |\n",
      "| reward: -91.450331 |\n",
      "| reward: -96.213430 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -93.393487 |\n",
      "| reward: -92.374048 |\n",
      "| reward: -95.273766 |\n",
      "| reward: -95.284587 |\n",
      "| reward: -91.489064 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 86       |\n",
      "| episodes               | 20       |\n",
      "| mean episode reward    | -94.3    |\n",
      "| steps                  | 133      |\n",
      "-------------------------------------\n",
      "| reward: -95.248027 |\n",
      "| reward: -94.295846 |\n",
      "| reward: -91.444484 |\n",
      "| reward: -98.106581 |\n",
      "| reward: -97.158187 |\n",
      "| reward: -96.210759 |\n",
      "| reward: -93.358925 |\n",
      "| reward: -95.259901 |\n",
      "| reward: -93.357762 |\n",
      "| reward: -93.363878 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 80       |\n",
      "| episodes               | 30       |\n",
      "| mean episode reward    | -94.4    |\n",
      "| steps                  | 198      |\n",
      "-------------------------------------\n",
      "| reward: -93.400307 |\n",
      "| reward: -95.239360 |\n",
      "| reward: -92.404647 |\n",
      "| reward: -97.158187 |\n",
      "| reward: -87.640886 |\n",
      "| reward: -94.323672 |\n",
      "| reward: -98.106581 |\n",
      "| reward: -94.296921 |\n",
      "| reward: -48.008602 |\n",
      "| reward: -94.297631 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 69       |\n",
      "| episodes               | 40       |\n",
      "| mean episode reward    | -93.2    |\n",
      "| steps                  | 310      |\n",
      "-------------------------------------\n",
      "| reward: -97.159643 |\n",
      "| reward: -88.621874 |\n",
      "| reward: -95.279819 |\n",
      "| reward: -94.311661 |\n",
      "| reward: -95.269253 |\n",
      "| reward: -95.247379 |\n",
      "| reward: -94.339077 |\n",
      "| reward: -70.745491 |\n",
      "| reward: -94.295347 |\n",
      "| reward: -93.396383 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 60       |\n",
      "| episodes               | 50       |\n",
      "| mean episode reward    | -92.9    |\n",
      "| steps                  | 404      |\n",
      "-------------------------------------\n",
      "| reward: -92.440695 |\n",
      "| reward: -95.288597 |\n",
      "| reward: -96.214437 |\n",
      "| reward: -96.215459 |\n",
      "| reward: -93.354053 |\n",
      "| reward: -96.212190 |\n",
      "| reward: -93.354632 |\n",
      "| reward: -94.336761 |\n",
      "| reward: -84.805656 |\n",
      "| reward: -92.446497 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 52       |\n",
      "| episodes               | 60       |\n",
      "| mean episode reward    | -93      |\n",
      "| steps                  | 483      |\n",
      "-------------------------------------\n",
      "| reward: -95.288166 |\n",
      "| reward: -93.348971 |\n",
      "| reward: -92.438427 |\n",
      "| reward: -94.300047 |\n",
      "| reward: -95.280028 |\n",
      "| reward: -95.271766 |\n",
      "| reward: -88.587764 |\n",
      "| reward: -85.757168 |\n",
      "| reward: -94.350890 |\n",
      "| reward: -90.506173 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 43       |\n",
      "| episodes               | 70       |\n",
      "| mean episode reward    | -92.9    |\n",
      "| steps                  | 572      |\n",
      "-------------------------------------\n",
      "| reward: -96.205312 |\n",
      "| reward: -95.242437 |\n",
      "| reward: -93.350643 |\n",
      "| reward: -96.195669 |\n",
      "| reward: -96.221624 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -87.669042 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 38       |\n",
      "| episodes               | 80       |\n",
      "| mean episode reward    | -93.2    |\n",
      "| steps                  | 632      |\n",
      "-------------------------------------\n",
      "| reward: -95.288046 |\n",
      "| reward: -75.717923 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -95.281758 |\n",
      "| reward: -90.549648 |\n",
      "| reward: -95.265032 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.213804 |\n",
      "| reward: -90.503106 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 29       |\n",
      "| episodes               | 90       |\n",
      "| mean episode reward    | -93.2    |\n",
      "| steps                  | 717      |\n",
      "-------------------------------------\n",
      "| reward: -90.494283 |\n",
      "| reward: -95.282201 |\n",
      "| reward: -90.547195 |\n",
      "| reward: -86.686618 |\n",
      "| reward: -96.212026 |\n",
      "| reward: -94.289728 |\n",
      "| reward: -95.252244 |\n",
      "| reward: -96.202753 |\n",
      "| reward: -90.481167 |\n",
      "| reward: -97.159643 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 21       |\n",
      "| episodes               | 100      |\n",
      "| mean episode reward    | -93.2    |\n",
      "| steps                  | 798      |\n",
      "-------------------------------------\n",
      "| reward: -94.337952 |\n",
      "| reward: -95.248051 |\n",
      "| reward: -96.189375 |\n",
      "| reward: -93.394749 |\n",
      "| reward: -93.388211 |\n",
      "| reward: -96.221624 |\n",
      "| reward: -95.242437 |\n",
      "| reward: -95.242437 |\n",
      "| reward: -95.276231 |\n",
      "| reward: -92.440013 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 15       |\n",
      "| episodes               | 110      |\n",
      "| mean episode reward    | -93.2    |\n",
      "| steps                  | 864      |\n",
      "-------------------------------------\n",
      "| reward: -93.388618 |\n",
      "| reward: -84.898993 |\n",
      "| reward: -95.280107 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -95.250105 |\n",
      "| reward: -92.405362 |\n",
      "| reward: -96.210515 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -88.645193 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 7        |\n",
      "| episodes               | 120      |\n",
      "| mean episode reward    | -93.2    |\n",
      "| steps                  | 941      |\n",
      "-------------------------------------\n",
      "| reward: -94.320498 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -97.159643 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -97.160033 |\n",
      "| reward: -82.028122 |\n",
      "| reward: -95.282201 |\n",
      "| reward: -95.290948 |\n",
      "| reward: -92.411133 |\n",
      "| reward: -82.976105 |\n",
      "| reward: -93.394595 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 130      |\n",
      "| mean episode reward    | -93      |\n",
      "| steps                  | 1029     |\n",
      "-------------------------------------\n",
      "| reward: -96.194989 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -87.698419 |\n",
      "| reward: -96.189375 |\n",
      "| reward: -96.202733 |\n",
      "| reward: -96.229138 |\n",
      "| reward: -96.221624 |\n",
      "| reward: -94.297903 |\n",
      "| reward: -93.393901 |\n",
      "| reward: -96.212089 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 140      |\n",
      "| mean episode reward    | -93.5    |\n",
      "| steps                  | 1093     |\n",
      "-------------------------------------\n",
      "| reward: -95.242437 |\n",
      "| reward: -94.335694 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -75.337390 |\n",
      "| reward: -95.248122 |\n",
      "| reward: -77.281815 |\n",
      "| reward: -96.210662 |\n",
      "| reward: -93.389761 |\n",
      "| reward: -95.242437 |\n",
      "| reward: -96.202733 |\n",
      "| reward: -82.027067 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 150      |\n",
      "| mean episode reward    | -93.3    |\n",
      "| steps                  | 1207     |\n",
      "-------------------------------------\n",
      "| reward: -70.136269 |\n",
      "| reward: -91.473884 |\n",
      "| reward: -94.338413 |\n",
      "| reward: -27.128781 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -63.516298 |\n",
      "| reward: -93.396688 |\n",
      "| reward: -96.202871 |\n",
      "| reward: -82.973017 |\n",
      "| reward: -96.212218 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 160      |\n",
      "| mean episode reward    | -92.1    |\n",
      "| steps                  | 1382     |\n",
      "-------------------------------------\n",
      "| reward: -92.440144 |\n",
      "| reward: -90.522276 |\n",
      "| reward: -89.592352 |\n",
      "| reward: -96.210781 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -95.242437 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -95.242437 |\n",
      "| reward: -74.451279 |\n",
      "| reward: -96.202733 |\n",
      "| reward: -93.391240 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 170      |\n",
      "| mean episode reward    | -92.1    |\n",
      "| steps                  | 1476     |\n",
      "-------------------------------------\n",
      "| reward: -36.224939 |\n",
      "| reward: -63.861005 |\n",
      "| reward: -96.211805 |\n",
      "| reward: -94.334628 |\n",
      "| reward: -96.221624 |\n",
      "| reward: -91.462120 |\n",
      "| reward: -89.592353 |\n",
      "| reward: -75.781409 |\n",
      "| reward: -83.914721 |\n",
      "| reward: -96.202732 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 180      |\n",
      "| mean episode reward    | -90.8    |\n",
      "| steps                  | 1659     |\n",
      "-------------------------------------\n",
      "| reward: -19.283428 |\n",
      "| reward: -84.856187 |\n",
      "| reward: -56.482662 |\n",
      "| reward: -92.442628 |\n",
      "| reward: -16.692779 |\n",
      "| reward: -93.397692 |\n",
      "| reward: -90.476793 |\n",
      "| reward: -86.751222 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -62.836837 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 190      |\n",
      "| mean episode reward    | -88.5    |\n",
      "| steps                  | 1949     |\n",
      "-------------------------------------\n",
      "| reward: -96.212986 |\n",
      "| reward: -91.492276 |\n",
      "| reward:  -7.429589 |\n",
      "| reward: -87.654720 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -96.208482 |\n",
      "| reward: -95.288046 |\n",
      "| reward: -71.611518 |\n",
      "| reward: -96.211531 |\n",
      "| reward: -97.159643 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 200      |\n",
      "| mean episode reward    | -87.5    |\n",
      "| steps                  | 2126     |\n",
      "-------------------------------------\n",
      "| reward: -96.212986 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -47.934397 |\n",
      "| reward: -95.248476 |\n",
      "| reward: -82.004581 |\n",
      "| reward: -94.292020 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -93.395828 |\n",
      "| reward: -56.454177 |\n",
      "| reward: -96.213965 |\n",
      "| reward: -79.647788 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 210      |\n",
      "| mean episode reward    | -86.4    |\n",
      "| steps                  | 2305     |\n",
      "-------------------------------------\n",
      "| reward: -75.359492 |\n",
      "| reward: -95.265032 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.214386 |\n",
      "| reward: -63.065010 |\n",
      "| reward: -70.495609 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -95.287629 |\n",
      "| reward: -95.284868 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 220      |\n",
      "| mean episode reward    | -85.9    |\n",
      "| steps                  | 2437     |\n",
      "-------------------------------------\n",
      "| reward: -87.681307 |\n",
      "| reward: -94.331267 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -77.734025 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -92.424885 |\n",
      "| reward: -74.410236 |\n",
      "| reward: -94.338197 |\n",
      "| reward: -50.153506 |\n",
      "| reward: -59.302165 |\n",
      "| reward: -71.877717 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 230      |\n",
      "| mean episode reward    | -84.6    |\n",
      "| steps                  | 2655     |\n",
      "-------------------------------------\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159378 |\n",
      "| reward: -94.291957 |\n",
      "| reward: -91.500706 |\n",
      "| reward: -89.596872 |\n",
      "| reward: -94.291659 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -95.286549 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -89.576085 |\n",
      "| reward: -95.242437 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 240      |\n",
      "| mean episode reward    | -84.5    |\n",
      "| steps                  | 2727     |\n",
      "-------------------------------------\n",
      "| reward: -95.239360 |\n",
      "| reward: -75.870224 |\n",
      "| reward: -96.215822 |\n",
      "| reward: -90.542917 |\n",
      "| reward: -95.286268 |\n",
      "| reward: -94.295675 |\n",
      "| reward: -81.387713 |\n",
      "| reward: -96.208457 |\n",
      "| reward: -97.160108 |\n",
      "| reward: -84.852444 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 250      |\n",
      "| mean episode reward    | -84.6    |\n",
      "| steps                  | 2832     |\n",
      "-------------------------------------\n",
      "| reward: -73.887680 |\n",
      "| reward: -95.250396 |\n",
      "| reward: -91.441993 |\n",
      "| reward: -61.155897 |\n",
      "| reward: -78.866059 |\n",
      "| reward: -74.632795 |\n",
      "| reward: -93.388795 |\n",
      "| reward: -82.108288 |\n",
      "| reward: -95.286927 |\n",
      "| reward: -93.392037 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 260      |\n",
      "| mean episode reward    | -84.9    |\n",
      "| steps                  | 2998     |\n",
      "-------------------------------------\n",
      "| reward: -89.587553 |\n",
      "| reward: -90.544786 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -87.681340 |\n",
      "| reward: -93.388554 |\n",
      "| reward: -52.049488 |\n",
      "| reward: -91.463881 |\n",
      "| reward: -96.202733 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 270      |\n",
      "| mean episode reward    | -84.6    |\n",
      "| steps                  | 3117     |\n",
      "-------------------------------------\n",
      "| reward: -75.206011 |\n",
      "| reward: -91.492707 |\n",
      "| reward: -95.242437 |\n",
      "| reward: -91.467149 |\n",
      "| reward: -95.282201 |\n",
      "| reward: -95.288046 |\n",
      "| reward: -45.212322 |\n",
      "| reward: -92.461062 |\n",
      "| reward: -91.499834 |\n",
      "| reward: -73.769817 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 280      |\n",
      "| mean episode reward    | -84.8    |\n",
      "| steps                  | 3268     |\n",
      "-------------------------------------\n",
      "| reward: -93.396348 |\n",
      "| reward: -87.708054 |\n",
      "| reward: -95.282201 |\n",
      "| reward: -95.287629 |\n",
      "| reward: -95.248051 |\n",
      "| reward: -96.195669 |\n",
      "| reward: -95.248418 |\n",
      "| reward: -95.249555 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.211078 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 290      |\n",
      "| mean episode reward    | -87.3    |\n",
      "| steps                  | 3334     |\n",
      "-------------------------------------\n",
      "| reward: -97.159643 |\n",
      "| reward: -77.544948 |\n",
      "| reward: -95.242437 |\n",
      "| reward: -89.585096 |\n",
      "| reward: -95.282201 |\n",
      "| reward: -96.229138 |\n",
      "| reward: -95.248051 |\n",
      "| reward: -96.195047 |\n",
      "| reward: -95.248922 |\n",
      "| reward: -90.552717 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 300      |\n",
      "| mean episode reward    | -88.2    |\n",
      "| steps                  | 3419     |\n",
      "-------------------------------------\n",
      "| reward: -85.847122 |\n",
      "| reward: -95.242499 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -96.194981 |\n",
      "| reward: -92.404498 |\n",
      "| reward: -95.288046 |\n",
      "| reward: -67.774702 |\n",
      "| reward: -94.346176 |\n",
      "| reward: -95.277854 |\n",
      "| reward: -96.211198 |\n",
      "| reward: -89.604861 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 310      |\n",
      "| mean episode reward    | -88.9    |\n",
      "| steps                  | 3521     |\n",
      "-------------------------------------\n",
      "| reward: -94.302408 |\n",
      "| reward: -96.202733 |\n",
      "| reward: -95.284868 |\n",
      "| reward: -89.603564 |\n",
      "| reward: -95.243117 |\n",
      "| reward: -87.773548 |\n",
      "| reward: -88.656730 |\n",
      "| reward: -95.282168 |\n",
      "| reward: -91.464216 |\n",
      "| reward: -85.774914 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 320      |\n",
      "| mean episode reward    | -89.3    |\n",
      "| steps                  | 3616     |\n",
      "-------------------------------------\n",
      "| reward: -95.243390 |\n",
      "| reward: -96.202726 |\n",
      "| reward: -90.508454 |\n",
      "| reward: -95.247920 |\n",
      "| reward: -81.589344 |\n",
      "| reward: -95.279120 |\n",
      "| reward: -94.338765 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -95.248051 |\n",
      "| reward: -91.466452 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 330      |\n",
      "| mean episode reward    | -90.6    |\n",
      "| steps                  | 3697     |\n",
      "-------------------------------------\n",
      "| reward: -94.304511 |\n",
      "| reward: -95.283991 |\n",
      "| reward: -96.221630 |\n",
      "| reward: -95.248576 |\n",
      "| reward: -95.277905 |\n",
      "| reward: -94.336533 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -95.287629 |\n",
      "| reward: -91.454481 |\n",
      "| reward: -86.777836 |\n",
      "| reward: -59.298317 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 340      |\n",
      "| mean episode reward    | -90.2    |\n",
      "| steps                  | 3803     |\n",
      "-------------------------------------\n",
      "| reward: -94.297602 |\n",
      "| reward: -92.460745 |\n",
      "| reward: -95.287629 |\n",
      "| reward: -95.279423 |\n",
      "| reward: -93.394355 |\n",
      "| reward: -78.954650 |\n",
      "| reward: -96.208738 |\n",
      "| reward: -94.292803 |\n",
      "| reward: -94.293818 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -95.286268 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 350      |\n",
      "| mean episode reward    | -90.5    |\n",
      "| steps                  | 3886     |\n",
      "-------------------------------------\n",
      "| reward: -91.502012 |\n",
      "| reward: -94.338013 |\n",
      "| reward: -94.293544 |\n",
      "| reward: -94.337983 |\n",
      "| reward: -95.282201 |\n",
      "| reward: -91.454090 |\n",
      "| reward: -86.471991 |\n",
      "| reward: -95.242445 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -85.831343 |\n",
      "| reward: -93.325762 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 360      |\n",
      "| mean episode reward    | -91.3    |\n",
      "| steps                  | 3978     |\n",
      "-------------------------------------\n",
      "| reward: -94.339824 |\n",
      "| reward: -95.270300 |\n",
      "| reward: -94.336644 |\n",
      "| reward: -95.280189 |\n",
      "| reward: -96.229078 |\n",
      "| reward: -92.431507 |\n",
      "| reward: -96.215267 |\n",
      "| reward: -96.208415 |\n",
      "| reward: -95.242436 |\n",
      "| reward: -92.437402 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 370      |\n",
      "| mean episode reward    | -91.9    |\n",
      "| steps                  | 4043     |\n",
      "-------------------------------------\n",
      "| reward: -91.462514 |\n",
      "| reward: -95.248731 |\n",
      "| reward: -82.983799 |\n",
      "| reward: -95.242508 |\n",
      "| reward: -88.634367 |\n",
      "| reward: -91.509299 |\n",
      "| reward: -96.212218 |\n",
      "| reward: -74.650885 |\n",
      "| reward: -96.189375 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -95.242757 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 380      |\n",
      "| mean episode reward    | -92.5    |\n",
      "| steps                  | 4150     |\n",
      "-------------------------------------\n",
      "| reward: -90.548170 |\n",
      "| reward: -88.645333 |\n",
      "| reward: -96.229138 |\n",
      "| reward: -96.221624 |\n",
      "| reward: -96.228936 |\n",
      "| reward: -94.296943 |\n",
      "| reward: -68.163648 |\n",
      "| reward: -81.076402 |\n",
      "| reward: -92.417790 |\n",
      "| reward: -95.287629 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 390      |\n",
      "| mean episode reward    | -92      |\n",
      "| steps                  | 4264     |\n",
      "-------------------------------------\n",
      "| reward: -88.625270 |\n",
      "| reward: -82.482893 |\n",
      "| reward: -93.366183 |\n",
      "| reward: -94.338986 |\n",
      "| reward: -72.137070 |\n",
      "| reward: -95.282149 |\n",
      "| reward: -90.551184 |\n",
      "| reward: -90.597715 |\n",
      "| reward: -93.389073 |\n",
      "| reward: -97.159924 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 400      |\n",
      "| mean episode reward    | -91.7    |\n",
      "| steps                  | 4380     |\n",
      "-------------------------------------\n",
      "| reward: -96.226361 |\n",
      "| reward: -94.335774 |\n",
      "| reward: -81.983949 |\n",
      "| reward: -92.444441 |\n",
      "| reward: -93.351153 |\n",
      "| reward: -95.248434 |\n",
      "| reward: -95.249213 |\n",
      "| reward: -84.869172 |\n",
      "| reward: -79.366443 |\n",
      "| reward: -96.214985 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 410      |\n",
      "| mean episode reward    | -91.7    |\n",
      "| steps                  | 4485     |\n",
      "-------------------------------------\n",
      "| reward: -95.242822 |\n",
      "| reward: -96.204221 |\n",
      "| reward: -95.283350 |\n",
      "| reward: -97.160596 |\n",
      "| reward: -90.520163 |\n",
      "| reward: -88.602461 |\n",
      "| reward: -95.249087 |\n",
      "| reward: -96.194989 |\n",
      "| reward: -95.242813 |\n",
      "| reward: -95.248163 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 420      |\n",
      "| mean episode reward    | -91.9    |\n",
      "| steps                  | 4553     |\n",
      "-------------------------------------\n",
      "| reward: -94.334017 |\n",
      "| reward: -93.385659 |\n",
      "| reward: -93.388003 |\n",
      "| reward: -96.189375 |\n",
      "| reward: -95.242851 |\n",
      "| reward: -96.214672 |\n",
      "| reward: -92.443076 |\n",
      "| reward: -95.242914 |\n",
      "| reward: -94.291832 |\n",
      "| reward: -97.159643 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 430      |\n",
      "| mean episode reward    | -92.1    |\n",
      "| steps                  | 4618     |\n",
      "-------------------------------------\n",
      "| reward: -96.216951 |\n",
      "| reward: -91.457541 |\n",
      "| reward: -88.696072 |\n",
      "| reward: -93.372249 |\n",
      "| reward: -95.287629 |\n",
      "| reward: -95.276491 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -92.410512 |\n",
      "| reward: -86.773189 |\n",
      "| reward: -95.242758 |\n",
      "| reward: -92.440408 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 440      |\n",
      "| mean episode reward    | -92.3    |\n",
      "| steps                  | 4705     |\n",
      "-------------------------------------\n",
      "| reward: -93.387663 |\n",
      "| reward: -94.320054 |\n",
      "| reward: -81.212466 |\n",
      "| reward: -91.463264 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -76.251135 |\n",
      "| reward: -95.254800 |\n",
      "| reward: -96.194990 |\n",
      "| reward: -95.280903 |\n",
      "| reward: -96.220915 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 450      |\n",
      "| mean episode reward    | -92.2    |\n",
      "| steps                  | 4801     |\n",
      "-------------------------------------\n",
      "| reward: -96.212986 |\n",
      "| reward: -94.293140 |\n",
      "| reward: -92.437347 |\n",
      "| reward: -91.500050 |\n",
      "| reward: -93.386328 |\n",
      "| reward: -96.189719 |\n",
      "| reward: -97.159100 |\n",
      "| reward: -95.287629 |\n",
      "| reward: -95.287584 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -93.386730 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 460      |\n",
      "| mean episode reward    | -92.4    |\n",
      "| steps                  | 4869     |\n",
      "-------------------------------------\n",
      "| reward: -92.407633 |\n",
      "| reward: -95.242437 |\n",
      "| reward: -96.194989 |\n",
      "| reward: -95.248418 |\n",
      "| reward: -94.335556 |\n",
      "| reward: -95.282201 |\n",
      "| reward: -91.496949 |\n",
      "| reward: -95.239985 |\n",
      "| reward: -94.337181 |\n",
      "| reward: -94.322768 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 470      |\n",
      "| mean episode reward    | -92.4    |\n",
      "| steps                  | 4938     |\n",
      "-------------------------------------\n",
      "| reward: -93.351260 |\n",
      "| reward: -95.242437 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -96.194989 |\n",
      "| reward: -96.221624 |\n",
      "| reward: -95.287623 |\n",
      "| reward: -95.247920 |\n",
      "| reward: -93.374506 |\n",
      "| reward: -96.234567 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159924 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 480      |\n",
      "| mean episode reward    | -92.9    |\n",
      "| steps                  | 4995     |\n",
      "-------------------------------------\n",
      "| reward: -92.436192 |\n",
      "| reward: -88.658578 |\n",
      "| reward: -90.528466 |\n",
      "| reward: -96.213143 |\n",
      "| reward: -94.338378 |\n",
      "| reward: -94.336533 |\n",
      "| reward: -81.133265 |\n",
      "| reward: -84.850652 |\n",
      "| reward: -88.590386 |\n",
      "| reward: -89.597650 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 490      |\n",
      "| mean episode reward    | -92.9    |\n",
      "| steps                  | 5109     |\n",
      "-------------------------------------\n",
      "| reward: -94.335248 |\n",
      "| reward: -91.491813 |\n",
      "| reward: -95.248104 |\n",
      "| reward: -90.522412 |\n",
      "| reward: -93.333531 |\n",
      "| reward: -89.604852 |\n",
      "| reward: -95.288658 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -94.295562 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 500      |\n",
      "| mean episode reward    | -93.3    |\n",
      "| steps                  | 5184     |\n",
      "-------------------------------------\n",
      "| reward: -93.399534 |\n",
      "| reward: -82.818302 |\n",
      "| reward: -95.246923 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -95.249572 |\n",
      "| reward: -94.296814 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.214437 |\n",
      "| reward: -97.160306 |\n",
      "| reward: -91.500794 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 510      |\n",
      "| mean episode reward    | -93.6    |\n",
      "| steps                  | 5256     |\n",
      "-------------------------------------\n",
      "| reward: -93.335108 |\n",
      "| reward: -95.255007 |\n",
      "| reward: -97.159046 |\n",
      "| reward: -87.707705 |\n",
      "| reward: -93.359877 |\n",
      "| reward: -95.242437 |\n",
      "| reward: -95.248051 |\n",
      "| reward: -92.404792 |\n",
      "| reward: -95.242562 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -95.242567 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 520      |\n",
      "| mean episode reward    | -93.6    |\n",
      "| steps                  | 5329     |\n",
      "-------------------------------------\n",
      "| reward: -95.287527 |\n",
      "| reward: -95.275330 |\n",
      "| reward: -90.547027 |\n",
      "| reward: -81.454873 |\n",
      "| reward: -96.195107 |\n",
      "| reward: -93.367224 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -90.555376 |\n",
      "| reward: -95.270550 |\n",
      "| reward: -96.212218 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 530      |\n",
      "| mean episode reward    | -93.4    |\n",
      "| steps                  | 5411     |\n",
      "-------------------------------------\n",
      "| reward: -86.720067 |\n",
      "| reward: -94.296581 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -92.410722 |\n",
      "| reward: -95.244802 |\n",
      "| reward: -95.243185 |\n",
      "| reward: -91.404901 |\n",
      "| reward: -93.376262 |\n",
      "| reward: -96.214197 |\n",
      "| reward: -92.467092 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 540      |\n",
      "| mean episode reward    | -93.5    |\n",
      "| steps                  | 5490     |\n",
      "-------------------------------------\n",
      "| reward: -95.248051 |\n",
      "| reward: -95.242442 |\n",
      "| reward: -88.654796 |\n",
      "| reward: -93.350096 |\n",
      "| reward: -94.297330 |\n",
      "| reward: -94.305547 |\n",
      "| reward: -95.252010 |\n",
      "| reward: -93.405534 |\n",
      "| reward: -95.263740 |\n",
      "| reward: -86.791178 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 550      |\n",
      "| mean episode reward    | -93.6    |\n",
      "| steps                  | 5572     |\n",
      "-------------------------------------\n",
      "| reward: -91.496909 |\n",
      "| reward: -95.242793 |\n",
      "| reward: -95.282144 |\n",
      "| reward: -95.282138 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.215363 |\n",
      "| reward: -92.422123 |\n",
      "| reward: -89.581288 |\n",
      "| reward: -95.287629 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -96.229076 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 560      |\n",
      "| mean episode reward    | -93.6    |\n",
      "| steps                  | 5641     |\n",
      "-------------------------------------\n",
      "| reward: -93.399531 |\n",
      "| reward: -84.497727 |\n",
      "| reward: -89.602393 |\n",
      "| reward: -94.335286 |\n",
      "| reward: -94.289925 |\n",
      "| reward: -95.242585 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -95.243230 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.210715 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 570      |\n",
      "| mean episode reward    | -93.5    |\n",
      "| steps                  | 5717     |\n",
      "-------------------------------------\n",
      "| reward: -93.393324 |\n",
      "| reward: -82.950845 |\n",
      "| reward: -95.269107 |\n",
      "| reward: -94.306693 |\n",
      "| reward: -87.685424 |\n",
      "| reward: -91.462619 |\n",
      "| reward: -94.341788 |\n",
      "| reward: -91.442291 |\n",
      "| reward: -95.284868 |\n",
      "| reward: -95.290146 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 580      |\n",
      "| mean episode reward    | -93.2    |\n",
      "| steps                  | 5810     |\n",
      "-------------------------------------\n",
      "| reward: -89.630930 |\n",
      "| reward: -96.208738 |\n",
      "| reward: -96.215847 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.212465 |\n",
      "| reward: -94.303587 |\n",
      "| reward: -94.342521 |\n",
      "| reward: -56.154524 |\n",
      "| reward: -92.468978 |\n",
      "| reward: -95.267806 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 590      |\n",
      "| mean episode reward    | -93.3    |\n",
      "| steps                  | 5914     |\n",
      "-------------------------------------\n",
      "| reward: -91.493543 |\n",
      "| reward: -95.268835 |\n",
      "| reward: -95.241887 |\n",
      "| reward: -94.293514 |\n",
      "| reward: -95.248051 |\n",
      "| reward: -92.430121 |\n",
      "| reward: -94.341997 |\n",
      "| reward: -92.451056 |\n",
      "| reward: -95.265081 |\n",
      "| reward: -96.215072 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 600      |\n",
      "| mean episode reward    | -93.3    |\n",
      "| steps                  | 5985     |\n",
      "-------------------------------------\n",
      "| reward: -95.242985 |\n",
      "| reward: -95.279837 |\n",
      "| reward: -95.287308 |\n",
      "| reward: -95.286504 |\n",
      "| reward: -95.284868 |\n",
      "| reward: -87.694916 |\n",
      "| reward: -92.453853 |\n",
      "| reward: -95.248059 |\n",
      "| reward: -94.340239 |\n",
      "| reward: -95.248055 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 610      |\n",
      "| mean episode reward    | -93.3    |\n",
      "| steps                  | 6057     |\n",
      "-------------------------------------\n",
      "| reward: -95.242714 |\n",
      "| reward: -94.294611 |\n",
      "| reward: -95.243805 |\n",
      "| reward: -96.204703 |\n",
      "| reward: -97.159204 |\n",
      "| reward: -95.281822 |\n",
      "| reward: -94.315051 |\n",
      "| reward: -96.212693 |\n",
      "| reward: -93.374935 |\n",
      "| reward: -96.214437 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 620      |\n",
      "| mean episode reward    | -93.5    |\n",
      "| steps                  | 6116     |\n",
      "-------------------------------------\n",
      "| reward: -95.276088 |\n",
      "| reward: -96.212904 |\n",
      "| reward: -91.445993 |\n",
      "| reward: -91.482709 |\n",
      "| reward: -94.292711 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -95.284868 |\n",
      "| reward: -94.323781 |\n",
      "| reward: -95.287629 |\n",
      "| reward: -95.286777 |\n",
      "| reward: -97.159643 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 630      |\n",
      "| mean episode reward    | -93.6    |\n",
      "| steps                  | 6183     |\n",
      "-------------------------------------\n",
      "| reward: -86.793511 |\n",
      "| reward: -96.212440 |\n",
      "| reward: -95.263120 |\n",
      "| reward: -97.160515 |\n",
      "| reward: -90.513432 |\n",
      "| reward: -94.303352 |\n",
      "| reward: -92.408401 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -87.760315 |\n",
      "| reward: -96.227770 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 640      |\n",
      "| mean episode reward    | -93.6    |\n",
      "| steps                  | 6263     |\n",
      "-------------------------------------\n",
      "| reward: -95.253072 |\n",
      "| reward: -90.548809 |\n",
      "| reward: -95.243143 |\n",
      "| reward: -95.248471 |\n",
      "| reward: -96.210542 |\n",
      "| reward: -95.246706 |\n",
      "| reward: -94.298751 |\n",
      "| reward: -95.282202 |\n",
      "| reward: -94.305004 |\n",
      "| reward: -95.287490 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 650      |\n",
      "| mean episode reward    | -93.7    |\n",
      "| steps                  | 6329     |\n",
      "-------------------------------------\n",
      "| reward: -83.877031 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -96.202733 |\n",
      "| reward: -95.248104 |\n",
      "| reward: -95.247718 |\n",
      "| reward: -92.423648 |\n",
      "| reward: -97.160596 |\n",
      "| reward: -97.159924 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 660      |\n",
      "| mean episode reward    | -93.8    |\n",
      "| steps                  | 6393     |\n",
      "-------------------------------------\n",
      "| reward: -94.332148 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.211728 |\n",
      "| reward: -96.212542 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -93.407541 |\n",
      "| reward: -95.242437 |\n",
      "| reward: -93.319421 |\n",
      "| reward: -95.242496 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 670      |\n",
      "| mean episode reward    | -94      |\n",
      "| steps                  | 6450     |\n",
      "-------------------------------------\n",
      "| reward: -95.246947 |\n",
      "| reward: -48.741719 |\n",
      "| reward: -91.431444 |\n",
      "| reward: -95.290141 |\n",
      "| reward: -95.287629 |\n",
      "| reward: -95.290152 |\n",
      "| reward: -95.247263 |\n",
      "| reward: -95.239338 |\n",
      "| reward: -95.248680 |\n",
      "| reward: -94.296193 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 680      |\n",
      "| mean episode reward    | -93.8    |\n",
      "| steps                  | 6553     |\n",
      "-------------------------------------\n",
      "| reward: -88.609476 |\n",
      "| reward: -95.243230 |\n",
      "| reward: -87.510732 |\n",
      "| reward: -95.250162 |\n",
      "| reward: -93.360006 |\n",
      "| reward: -95.282201 |\n",
      "| reward: -84.575594 |\n",
      "| reward: -96.208738 |\n",
      "| reward: -96.196493 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -97.159924 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 690      |\n",
      "| mean episode reward    | -94      |\n",
      "| steps                  | 6637     |\n",
      "-------------------------------------\n",
      "| reward: -95.243931 |\n",
      "| reward: -91.502431 |\n",
      "| reward: -85.815644 |\n",
      "| reward: -93.329782 |\n",
      "| reward: -91.507531 |\n",
      "| reward: -96.213471 |\n",
      "| reward: -94.323554 |\n",
      "| reward: -94.340254 |\n",
      "| reward: -96.221532 |\n",
      "| reward: -96.229138 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 700      |\n",
      "| mean episode reward    | -93.9    |\n",
      "| steps                  | 6716     |\n",
      "-------------------------------------\n",
      "| reward: -96.217131 |\n",
      "| reward: -96.217681 |\n",
      "| reward: -91.498037 |\n",
      "| reward: -95.279120 |\n",
      "| reward: -96.215009 |\n",
      "| reward: -96.211531 |\n",
      "| reward: -97.161398 |\n",
      "| reward: -93.378476 |\n",
      "| reward: -93.353336 |\n",
      "| reward: -93.378506 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 710      |\n",
      "| mean episode reward    | -94      |\n",
      "| steps                  | 6780     |\n",
      "-------------------------------------\n",
      "| reward: -92.390880 |\n",
      "| reward: -86.627385 |\n",
      "| reward: -96.217131 |\n",
      "| reward: -92.443718 |\n",
      "| reward: -96.217131 |\n",
      "| reward: -88.582008 |\n",
      "| reward: -92.424578 |\n",
      "| reward: -95.288466 |\n",
      "| reward: -94.282761 |\n",
      "| reward: -63.222743 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 720      |\n",
      "| mean episode reward    | -93.4    |\n",
      "| steps                  | 6896     |\n",
      "-------------------------------------\n",
      "| reward: -93.343191 |\n",
      "| reward: -91.473091 |\n",
      "| reward: -95.286613 |\n",
      "| reward: -90.502673 |\n",
      "| reward: -89.604694 |\n",
      "| reward: -96.195669 |\n",
      "| reward: -93.348075 |\n",
      "| reward: -95.247154 |\n",
      "| reward: -78.740069 |\n",
      "| reward: -90.549845 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 730      |\n",
      "| mean episode reward    | -93.1    |\n",
      "| steps                  | 6995     |\n",
      "-------------------------------------\n",
      "| reward: -93.398994 |\n",
      "| reward: -95.242580 |\n",
      "| reward: -91.477576 |\n",
      "| reward: -96.227770 |\n",
      "| reward: -81.006235 |\n",
      "| reward: -94.345751 |\n",
      "| reward: -95.281816 |\n",
      "| reward: -95.286058 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -88.658497 |\n",
      "| reward: -94.339500 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 740      |\n",
      "| mean episode reward    | -93      |\n",
      "| steps                  | 7084     |\n",
      "-------------------------------------\n",
      "| reward: -92.446566 |\n",
      "| reward: -97.161375 |\n",
      "| reward: -95.254431 |\n",
      "| reward: -90.532239 |\n",
      "| reward: -96.226361 |\n",
      "| reward: -81.358144 |\n",
      "| reward: -92.402352 |\n",
      "| reward: -81.982249 |\n",
      "| reward: -92.442672 |\n",
      "| reward: -95.248799 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 750      |\n",
      "| mean episode reward    | -92.7    |\n",
      "| steps                  | 7183     |\n",
      "-------------------------------------\n",
      "| reward: -95.282201 |\n",
      "| reward: -95.277905 |\n",
      "| reward: -96.221621 |\n",
      "| reward: -93.352678 |\n",
      "| reward: -88.619786 |\n",
      "| reward: -83.936485 |\n",
      "| reward: -95.254399 |\n",
      "| reward: -95.248360 |\n",
      "| reward: -83.052864 |\n",
      "| reward: -93.355131 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 760      |\n",
      "| mean episode reward    | -92.4    |\n",
      "| steps                  | 7277     |\n",
      "-------------------------------------\n",
      "| reward: -94.340959 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -86.729285 |\n",
      "| reward: -94.343622 |\n",
      "| reward: -73.613654 |\n",
      "| reward: -95.243117 |\n",
      "| reward: -94.327883 |\n",
      "| reward: -95.243117 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -95.242499 |\n",
      "| reward: -92.431507 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 770      |\n",
      "| mean episode reward    | -92      |\n",
      "| steps                  | 7373     |\n",
      "-------------------------------------\n",
      "| reward: -83.958462 |\n",
      "| reward: -88.624872 |\n",
      "| reward: -95.289124 |\n",
      "| reward: -93.351792 |\n",
      "| reward: -93.354119 |\n",
      "| reward: -90.478090 |\n",
      "| reward: -92.420547 |\n",
      "| reward: -96.212046 |\n",
      "| reward: -94.323780 |\n",
      "| reward: -87.714204 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 780      |\n",
      "| mean episode reward    | -92.2    |\n",
      "| steps                  | 7472     |\n",
      "-------------------------------------\n",
      "| reward: -95.284868 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -89.531119 |\n",
      "| reward: -94.338241 |\n",
      "| reward: -86.508819 |\n",
      "| reward: -90.560044 |\n",
      "| reward: -93.349074 |\n",
      "| reward: -93.394956 |\n",
      "| reward: -96.195144 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -84.860122 |\n",
      "| reward: -94.296660 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 790      |\n",
      "| mean episode reward    | -92.1    |\n",
      "| steps                  | 7568     |\n",
      "-------------------------------------\n",
      "| reward: -96.194992 |\n",
      "| reward: -95.281378 |\n",
      "| reward: -94.310303 |\n",
      "| reward: -95.282201 |\n",
      "| reward: -90.542279 |\n",
      "| reward: -95.279858 |\n",
      "| reward: -73.340215 |\n",
      "| reward: -91.502823 |\n",
      "| reward: -88.665187 |\n",
      "| reward: -94.294670 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 800      |\n",
      "| mean episode reward    | -91.9    |\n",
      "| steps                  | 7667     |\n",
      "-------------------------------------\n",
      "| reward: -78.170593 |\n",
      "| reward: -94.329274 |\n",
      "| reward: -95.239338 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -79.813331 |\n",
      "| reward: -92.456865 |\n",
      "| reward: -96.213891 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -96.221488 |\n",
      "| reward: -85.772744 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 810      |\n",
      "| mean episode reward    | -91.5    |\n",
      "| steps                  | 7769     |\n",
      "-------------------------------------\n",
      "| reward: -82.064303 |\n",
      "| reward: -94.297853 |\n",
      "| reward: -89.609342 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -92.442234 |\n",
      "| reward: -97.158468 |\n",
      "| reward: -93.335141 |\n",
      "| reward: -95.284868 |\n",
      "| reward: -95.282201 |\n",
      "| reward: -96.229136 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 820      |\n",
      "| mean episode reward    | -91.9    |\n",
      "| steps                  | 7850     |\n",
      "-------------------------------------\n",
      "| reward: -92.443295 |\n",
      "| reward: -94.335854 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -76.373721 |\n",
      "| reward: -94.349960 |\n",
      "| reward: -96.234567 |\n",
      "| reward: -95.287393 |\n",
      "| reward: -95.279423 |\n",
      "| reward: -82.669861 |\n",
      "| reward: -96.215062 |\n",
      "| reward: -96.214062 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 830      |\n",
      "| mean episode reward    | -91.9    |\n",
      "| steps                  | 7943     |\n",
      "-------------------------------------\n",
      "| reward: -97.159643 |\n",
      "| reward: -96.213857 |\n",
      "| reward: -95.252294 |\n",
      "| reward: -96.194989 |\n",
      "| reward: -94.323780 |\n",
      "| reward: -95.282197 |\n",
      "| reward: -92.445989 |\n",
      "| reward: -95.281855 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -97.159643 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 840      |\n",
      "| mean episode reward    | -92.2    |\n",
      "| steps                  | 7999     |\n",
      "-------------------------------------\n",
      "| reward: -96.212986 |\n",
      "| reward: -93.389222 |\n",
      "| reward: -93.393715 |\n",
      "| reward: -88.369757 |\n",
      "| reward: -95.267603 |\n",
      "| reward: -85.772673 |\n",
      "| reward: -92.423596 |\n",
      "| reward: -93.373299 |\n",
      "| reward: -96.219955 |\n",
      "| reward: -86.768398 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 850      |\n",
      "| mean episode reward    | -92.3    |\n",
      "| steps                  | 8092     |\n",
      "-------------------------------------\n",
      "| reward: -92.427300 |\n",
      "| reward: -95.253072 |\n",
      "| reward: -95.243117 |\n",
      "| reward: -84.533407 |\n",
      "| reward: -95.286268 |\n",
      "| reward: -94.305690 |\n",
      "| reward: -89.571772 |\n",
      "| reward: -93.336219 |\n",
      "| reward: -65.372951 |\n",
      "| reward: -95.242515 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 860      |\n",
      "| mean episode reward    | -92.1    |\n",
      "| steps                  | 8205     |\n",
      "-------------------------------------\n",
      "| reward: -87.681058 |\n",
      "| reward: -71.812733 |\n",
      "| reward: -94.338404 |\n",
      "| reward: -95.287284 |\n",
      "| reward: -96.213326 |\n",
      "| reward: -95.288046 |\n",
      "| reward: -92.440260 |\n",
      "| reward: -94.310099 |\n",
      "| reward: -95.246392 |\n",
      "| reward: -81.503870 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 870      |\n",
      "| mean episode reward    | -92      |\n",
      "| steps                  | 8312     |\n",
      "-------------------------------------\n",
      "| reward: -92.459192 |\n",
      "| reward: -94.336052 |\n",
      "| reward: -94.338694 |\n",
      "| reward: -59.562211 |\n",
      "| reward: -97.161095 |\n",
      "| reward: -88.597139 |\n",
      "| reward: -96.205312 |\n",
      "| reward: -94.336533 |\n",
      "| reward: -95.287629 |\n",
      "| reward: -95.287053 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 880      |\n",
      "| mean episode reward    | -91.9    |\n",
      "| steps                  | 8413     |\n",
      "-------------------------------------\n",
      "| reward: -96.210272 |\n",
      "| reward: -94.340507 |\n",
      "| reward: -96.209632 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -91.484003 |\n",
      "| reward: -95.243230 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -96.229081 |\n",
      "| reward: -90.548501 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 890      |\n",
      "| mean episode reward    | -92.2    |\n",
      "| steps                  | 8475     |\n",
      "-------------------------------------\n",
      "| reward: -94.323780 |\n",
      "| reward: -96.212440 |\n",
      "| reward: -96.214169 |\n",
      "| reward: -96.208738 |\n",
      "| reward: -96.215351 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -94.296600 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.229136 |\n",
      "| reward: -95.279120 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 900      |\n",
      "| mean episode reward    | -92.6    |\n",
      "| steps                  | 8528     |\n",
      "-------------------------------------\n",
      "| reward: -96.229138 |\n",
      "| reward: -80.741765 |\n",
      "| reward: -95.284868 |\n",
      "| reward: -95.280832 |\n",
      "| reward: -94.329148 |\n",
      "| reward: -94.302032 |\n",
      "| reward: -98.106581 |\n",
      "| reward: -93.382176 |\n",
      "| reward: -95.282058 |\n",
      "| reward: -94.339477 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 910      |\n",
      "| mean episode reward    | -92.9    |\n",
      "| steps                  | 8604     |\n",
      "-------------------------------------\n",
      "| reward: -96.215474 |\n",
      "| reward: -95.277905 |\n",
      "| reward: -88.657336 |\n",
      "| reward: -92.414137 |\n",
      "| reward: -94.346623 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -97.159012 |\n",
      "| reward: -95.281161 |\n",
      "| reward: -91.527308 |\n",
      "| reward: -96.213882 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 920      |\n",
      "| mean episode reward    | -93      |\n",
      "| steps                  | 8674     |\n",
      "-------------------------------------\n",
      "| reward: -94.297817 |\n",
      "| reward: -93.360099 |\n",
      "| reward: -96.212044 |\n",
      "| reward: -95.250042 |\n",
      "| reward: -96.190055 |\n",
      "| reward: -97.158187 |\n",
      "| reward: -90.582437 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -95.282198 |\n",
      "| reward: -95.279423 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 930      |\n",
      "| mean episode reward    | -93.3    |\n",
      "| steps                  | 8736     |\n",
      "-------------------------------------\n",
      "| reward: -96.229138 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -81.777990 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -96.217131 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.161095 |\n",
      "| reward: -95.242499 |\n",
      "| reward: -91.489756 |\n",
      "| reward: -88.639827 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 940      |\n",
      "| mean episode reward    | -93.1    |\n",
      "| steps                  | 8811     |\n",
      "-------------------------------------\n",
      "| reward: -95.245271 |\n",
      "| reward: -96.214100 |\n",
      "| reward: -92.423053 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -96.217450 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -95.280686 |\n",
      "| reward: -96.210611 |\n",
      "| reward: -20.674885 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 950      |\n",
      "| mean episode reward    | -92.7    |\n",
      "| steps                  | 8919     |\n",
      "-------------------------------------\n",
      "| reward: -96.217131 |\n",
      "| reward: -97.159842 |\n",
      "| reward: -96.213804 |\n",
      "| reward: -95.251082 |\n",
      "| reward: -83.913637 |\n",
      "| reward: -92.426361 |\n",
      "| reward: -96.213543 |\n",
      "| reward: -96.221016 |\n",
      "| reward: -95.282201 |\n",
      "| reward: -95.286925 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 960      |\n",
      "| mean episode reward    | -93.2    |\n",
      "| steps                  | 8988     |\n",
      "-------------------------------------\n",
      "| reward: -86.567402 |\n",
      "| reward: -95.287575 |\n",
      "| reward: -96.203032 |\n",
      "| reward: -93.393072 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -93.345542 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -96.229138 |\n",
      "| reward: -95.279584 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 970      |\n",
      "| mean episode reward    | -93.6    |\n",
      "| steps                  | 9053     |\n",
      "-------------------------------------\n",
      "| reward: -94.298722 |\n",
      "| reward: -94.302518 |\n",
      "| reward: -91.490908 |\n",
      "| reward: -96.214926 |\n",
      "| reward: -91.499060 |\n",
      "| reward: -90.565942 |\n",
      "| reward: -95.244227 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -82.962460 |\n",
      "| reward: -94.349356 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 980      |\n",
      "| mean episode reward    | -93.8    |\n",
      "| steps                  | 9138     |\n",
      "-------------------------------------\n",
      "| reward: -91.448040 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -94.347571 |\n",
      "| reward: -63.616068 |\n",
      "| reward: -95.275739 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.215783 |\n",
      "| reward: -95.290152 |\n",
      "| reward: -95.280409 |\n",
      "| reward: -93.361466 |\n",
      "| reward: -91.533518 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 990      |\n",
      "| mean episode reward    | -93.5    |\n",
      "| steps                  | 9238     |\n",
      "-------------------------------------\n",
      "| reward: -96.211531 |\n",
      "| reward: -93.378331 |\n",
      "| reward: -96.212218 |\n",
      "| reward: -97.159814 |\n",
      "| reward: -95.287236 |\n",
      "| reward: -86.765056 |\n",
      "| reward: -78.646805 |\n",
      "| reward: -95.279120 |\n",
      "| reward: -95.284524 |\n",
      "| reward: -95.287475 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1000     |\n",
      "| mean episode reward    | -93.2    |\n",
      "| steps                  | 9322     |\n",
      "-------------------------------------\n",
      "| reward: -95.281785 |\n",
      "| reward: -96.216161 |\n",
      "| reward: -95.287581 |\n",
      "| reward: -94.295456 |\n",
      "| reward: -95.287950 |\n",
      "| reward: -95.279450 |\n",
      "| reward: -93.378235 |\n",
      "| reward: -68.873921 |\n",
      "| reward: -92.444047 |\n",
      "| reward: -91.499815 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1010     |\n",
      "| mean episode reward    | -93      |\n",
      "| steps                  | 9417     |\n",
      "-------------------------------------\n",
      "| reward: -97.159924 |\n",
      "| reward: -90.537741 |\n",
      "| reward: -86.435039 |\n",
      "| reward: -86.811576 |\n",
      "| reward: -95.284868 |\n",
      "| reward: -94.324357 |\n",
      "| reward: -96.233206 |\n",
      "| reward: -93.388211 |\n",
      "| reward: -88.661054 |\n",
      "| reward: -94.293416 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1020     |\n",
      "| mean episode reward    | -92.8    |\n",
      "| steps                  | 9508     |\n",
      "-------------------------------------\n",
      "| reward: -91.466952 |\n",
      "| reward: -95.279423 |\n",
      "| reward: -96.194989 |\n",
      "| reward: -95.279120 |\n",
      "| reward: -87.736476 |\n",
      "| reward: -93.391006 |\n",
      "| reward: -86.765074 |\n",
      "| reward: -93.337998 |\n",
      "| reward: -91.498410 |\n",
      "| reward: -92.448601 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1030     |\n",
      "| mean episode reward    | -92.5    |\n",
      "| steps                  | 9599     |\n",
      "-------------------------------------\n",
      "| reward: -90.517666 |\n",
      "| reward: -97.161375 |\n",
      "| reward: -95.279725 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -95.282195 |\n",
      "| reward: -96.220915 |\n",
      "| reward: -93.348934 |\n",
      "| reward: -90.524176 |\n",
      "| reward: -90.550894 |\n",
      "| reward: -86.593401 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1040     |\n",
      "| mean episode reward    | -92.4    |\n",
      "| steps                  | 9680     |\n",
      "-------------------------------------\n",
      "| reward: -91.492048 |\n",
      "| reward: -95.288611 |\n",
      "| reward: -95.283262 |\n",
      "| reward: -89.612747 |\n",
      "| reward: -95.287498 |\n",
      "| reward: -94.300266 |\n",
      "| reward: -95.282202 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -86.662706 |\n",
      "| reward: -95.242002 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1050     |\n",
      "| mean episode reward    | -93      |\n",
      "| steps                  | 9758     |\n",
      "-------------------------------------\n",
      "| reward: -94.335752 |\n",
      "| reward: -89.513185 |\n",
      "| reward: -95.281161 |\n",
      "| reward: -71.288317 |\n",
      "| reward: -89.621450 |\n",
      "| reward: -85.890111 |\n",
      "| reward: -95.287893 |\n",
      "| reward: -93.388259 |\n",
      "| reward: -96.227770 |\n",
      "| reward: -92.394084 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1060     |\n",
      "| mean episode reward    | -92.5    |\n",
      "| steps                  | 9868     |\n",
      "-------------------------------------\n",
      "| reward: -96.213543 |\n",
      "| reward: -94.305088 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -96.212440 |\n",
      "| reward: -95.282113 |\n",
      "| reward: -90.520209 |\n",
      "| reward: -96.216098 |\n",
      "| reward: -96.203068 |\n",
      "| reward: -94.346575 |\n",
      "| reward: -96.202768 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1070     |\n",
      "| mean episode reward    | -92.6    |\n",
      "| steps                  | 9928     |\n",
      "-------------------------------------\n",
      "| reward: -97.159128 |\n",
      "| reward: -95.284868 |\n",
      "| reward: -96.213543 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -97.160108 |\n",
      "| reward: -93.351216 |\n",
      "| reward: -76.799170 |\n",
      "| reward: -95.262120 |\n",
      "| reward: -94.297256 |\n",
      "| reward: -89.309770 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1080     |\n",
      "| mean episode reward    | -92.6    |\n",
      "| steps                  | 10009    |\n",
      "-------------------------------------\n",
      "| reward: -95.242499 |\n",
      "| reward: -91.452253 |\n",
      "| reward: -96.203026 |\n",
      "| reward: -96.194989 |\n",
      "| reward: -88.618938 |\n",
      "| reward: -96.215036 |\n",
      "| reward: -84.869440 |\n",
      "| reward: -96.212352 |\n",
      "| reward: -94.344321 |\n",
      "| reward: -88.669915 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1090     |\n",
      "| mean episode reward    | -92.8    |\n",
      "| steps                  | 10095    |\n",
      "-------------------------------------\n",
      "| reward: -92.440182 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -95.242002 |\n",
      "| reward: -95.242542 |\n",
      "| reward: -94.295655 |\n",
      "| reward: -95.248841 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.227770 |\n",
      "| reward: -91.463981 |\n",
      "| reward: -95.270300 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1100     |\n",
      "| mean episode reward    | -93      |\n",
      "| steps                  | 10159    |\n",
      "-------------------------------------\n",
      "| reward: -91.467922 |\n",
      "| reward: -86.773780 |\n",
      "| reward: -93.403922 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -95.282076 |\n",
      "| reward: -91.456180 |\n",
      "| reward: -90.468140 |\n",
      "| reward: -95.286268 |\n",
      "| reward: -86.818047 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1110     |\n",
      "| mean episode reward    | -93      |\n",
      "| steps                  | 10248    |\n",
      "-------------------------------------\n",
      "| reward: -95.281922 |\n",
      "| reward: -96.221516 |\n",
      "| reward: -94.279443 |\n",
      "| reward: -94.338344 |\n",
      "| reward: -95.242439 |\n",
      "| reward: -93.373686 |\n",
      "| reward: -96.212337 |\n",
      "| reward: -91.459618 |\n",
      "| reward: -97.160515 |\n",
      "| reward: -92.441536 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1120     |\n",
      "| mean episode reward    | -93.3    |\n",
      "| steps                  | 10315    |\n",
      "-------------------------------------\n",
      "| reward: -96.229129 |\n",
      "| reward: -95.287314 |\n",
      "| reward: -95.282207 |\n",
      "| reward: -95.287256 |\n",
      "| reward: -96.221602 |\n",
      "| reward: -95.287206 |\n",
      "| reward: -96.221568 |\n",
      "| reward: -90.450908 |\n",
      "| reward: -95.279423 |\n",
      "| reward: -92.419759 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1130     |\n",
      "| mean episode reward    | -93.5    |\n",
      "| steps                  | 10380    |\n",
      "-------------------------------------\n",
      "| reward: -84.822893 |\n",
      "| reward: -97.159924 |\n",
      "| reward: -96.208618 |\n",
      "| reward: -93.382019 |\n",
      "| reward: -94.325157 |\n",
      "| reward: -96.213105 |\n",
      "| reward: -95.243230 |\n",
      "| reward: -92.409010 |\n",
      "| reward: -92.438960 |\n",
      "| reward: -96.221484 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1140     |\n",
      "| mean episode reward    | -93.6    |\n",
      "| steps                  | 10455    |\n",
      "-------------------------------------\n",
      "| reward: -96.212842 |\n",
      "| reward: -92.443027 |\n",
      "| reward: -94.330805 |\n",
      "| reward: -88.612472 |\n",
      "| reward: -91.464890 |\n",
      "| reward: -84.916664 |\n",
      "| reward: -93.397202 |\n",
      "| reward: -82.823364 |\n",
      "| reward: -91.495690 |\n",
      "| reward: -94.336229 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1150     |\n",
      "| mean episode reward    | -93.3    |\n",
      "| steps                  | 10560    |\n",
      "-------------------------------------\n",
      "| reward:  -5.463620 |\n",
      "| reward: -83.463867 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent missed 1 observation(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| reward: -80.363696 |\n",
      "| reward: -96.215530 |\n",
      "| reward: -89.581738 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -94.340815 |\n",
      "| reward: -96.214073 |\n",
      "| reward: -96.214858 |\n",
      "| reward: -91.499472 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1160     |\n",
      "| mean episode reward    | -92.6    |\n",
      "| steps                  | 10708    |\n",
      "-------------------------------------\n",
      "| reward: -91.481795 |\n",
      "| reward: -87.698477 |\n",
      "| reward: -84.838623 |\n",
      "| reward: -93.375581 |\n",
      "| reward: -92.422486 |\n",
      "| reward: -96.229086 |\n",
      "| reward: -94.339165 |\n",
      "| reward: -78.604880 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -92.417762 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1170     |\n",
      "| mean episode reward    | -92.1    |\n",
      "| steps                  | 10814    |\n",
      "-------------------------------------\n",
      "| reward: -97.159643 |\n",
      "| reward: -96.218912 |\n",
      "| reward: -96.227770 |\n",
      "| reward: -95.287465 |\n",
      "| reward: -95.275986 |\n",
      "| reward: -90.553018 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -96.228434 |\n",
      "| reward: -95.287454 |\n",
      "| reward: -95.282201 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1180     |\n",
      "| mean episode reward    | -92.4    |\n",
      "| steps                  | 10872    |\n",
      "-------------------------------------\n",
      "| reward: -90.541001 |\n",
      "| reward: -95.282043 |\n",
      "| reward: -94.295340 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -94.324328 |\n",
      "| reward: -92.418043 |\n",
      "| reward: -93.388047 |\n",
      "| reward: -92.428658 |\n",
      "| reward: -95.288046 |\n",
      "| reward: -93.349766 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1190     |\n",
      "| mean episode reward    | -92.5    |\n",
      "| steps                  | 10947    |\n",
      "-------------------------------------\n",
      "| reward: -96.212775 |\n",
      "| reward: -92.459204 |\n",
      "| reward: -95.279847 |\n",
      "| reward: -93.381755 |\n",
      "| reward: -94.330694 |\n",
      "| reward: -94.341029 |\n",
      "| reward: -93.384743 |\n",
      "| reward: -94.313059 |\n",
      "| reward: -85.652948 |\n",
      "| reward: -94.281935 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1200     |\n",
      "| mean episode reward    | -92.3    |\n",
      "| steps                  | 11027    |\n",
      "-------------------------------------\n",
      "| reward: -61.603667 |\n",
      "| reward: -95.274030 |\n",
      "| reward: -92.431426 |\n",
      "| reward: -96.211607 |\n",
      "| reward: -93.346035 |\n",
      "| reward: -92.427259 |\n",
      "| reward: -95.248125 |\n",
      "| reward: -94.323224 |\n",
      "| reward: -95.281098 |\n",
      "| reward: -95.282201 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1210     |\n",
      "| mean episode reward    | -92.2    |\n",
      "| steps                  | 11128    |\n",
      "-------------------------------------\n",
      "| reward: -94.302471 |\n",
      "| reward: -95.282149 |\n",
      "| reward: -95.237348 |\n",
      "| reward: -96.211305 |\n",
      "| reward: -90.549637 |\n",
      "| reward: -84.891876 |\n",
      "| reward: -93.388154 |\n",
      "| reward: -97.159643 |\n",
      "| reward: -96.212986 |\n",
      "| reward: -94.314951 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1220     |\n",
      "| mean episode reward    | -92.1    |\n",
      "| steps                  | 11204    |\n",
      "-------------------------------------\n",
      "| reward: -95.279540 |\n",
      "| reward: -86.731381 |\n",
      "| reward: -93.391742 |\n",
      "| reward: -96.212190 |\n",
      "| reward: -95.283173 |\n",
      "| reward: -90.510249 |\n",
      "| reward: -93.396167 |\n",
      "| reward: -95.282106 |\n",
      "| reward: -76.899702 |\n",
      "| reward: -94.294545 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1230     |\n",
      "| mean episode reward    | -91.8    |\n",
      "| steps                  | 11300    |\n",
      "-------------------------------------\n",
      "| reward: -95.274096 |\n",
      "| reward: -90.532488 |\n",
      "| reward: -94.298184 |\n",
      "| reward: -96.210074 |\n",
      "| reward: -93.368582 |\n",
      "| reward: -96.221532 |\n",
      "| reward: -67.213839 |\n",
      "| reward: -91.464126 |\n",
      "| reward: -96.215986 |\n",
      "| reward: -95.274371 |\n",
      "-------------------------------------\n",
      "| % time spent exploring | 2        |\n",
      "| episodes               | 1240     |\n",
      "| mean episode reward    | -91.6    |\n",
      "| steps                  | 11398    |\n",
      "-------------------------------------\n",
      "This run of OpenAI - DQN - CliffWalking ran for 0:29:47 and logs are available locally at: /home/ubuntu/.hyperdash/logs/openai-dqn-cliffwalking/openai-dqn-cliffwalking_2018-01-01t22-01-09-557179.log\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissionException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda2/envs/casper/lib/python3.5/site-packages/gym_minecraft-0.0.2-py3.5.egg/gym_minecraft/envs/minecraft_env.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_host\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartMission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmission_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmission_record_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissionException\u001b[0m: Failed to find an available client for this mission - tried all the clients in the supplied client pool.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d9c983717d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory capacity\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_to_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-01ee8361807d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, exp, model, buffer_size, epsilon, learning_rate, preprocessor)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/casper/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/minecraftRL/MinecraftGym.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/casper/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/casper/lib/python3.5/site-packages/gym_minecraft-0.0.2-py3.5.egg/gym_minecraft/envs/minecraft_env.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_host\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartMission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmission_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmission_record_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"experiment_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_host\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartMission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmission_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmission_record_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pre_env = gym.make(\"MinecraftCliffWalking1-v0\")\n",
    "pre_env.init(videoResolution=[400,400],allowContinuousMovement=[\"move\", \"turn\", \"strafe\"],observeGrid=[20,-1,20,20,-1,20],observeDistance=[4,45,12])\n",
    "env = MinecraftWrapper(pre_env,1/5,(41,41))\n",
    "\n",
    "exp = Experiment(\"OpenAI - DQN - CliffWalking\")\n",
    "exp.param(\"memory capacity\",1000000)\n",
    "model1 = cnn_to_mlp([(32,8,4),(64,4,2),(64,3,1)],[256],True,True)\n",
    "train(env,exp,model1,1000000,1,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-01T01:56:56.421095Z",
     "start_time": "2018-01-01T01:56:56.413981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones_like((5,5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-27T20:27:24.193205Z",
     "start_time": "2017-12-27T20:27:24.146277Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reward_proc(info,grid_shape = (41,41)):\n",
    "    if \"grid\" not in info['observation'].keys():\n",
    "        return 0\n",
    "    # reformat grid to a vector that only show the floor with blocks\n",
    "    vec = []    \n",
    "    for item in info['observation']['grid'][::-1]:\n",
    "        if 'lava' in item:\n",
    "            vec.append(1)\n",
    "        elif 'lapis' in item:\n",
    "            vec.append(2)\n",
    "        else:\n",
    "            vec.append(0)\n",
    "\n",
    "     # compute reward depending on distance to target\n",
    "    new_state = np.array(vec)\n",
    "    tmp = np.array(vec).reshape(grid_shape)\n",
    "    idx2 = np.argwhere(tmp == 2)\n",
    "\n",
    "    size = grid_shape[0]\n",
    "    idx1 = (np.ceil(size/2),np.ceil(size/2))\n",
    "\n",
    "    a = (self._dist(idx2,idx1))\n",
    "    if(a > 0):\n",
    "        try:\n",
    "            dist_reward = 2000/(a)\n",
    "        except:\n",
    "            dist_reward = 0\n",
    "        return dist_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-27T20:37:22.909889Z",
     "start_time": "2017-12-27T20:37:22.903651Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(x,y,handle,plot):\n",
    "    plot.data_source.data['x'] += [x]\n",
    "    plot.data_source.data['y'] += [y]\n",
    "    push_notebook(handle=handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-27T20:40:52.294008Z",
     "start_time": "2017-12-27T20:40:52.191419Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_test = figure(plot_width=1000, plot_height=400,title=\"rewards_test\",\n",
    "                      x_axis_label=\"x\",\n",
    "                      y_axis_label=\"y\")\n",
    "test_plot = fig_test.line([],[],color=\"navy\",line_width=2)\n",
    "# make a grid\n",
    "handle_test = show(fig_test, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-27T20:41:38.507864Z",
     "start_time": "2017-12-27T20:41:17.103285Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for e in range(100):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    #env.render()\n",
    "    count = 0\n",
    "    while not done:\n",
    "        \n",
    "        #env.render()\n",
    "        a = env.action_space.sample()\n",
    "        s,r,done,info = env.step(a)\n",
    "        r_ = proc2_reward(info)\n",
    "        print(r_)\n",
    "        r += r_\n",
    "        update(count,r_,handle_test,test_plot)\n",
    "        count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-27T20:27:35.534116Z",
     "start_time": "2017-12-27T20:27:35.491428Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-27T20:16:30.979433Z",
     "start_time": "2017-12-27T20:16:30.328041Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "s,r,done,info = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-27T20:16:31.562802Z",
     "start_time": "2017-12-27T20:16:31.511331Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s,r,done,info = env.step(0)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-27T20:16:35.591597Z",
     "start_time": "2017-12-27T20:16:35.581697Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proc2_reward(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "            act, train, update_target, debug = deepq.build_train(\n",
    "                make_obs_ph=lambda name: U.BatchInput(env.observation_space.shape, name=name),\n",
    "                q_func= model,\n",
    "                num_actions=env.action_space.n,\n",
    "                optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate),\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T08:05:45.868620Z",
     "start_time": "2017-12-31T08:05:45.861164Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " make_obs_ph=lambda name: U.BatchInput(env.observation_space.shape, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T08:05:52.341566Z",
     "start_time": "2017-12-31T08:05:52.338380Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_func= model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T08:06:07.082146Z",
     "start_time": "2017-12-31T08:06:07.077695Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T08:06:21.066241Z",
     "start_time": "2017-12-31T08:06:21.060172Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_t_input = U.ensure_tf_input(make_obs_ph(\"obs_t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T08:06:59.094959Z",
     "start_time": "2017-12-31T08:06:59.034551Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_t = q_func(obs_t_input.get(), 6, scope=\"q_func\", reuse=True)  # reuse parameters from act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T08:07:08.408175Z",
     "start_time": "2017-12-31T08:07:08.402765Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_func_vars = U.scope_vars(U.absolute_scope_name(\"q_func\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-31T08:10:06.556430Z",
     "start_time": "2017-12-31T08:10:06.546423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'deepq/eps:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/q_func/conv2d_1/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/q_func/conv2d_1/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/q_func/conv2d_2/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/q_func/conv2d_2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/q_func/dense_1/kernel:0' shape=(61504, 6) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/q_func/dense_1/bias:0' shape=(6,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func/conv2d_3/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func/conv2d_3/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func/conv2d_4/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func/conv2d_4/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func/dense_2/kernel:0' shape=(61504, 6) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func/dense_2/bias:0' shape=(6,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/target_q_func/conv2d_5/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/target_q_func/conv2d_5/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/target_q_func/conv2d_6/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/target_q_func/conv2d_6/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/target_q_func/dense_3/kernel:0' shape=(61504, 6) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/target_q_func/dense_3/bias:0' shape=(6,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func_1/conv2d_7/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func_1/conv2d_7/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func_1/conv2d_8/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func_1/conv2d_8/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func_1/dense_4/kernel:0' shape=(61504, 6) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq_1/q_func_1/dense_4/bias:0' shape=(6,) dtype=float32_ref>,\n",
       " <tf.Variable 'q_func/conv2d_9/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'q_func/conv2d_9/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'q_func/conv2d_10/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'q_func/conv2d_10/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'q_func/dense_5/kernel:0' shape=(61504, 6) dtype=float32_ref>,\n",
       " <tf.Variable 'q_func/dense_5/bias:0' shape=(6,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:casper]",
   "language": "python",
   "name": "conda-env-casper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
